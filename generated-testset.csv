model_input,pred_correct,expected_if,predicted_if,pred_score
"def remove_point(self, time):
    """"""Remove a point, if no point is found nothing happens.

        :param int time: Time of the point.
        :raises TierTypeException: If the tier is not a TextTier.
        """"""
    if '<MASK>':
        raise Exception('Tiertype must be TextTier.')
    self.intervals = [i for i in self.intervals if i[0] != time]",False,"[""self.tier_type != 'TextTier'""]",___________________,0.0
"def confirm_authorization_request(self):
    """"""When consumer confirm the authorization.""""""
    server = self.server
    scope = request.values.get('scope') or ''
    scopes = scope.split()
    credentials = dict(client_id=request.values.get('client_id'), redirect_uri=request.values.get('redirect_uri', None), response_type=request.values.get('response_type', None), state=request.values.get('state', None))
    log.debug('Fetched credentials from request %r.', credentials)
    redirect_uri = credentials.get('redirect_uri')
    log.debug('Found redirect_uri %s.', redirect_uri)
    uri, http_method, body, headers = extract_params()
    try:
        ret = server.create_authorization_response(uri, http_method, body, headers, scopes, credentials)
        log.debug('Authorization successful.')
        return create_response(*ret)
    except oauth2.FatalClientError as e:
        log.debug('Fatal client error %r', e, exc_info=True)
        return self._on_exception(e, e.in_uri(self.error_uri))
    except oauth2.OAuth2Error as e:
        log.debug('OAuth2Error: %r', e, exc_info=True)
        state = request.values.get('state')
        if '<MASK>':
            e.state = state
        return self._on_exception(e, e.in_uri(redirect_uri or self.error_uri))
    except Exception as e:
        log.exception(e)
        return self._on_exception(e, add_params_to_uri(self.error_uri, {'error': str(e)}))",False,['state and (not e.state)'],___________________,0.0
"def getThing(self, thingTypeId, thingId):
    """"""
        Gets thing details.
        It accepts thingTypeId (string), thingId (string)
        In case of failure it throws APIException
        """"""
    thingUrl = ApiClient.thingUrl % (self.host, thingTypeId, thingId)
    r = requests.get(thingUrl, auth=self.credentials, verify=self.verify)
    status = r.status_code
    if '<MASK>':
        self.logger.debug('Thing instance was successfully retrieved')
        return r.json()
    elif '<MASK>':
        raise ibmiotf.APIException(304, 'The state of the thing has not been modified (response to a conditional GET).', None)
    elif '<MASK>':
        raise ibmiotf.APIException(401, 'The authentication token is empty or invalid', None)
    elif '<MASK>':
        raise ibmiotf.APIException(403, 'The authentication method is invalid or the api key used does not exist', None)
    elif '<MASK>':
        raise ibmiotf.APIException(404, 'A thing type with the specified id, or a thing with the specified id, does not exist.', None)
    elif '<MASK>':
        raise ibmiotf.APIException(500, 'Unexpected error', None)
    else:
        raise ibmiotf.APIException(None, 'Unexpected error', None)",False,"['status == 200', 'status == 304', 'status == 401', 'status == 403', 'status == 404', 'status == 500']",___________________,0.0
"def _wrap_handling(kwargs):
    """""" Starts running a queue handler and creates a log file for the queue.""""""
    _configure_logging(kwargs, extract=False)
    handler = kwargs['handler']
    graceful_exit = kwargs['graceful_exit']
    if '<MASK>':
        sigint_handling.start()
    handler.run()",False,['graceful_exit'],___________________,0.0
"def get_questions(self, from_date):
    """"""Retrieve all the questions from a given date.

        :param from_date: obtain questions updated since this date
        """"""
    page = 1
    url = urijoin(self.base_url, self.VERSION_API, 'questions')
    req = self.fetch(url, payload=self.__build_payload(page, from_date))
    questions = req.text
    data = req.json()
    tquestions = data['total']
    nquestions = data['page_size']
    self.__log_status(data['quota_remaining'], data['quota_max'], nquestions, tquestions)
    while questions:
        yield questions
        questions = None
        if '<MASK>':
            page += 1
            backoff = data.get('backoff', None)
            if '<MASK>':
                logger.debug('Expensive query. Wait %s secs to send a new request', backoff)
                time.sleep(float(backoff))
            req = self.fetch(url, payload=self.__build_payload(page, from_date))
            data = req.json()
            questions = req.text
            nquestions += data['page_size']
            self.__log_status(data['quota_remaining'], data['quota_max'], nquestions, tquestions)",False,"[""data['has_more']"", 'backoff']",___________________,0.0
"def get_submissions_multiple_assignments_by_sis_id(self, is_section, sis_id, students=None, assignments=None, **params):
    """"""
        List submissions for multiple assignments by course/section sis id and
        optionally student

        https://canvas.instructure.com/doc/api/submissions.html#method.submissions_api.for_students
        """"""
    if '<MASK>':
        return self.get_submissions_multiple_assignments(is_section, self._sis_id(sis_id, 'section'), students, assignments, **params)
    else:
        return self.get_submissions_multiple_assignments(is_section, self._sis_id(sis_id, 'course'), students, assignments, **params)",False,['is_section'],___________________,0.0
"def variables(self):
    """"""A list of Theano variables for loss computations.""""""
    result = self.inputs
    seen = set((i.name for i in result))
    for loss in self.losses:
        for v in loss.variables:
            if '<MASK>':
                result.append(v)
                seen.add(v.name)
    return result",False,['v.name not in seen'],(((((((((((((((((((,0.0
"def fetch_items(self, category, **kwargs):
    """"""Fetch the articles

        :param category: the category of items to fetch
        :param kwargs: backend arguments

        :returns: a generator of items
        """"""
    offset = kwargs['offset']
    logger.info(""Fetching articles of '%s' group on '%s' offset %s"", self.group, self.host, str(offset))
    narts, iarts, tarts = (0, 0, 0)
    _, _, first, last, _ = self.client.group(self.group)
    if '<MASK>':
        first = max(first, offset)
        _, overview = self.client.over((first, last))
    else:
        overview = []
    tarts = len(overview)
    logger.debug('Total number of articles to fetch: %s', tarts)
    for article_id, _ in overview:
        try:
            article_raw = self.client.article(article_id)
            article = self.__parse_article(article_raw)
        except ParseError:
            logger.warning('Error parsing %s article; skipping', article_id)
            iarts += 1
            continue
        except nntplib.NNTPTemporaryError as e:
            logger.warning(""Error '%s' fetching article %s; skipping"", e.response, article_id)
            iarts += 1
            continue
        yield article
        narts += 1",False,['offset <= last'],___________________,0.0
"def _make_fileobj(self):
    """"""
        Build file object from items.
        """"""
    bio = BytesIO()
    f = gzip.GzipFile(mode='wb', fileobj=bio) if self.use_gzip else bio
    exporter = JsonLinesItemExporter(f)
    exporter.start_exporting()
    for item in self.items:
        exporter.export_item(item)
    exporter.finish_exporting()
    if '<MASK>':
        f.close()
    bio.seek(0)
    return bio",False,['f is not bio'],_((((((((((((((((((,0.0
"def packIntf(intf, masterDirEqTo=DIRECTION.OUT, exclude=None):
    """"""
    Concatenate all signals to one big signal, recursively

    :param masterDirEqTo: only signals with this direction are packed
    :param exclude: sequence of signals/interfaces to exclude
    """"""
    if '<MASK>':
        if '<MASK>':
            return intf._sig
        return None
    res = None
    for i in intf._interfaces:
        if '<MASK>':
            continue
        if '<MASK>':
            if '<MASK>':
                d = DIRECTION.opposite(masterDirEqTo)
            else:
                d = masterDirEqTo
            s = i._pack(d, exclude=exclude)
        elif '<MASK>':
            s = i._sig
        else:
            s = None
        if '<MASK>':
            if '<MASK>':
                res = s
            else:
                res = res._concat(s)
    return res",False,"['not intf._interfaces', 'intf._masterDir == masterDirEqTo', 'exclude is not None and i in exclude', 'i._interfaces', 's is not None', 'i._masterDir == DIRECTION.IN', 'i._masterDir == masterDirEqTo', 'res is None']",___________________,0.0
"def extract_concepts(self, sentences=None, ids=None, filename=None, restrict_to_sts=None, restrict_to_sources=None):
    """""" extract_concepts takes a list of sentences and ids(optional)
            then returns a list of Concept objects extracted via
            MetaMapLite.

            Supported Options:
                Restrict to Semantic Types --restrict_to_sts
                Restrict to Sources --restrict_to_sources

            For information about the available options visit
            http://metamap.nlm.nih.gov/.

            Note: If an error is encountered the process will be closed
                  and whatever was processed, if anything, will be
                  returned along with the error found.
        """"""
    if '<MASK>':
        raise ValueError('You must either pass a list of sentences OR a filename.')
    input_file = None
    if '<MASK>':
        input_file = tempfile.NamedTemporaryFile(mode='wb', delete=False)
    else:
        input_file = open(filename, 'r')
    output_file_name = None
    error = None
    try:
        if '<MASK>':
            if '<MASK>':
                for identifier, sentence in zip(ids, sentences):
                    input_file.write('{0!r}|{1!r}\n'.format(identifier, sentence).encode('utf8'))
            else:
                for sentence in sentences:
                    input_file.write('{0!r}\n'.format(sentence).encode('utf8'))
            input_file.flush()
        command = ['bash', os.path.join(self.metamap_filename, 'metamaplite.sh')]
        if '<MASK>':
            if '<MASK>':
                restrict_to_sts = [restrict_to_sts]
            if '<MASK>':
                command.append('--restrict_to_sts')
                command.append(str(','.join(restrict_to_sts)))
        if '<MASK>':
            if '<MASK>':
                restrict_to_sources = [restrict_to_sources]
            if '<MASK>':
                command.append('--restrict_to_sources')
                command.append(str(','.join(restrict_to_sources)))
        if '<MASK>':
            command.append('--inputformat=sldiwi')
        command.append(input_file.name)
        metamap_process = subprocess.Popen(command, stdout=subprocess.PIPE)
        while metamap_process.poll() is None:
            stdout = str(metamap_process.stdout.readline())
            if '<MASK>':
                metamap_process.terminate()
                error = stdout.rstrip()
        output_file_name, file_extension = os.path.splitext(input_file.name)
        output_file_name += '.' + 'mmi'
        with open(output_file_name) as fd:
            output = fd.read()
    finally:
        if '<MASK>':
            os.remove(input_file.name)
        else:
            input_file.close()
        os.remove(output_file_name)
    concepts = CorpusLite.load(output.splitlines())
    return (concepts, error)",False,"['sentences is not None and filename is not None or (sentences is None and filename is None)', 'sentences is not None', 'sentences is not None', 'restrict_to_sts', 'restrict_to_sources', 'ids is not None', 'sentences is not None', 'ids is not None', 'isinstance(restrict_to_sts, str)', 'len(restrict_to_sts) > 0', 'isinstance(restrict_to_sources, str)', 'len(restrict_to_sources) > 0', ""'ERROR' in stdout""]",___________________,0.0
"def recursive_processing(self, base_dir, target_dir, it):
    """"""Method to recursivly process the notebooks in the `base_dir`

        Parameters
        ----------
        base_dir: str
            Path to the base example directory (see the `examples_dir`
            parameter for the :class:`Gallery` class)
        target_dir: str
            Path to the output directory for the rst files (see the
            `gallery_dirs` parameter for the :class:`Gallery` class)
        it: iterable
            The iterator over the subdirectories and files in `base_dir`
            generated by the :func:`os.walk` function""""""
    try:
        file_dir, dirs, files = next(it)
    except StopIteration:
        return ('', [])
    readme_files = {'README.md', 'README.rst', 'README.txt'}
    if '<MASK>':
        foutdir = file_dir.replace(base_dir, target_dir)
        create_dirs(foutdir)
        this_nbps = [NotebookProcessor(infile=f, outfile=os.path.join(foutdir, os.path.basename(f)), disable_warnings=self.disable_warnings, preprocess=(self.preprocess is True or f in self.preprocess) and (not (self.dont_preprocess is True or f in self.dont_preprocess)), clear=(self.clear is True or f in self.clear) and (not (self.dont_clear is True or f in self.dont_clear)), code_example=self.code_examples.get(f), supplementary_files=self.supplementary_files.get(f), other_supplementary_files=self.osf.get(f), thumbnail_figure=self.thumbnail_figures.get(f), url=self.get_url(f.replace(base_dir, '')), **self._nbp_kws) for f in map(lambda f: os.path.join(file_dir, f), filter(self.pattern.match, files))]
        readme_file = next(iter(readme_files.intersection(files)))
    else:
        return ('', [])
    labels = OrderedDict()
    this_label = 'gallery_' + foutdir.replace(os.path.sep, '_')
    if '<MASK>':
        this_label = this_label[:-1]
    for d in dirs:
        label, nbps = self.recursive_processing(base_dir, target_dir, it)
        if '<MASK>':
            labels[label] = nbps
    s = '.. _%s:\n\n' % this_label
    with open(os.path.join(file_dir, readme_file)) as f:
        s += f.read().rstrip() + '\n\n'
    s += '\n\n.. toctree::\n\n'
    s += ''.join(('    %s\n' % os.path.splitext(os.path.basename(nbp.get_out_file()))[0] for nbp in this_nbps))
    for d in dirs:
        findex = os.path.join(d, 'index.rst')
        if '<MASK>':
            s += '    %s\n' % os.path.splitext(findex)[0]
    s += '\n'
    for nbp in this_nbps:
        code_div = nbp.code_div
        if '<MASK>':
            s += code_div + '\n'
        else:
            s += nbp.thumbnail_div + '\n'
    s += ""\n.. raw:: html\n\n    <div style='clear:both'></div>\n""
    for label, nbps in labels.items():
        s += '\n.. only:: html\n\n    .. rubric:: :ref:`%s`\n\n' % label
        for nbp in nbps:
            code_div = nbp.code_div
            if '<MASK>':
                s += code_div + '\n'
            else:
                s += nbp.thumbnail_div + '\n'
        s += ""\n.. raw:: html\n\n    <div style='clear:both'></div>\n""
    s += '\n'
    with open(os.path.join(foutdir, 'index.rst'), 'w') as f:
        f.write(s)
    return (this_label, list(chain(this_nbps, *labels.values())))",False,"['readme_files.intersection(files)', ""this_label.endswith('_')"", 'label', 'os.path.exists(os.path.join(foutdir, findex))', 'code_div is not None', 'code_div is not None']",___________________,0.0
"def _exec_nb(self, cmd, cwd=None, env=None, encoding='utf-8'):
    """"""Run a command with a non blocking call.

        Execute `cmd` command with a non blocking call. The command will
        be run in the directory set by `cwd`. Enviroment variables can be
        set using the `env` dictionary. The output data is returned
        as encoded bytes in an iterator. Each item will be a line of the
        output.

        :returns: an iterator with the output of the command as encoded bytes

        :raises RepositoryError: when an error occurs running the command
        """"""
    self.failed_message = None
    logger.debug('Running command %s (cwd: %s, env: %s)', ' '.join(cmd), cwd, str(env))
    try:
        self.proc = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, cwd=cwd, env=env)
        err_thread = threading.Thread(target=self._read_stderr, kwargs={'encoding': encoding}, daemon=True)
        err_thread.start()
        for line in self.proc.stdout:
            yield line.decode(encoding, errors='surrogateescape')
        err_thread.join()
        self.proc.communicate()
        self.proc.stdout.close()
        self.proc.stderr.close()
    except OSError as e:
        err_thread.join()
        raise RepositoryError(cause=str(e))
    if '<MASK>':
        cause = 'git command - %s (return code: %d)' % (self.failed_message, self.proc.returncode)
        raise RepositoryError(cause=cause)",False,['self.proc.returncode != 0'],___________________,0.0
"def get_root_url(url, warn=True):
    """"""
    Get the ""root URL"" for a URL, as described in the LuminosoClient
    documentation.
    """"""
    parsed_url = urlparse(url)
    if '<MASK>':
        raise ValueError('Please supply a full URL, beginning with http:// or https:// .')
    root_url = '%s://%s/api/v4' % (parsed_url.scheme, parsed_url.netloc)
    if '<MASK>':
        logger.warning('Using %s as the root url' % root_url)
    return root_url",False,"['not parsed_url.scheme', ""warn and (not parsed_url.path.startswith('/api/v4'))""]",_url_url_url_url_url_url_url_url_url_,0.0
"def get_report_zip(results):
    """"""
    Creates a zip file of parsed report output

    Args:
        results (OrderedDict): The parsed results

    Returns:
        bytes: zip file bytes
    """"""

    def add_subdir(root_path, subdir):
        subdir_path = os.path.join(root_path, subdir)
        for subdir_root, subdir_dirs, subdir_files in os.walk(subdir_path):
            for subdir_file in subdir_files:
                subdir_file_path = os.path.join(root_path, subdir, subdir_file)
                if '<MASK>':
                    rel_path = os.path.relpath(subdir_root, subdir_file_path)
                    subdir_arc_name = os.path.join(rel_path, subdir_file)
                    zip_file.write(subdir_file_path, subdir_arc_name)
            for subdir in subdir_dirs:
                add_subdir(subdir_path, subdir)
    storage = BytesIO()
    tmp_dir = tempfile.mkdtemp()
    try:
        save_output(results, tmp_dir)
        with zipfile.ZipFile(storage, 'w', zipfile.ZIP_DEFLATED) as zip_file:
            for root, dirs, files in os.walk(tmp_dir):
                for file in files:
                    file_path = os.path.join(root, file)
                    if '<MASK>':
                        arcname = os.path.join(os.path.relpath(root, tmp_dir), file)
                        zip_file.write(file_path, arcname)
                for directory in dirs:
                    dir_path = os.path.join(root, directory)
                    if '<MASK>':
                        zip_file.write(dir_path, directory)
                        add_subdir(root, directory)
    finally:
        shutil.rmtree(tmp_dir)
    return storage.getvalue()",False,"['os.path.isfile(subdir_file_path)', 'os.path.isfile(file_path)', 'os.path.isdir(dir_path)']",___________________,0.0
"def _build_key_wrapping_specification(self, value):
    """"""
        Build a KeyWrappingSpecification struct from a dictionary.

        Args:
            value (dict): A dictionary containing the key/value pairs for a
                KeyWrappingSpecification struct.

        Returns:
            KeyWrappingSpecification: a KeyWrappingSpecification struct

        Raises:
            TypeError: if the input argument is invalid
        """"""
    if '<MASK>':
        return None
    if '<MASK>':
        raise TypeError('Key wrapping specification must be a dictionary.')
    encryption_key_info = self._build_encryption_key_information(value.get('encryption_key_information'))
    mac_signature_key_info = self._build_mac_signature_key_information(value.get('mac_signature_key_information'))
    key_wrapping_specification = cobjects.KeyWrappingSpecification(wrapping_method=value.get('wrapping_method'), encryption_key_information=encryption_key_info, mac_signature_key_information=mac_signature_key_info, attribute_names=value.get('attribute_names'), encoding_option=value.get('encoding_option'))
    return key_wrapping_specification",False,"['value is None', 'not isinstance(value, dict)']","_,,,,,,,,,,,,,,,,,,",0.0
"def recommendations(self, seed_artists, seed_genres, seed_tracks, *, limit=20, market=None, **filters):
    """"""Get Recommendations Based on Seeds.

        Parameters
        ----------
        seed_artists : str
            A comma separated list of Spotify IDs for seed artists. Up to 5 seed values may be provided.
        seed_genres : str
            A comma separated list of any genres in the set of available genre seeds. Up to 5 seed values may be provided.
        seed_tracks : str
            A comma separated list of Spotify IDs for a seed track. Up to 5 seed values may be provided.
        limit : Optional[int]
            The maximum number of items to return. Default: 20. Minimum: 1. Maximum: 50.
        market : Optional[str]
            An ISO 3166-1 alpha-2 country code.
        max_* : Optional[Keyword arguments]
            For each tunable track attribute, a hard ceiling on the selected track attribute’s value can be provided.
        min_* : Optional[Keyword arguments]
            For each tunable track attribute, a hard floor on the selected track attribute’s value can be provided.
        target_* : Optional[Keyword arguments]
            For each of the tunable track attributes (below) a target value may be provided.
        """"""
    route = Route('GET', '/recommendations')
    payload = {'seed_artists': seed_artists, 'seed_genres': seed_genres, 'seed_tracks': seed_tracks, 'limit': limit}
    if '<MASK>':
        payload['market'] = market
    if '<MASK>':
        payload.update(filters)
    return self.request(route, param=payload)",False,"['market', 'filters']",___________________,0.0
"def users(context):
    """"""Show all users in the database""""""
    LOG.info('Running scout view users')
    adapter = context.obj['adapter']
    user_objs = adapter.users()
    if '<MASK>':
        LOG.info('No users found')
        context.abort()
    click.echo('#name\temail\troles\tinstitutes')
    for user_obj in user_objs:
        click.echo('{0}\t{1}\t{2}\t{3}\t'.format(user_obj['name'], user_obj.get('mail', user_obj['_id']), ', '.join(user_obj.get('roles', [])), ', '.join(user_obj.get('institutes', []))))",False,['user_objs.count() == 0'],___________________,0.0
"def addColor(self, level, text):
    """"""addColor to the prompt (usually prefix) if terminal
        supports, and specified to do so""""""
    if '<MASK>':
        if '<MASK>':
            text = '%s%s%s' % (self.colors[level], text, self.colors['OFF'])
    return text",False,"['self.colorize', 'level in self.colors']",___________________,0.0
"def add_files(self, *filenames: str, owner: str=SANDBOX_USERNAME, read_only: bool=False):
    """"""
        Copies the specified files into the working directory of this
        sandbox.
        The filenames specified can be absolute paths or relative paths
        to the current working directory.

        :param owner: The name of a user who should be granted ownership of
            the newly added files.
            Must be either autograder_sandbox.SANDBOX_USERNAME or 'root',
            otherwise ValueError will be raised.
        :param read_only: If true, the new files' permissions will be set to
            read-only.
        """"""
    if '<MASK>':
        raise ValueError('Invalid value for parameter ""owner"": {}'.format(owner))
    with tempfile.TemporaryFile() as f, tarfile.TarFile(fileobj=f, mode='w') as tar_file:
        for filename in filenames:
            tar_file.add(filename, arcname=os.path.basename(filename))
        f.seek(0)
        subprocess.check_call(['docker', 'cp', '-', self.name + ':' + SANDBOX_WORKING_DIR_NAME], stdin=f)
        file_basenames = [os.path.basename(filename) for filename in filenames]
        if '<MASK>':
            self._chown_files(file_basenames)
        if '<MASK>':
            chmod_cmd = ['chmod', '444'] + file_basenames
            self.run_command(chmod_cmd, as_root=True)",False,"[""owner != SANDBOX_USERNAME and owner != 'root'"", 'owner == SANDBOX_USERNAME', 'read_only']",___________________,0.0
"def asHdl(cls, obj, ctx: SerializerCtx):
    """"""
        Convert object to HDL string

        :param obj: object to serialize
        :param ctx: SerializerCtx instance
        """"""
    if '<MASK>':
        return cls.SignalItem(obj, ctx)
    elif '<MASK>':
        return cls.Value(obj, ctx)
    else:
        try:
            serFn = getattr(cls, obj.__class__.__name__)
        except AttributeError:
            raise SerializerException('Not implemented for', obj)
        return serFn(obj, ctx)",False,"['isinstance(obj, RtlSignalBase)', 'isinstance(obj, Value)']","((,,,,,,,,,,,,,,,,,",0.0
"def read_next_data_block(self):
    """""" Read the next block of data and its header

        Returns: (header, data)
            header (dict): dictionary of header metadata
            data (np.array): Numpy array of data, converted into to complex64.
        """"""
    header, data_idx = self.read_header()
    self.file_obj.seek(data_idx)
    n_chan = int(header['OBSNCHAN'])
    n_pol = int(header['NPOL'])
    n_bit = int(header['NBITS'])
    n_samples = int(int(header['BLOCSIZE']) / (n_chan * n_pol * (n_bit / 8)))
    d = np.ascontiguousarray(np.fromfile(self.file_obj, count=header['BLOCSIZE'], dtype='int8'))
    if '<MASK>':
        d = unpack(d, n_bit)
    dshape = self.read_next_data_block_shape()
    d = d.reshape(dshape)
    if '<MASK>':
        self._d = np.zeros(d.shape, dtype='float32')
    self._d[:] = d
    return (header, self._d[:].view('complex64'))",False,"['n_bit != 8', 'self._d.shape != d.shape']",___________________,0.0
"def _change_logging_kwargs(kwargs):
    """""" Helper function to turn the simple logging kwargs into a `log_config`.""""""
    log_levels = kwargs.pop('log_level', None)
    log_folder = kwargs.pop('log_folder', 'logs')
    logger_names = kwargs.pop('logger_names', '')
    if '<MASK>':
        log_levels = kwargs.pop('log_levels', logging.INFO)
    log_multiproc = kwargs.pop('log_multiproc', True)
    if '<MASK>':
        logger_names = [logger_names]
    if '<MASK>':
        log_levels = [log_levels]
    if '<MASK>':
        log_levels = [log_levels[0] for _ in logger_names]
    dictionary = copy.deepcopy(LOGGING_DICT)
    prefixes = ['']
    if '<MASK>':
        for key in list(dictionary.keys()):
            if '<MASK>':
                del dictionary[key]
    else:
        prefixes.append('multiproc_')
    for prefix in prefixes:
        for handler_dict in dictionary[prefix + 'handlers'].values():
            if '<MASK>':
                filename = os.path.join(log_folder, handler_dict['filename'])
                filename = os.path.normpath(filename)
                handler_dict['filename'] = filename
        dictionary[prefix + 'loggers'] = {}
        logger_dict = dictionary[prefix + 'loggers']
        for idx, logger_name in enumerate(logger_names):
            logger_dict[logger_name] = {'level': log_levels[idx], 'handlers': list(dictionary[prefix + 'handlers'].keys())}
    kwargs['log_config'] = dictionary",False,"['log_levels is None', 'not isinstance(logger_names, (tuple, list))', 'not isinstance(log_levels, (tuple, list))', 'len(log_levels) == 1', 'not log_multiproc', ""key.startswith('multiproc_')"", ""'filename' in handler_dict""]",___________________,0.0
"def _validate_type(cls, typeobj):
    """"""
        Validate that all required type methods are implemented.

        At minimum a type must have:
        - a convert() or convert_binary() function
        - a default_formatter() function

        Raises an ArgumentError if the type is not valid
        """"""
    if '<MASK>':
        raise ArgumentError('type is invalid, does not have convert or convert_binary function', type=typeobj, methods=dir(typeobj))
    if '<MASK>':
        raise ArgumentError('type is invalid, does not have default_formatter function', type=typeobj, methods=dir(typeobj))",False,"[""not (hasattr(typeobj, 'convert') or hasattr(typeobj, 'convert_binary'))"", ""not hasattr(typeobj, 'default_formatter')""]",___________________,0.0
"def prepare(self, **kwargs):
    """""" Prepare for rendering """"""
    for k, v in kwargs.items():
        setattr(self, k, v)
    if '<MASK>':
        self.initialize()
    if '<MASK>':
        self.activate_proxy()",False,"['not self.is_initialized', 'not self.proxy_is_active']","(self,,,,,,,,,,,,,,,,,",0.0
"def reindex(self, z=None):
    """"""Raises/lower the component in the window hierarchy (Z-order/tab order)""""""
    if '<MASK>':
        if '<MASK>':
            return len(self._parent._children_list)
        i = self._parent._children_list.index(self)
        if '<MASK>':
            return i
        if '<MASK>':
            raise RuntimeError('reindexing can only be done on design mode')
        del self._parent._children_list[i]
        if '<MASK>':
            self._parent._children_list.append(self)
        else:
            self._parent._children_list.insert(z, self)",False,"['isinstance(self._parent, Component)', 'not self in self._parent._children_list', 'z is None', ""not hasattr(self, 'designer') and (not self.designer)"", 'z < 0']",(((((((((((((((((((,0.0
"def get_table(self, items):
    """"""Generate a proper list of table nodes for autosummary:: directive.

        *items* is a list produced by :meth:`get_items`.
        """"""
    table_spec = addnodes.tabular_col_spec()
    table_spec['spec'] = 'll'
    table = autosummary_table('')
    real_table = nodes.table('', classes=['longtable'])
    table.append(real_table)
    group = nodes.tgroup('', cols=2)
    real_table.append(group)
    group.append(nodes.colspec('', colwidth=10))
    group.append(nodes.colspec('', colwidth=90))
    body = nodes.tbody('')
    group.append(body)

    def append_row(*column_texts):
        row = nodes.row('')
        for text in column_texts:
            node = nodes.paragraph('')
            vl = ViewList()
            vl.append(text, '<autosummary>')
            self.state.nested_parse(vl, 0, node)
            try:
                if '<MASK>':
                    node = node[0]
            except IndexError:
                pass
            row.append(nodes.entry('', node))
        body.append(row)
    for name, sig, summary, real_name in items:
        qualifier = 'obj'
        if '<MASK>':
            col1 = ':%s:`%s <%s>`\\ %s' % (qualifier, name, real_name, sig)
        else:
            col1 = ':%s:`%s <%s>`' % (qualifier, name, real_name)
        col2 = summary
        append_row(col1, col2)
    return [table_spec, table]",False,"[""'nosignatures' not in self.options"", 'isinstance(node[0], nodes.paragraph)']",___________________,0.0
"def widget(self, param_name):
    """"""Get widget for param_name""""""
    if '<MASK>':
        self._widgets[param_name] = self._make_widget(param_name)
    return self._widgets[param_name]",False,['param_name not in self._widgets'],"_name):
    """""" """""" """""" """""" """""" """""" """""" """""" """""" """""" """""" """""" """""" """"""",0.0
"def container_search(self, query, across_collections=False):
    """"""search for a specific container. If across collections is False,
    the query is parsed as a full container name and a specific container
    is returned. If across_collections is True, the container is searched
    for across collections. If across collections is True, details are
    not shown""""""
    query = query.lower().strip('/')
    q = parse_image_name(remove_uri(query), defaults=False)
    if '<MASK>':
        if '<MASK>':
            url = '%s/container/search/name/%s/tag/%s' % (self.base, q['image'], q['tag'])
        else:
            url = '%s/container/search/collection/%s/name/%s/tag/%s' % (self.base, q['collection'], q['image'], q['tag'])
    elif '<MASK>':
        if '<MASK>':
            url = '%s/container/search/name/%s' % (self.base, q['image'])
        else:
            url = '%s/container/search/collection/%s/name/%s' % (self.base, q['collection'], q['image'])
    result = self._get(url)
    if '<MASK>':
        result = result['containers']
    if '<MASK>':
        bot.info('No containers found.')
        sys.exit(1)
    bot.info('Containers %s' % query)
    rows = []
    for c in result:
        rows.append(['%s/%s' % (c['collection'], c['name']), c['tag']])
    bot.table(rows)
    return rows",False,"[""q['tag'] is not None"", ""'containers' in result"", 'len(result) == 0', 'across_collections is True', ""q['tag'] is None"", 'across_collections is True']",___________________,0.0
"def get_items(self, names):
    """"""Try to import the given names, and return a list of
        ``[(name, signature, summary_string, real_name), ...]``.
        """"""
    env = self.state.document.settings.env
    prefixes = get_import_prefixes_from_env(env)
    items = []
    max_item_chars = 50
    for name in names:
        display_name = name
        if '<MASK>':
            name = name[1:]
            display_name = name.split('.')[-1]
        try:
            real_name, obj, parent = import_by_name(name, prefixes=prefixes)
        except ImportError:
            self.warn('failed to import %s' % name)
            items.append((name, '', '', name))
            continue
        documenter = get_documenter(obj, parent)(self, real_name)
        if '<MASK>':
            self.warn('failed to parse name %s' % real_name)
            items.append((display_name, '', '', real_name))
            continue
        if '<MASK>':
            self.warn('failed to import object %s' % real_name)
            items.append((display_name, '', '', real_name))
            continue
        sig = documenter.format_signature()
        if '<MASK>':
            sig = ''
        else:
            max_chars = max(10, max_item_chars - len(display_name))
            sig = mangle_signature(sig, max_chars=max_chars)
            sig = sig.replace('*', '\\*')
        doc = list(documenter.process_doc(documenter.get_doc()))
        while doc and (not doc[0].strip()):
            doc.pop(0)
        m = re.search('^([A-Z][^A-Z]*?\\.\\s)', ' '.join(doc).strip())
        if '<MASK>':
            summary = m.group(1).strip()
        elif '<MASK>':
            summary = doc[0].strip()
        else:
            summary = ''
        items.append((display_name, sig, summary, real_name))
    return items",False,"[""name.startswith('~')"", 'not documenter.parse_name()', 'not documenter.import_object()', 'not sig', 'm', 'doc']","_____)
        """"""..........",0.0
"def addPhysicalInterfaceToDeviceType(self, typeId, physicalInterfaceId):
    """"""
        Adds a physical interface to a device type.
        Parameters:
            - typeId (string) - the device type
            - physicalInterfaceId (string) - the id returned by the platform on creation of the physical interface
        Throws APIException on failure.
        """"""
    req = ApiClient.oneDeviceTypePhysicalInterfaceUrl % (self.host, '/draft', typeId)
    body = {'id': physicalInterfaceId}
    resp = requests.post(req, auth=self.credentials, headers={'Content-Type': 'application/json'}, data=json.dumps(body), verify=self.verify)
    if '<MASK>':
        self.logger.debug('Physical interface added to a device type')
    else:
        raise ibmiotf.APIException(resp.status_code, 'HTTP error adding physical interface to a device type', resp)
    return resp.json()",False,['resp.status_code == 201'],___________________,0.0
"def _get_chunks(chunksize, *iterables):
    """"""Iterates over zip()ed iterables in chunks. """"""
    if '<MASK>':
        it = itertools.izip(*iterables)
    else:
        it = zip(*iterables)
    while True:
        chunk = tuple(itertools.islice(it, chunksize))
        if '<MASK>':
            return
        yield chunk",False,"['sys.version_info < (3, 3)', 'not chunk']",",,,,,,,,,,,,,,,,,,,",0.0
"def gene_variants(institute_id):
    """"""Display a list of SNV variants.""""""
    page = int(request.form.get('page', 1))
    institute_obj = institute_and_case(store, institute_id)
    if '<MASK>':
        form = GeneVariantFiltersForm(request.form)
    else:
        form = GeneVariantFiltersForm(request.args)
    variant_type = form.data.get('variant_type', 'clinical')
    hgnc_symbols = []
    non_clinical_symbols = []
    not_found_symbols = []
    not_found_ids = []
    data = {}
    if '<MASK>':
        is_clinical = form.data.get('variant_type', 'clinical') == 'clinical'
        clinical_symbols = store.clinical_symbols(case_obj) if is_clinical else None
        for hgnc_symbol in form.hgnc_symbols.data:
            if '<MASK>':
                hgnc_gene = store.hgnc_gene(int(hgnc_symbol))
                if '<MASK>':
                    not_found_ids.append(hgnc_symbol)
                else:
                    hgnc_symbols.append(hgnc_gene['hgnc_symbol'])
            elif '<MASK>':
                not_found_symbols.append(hgnc_symbol)
            elif '<MASK>':
                non_clinical_symbols.append(hgnc_symbol)
            else:
                hgnc_symbols.append(hgnc_symbol)
        if '<MASK>':
            flash('HGNC id not found: {}'.format(', '.join(not_found_ids)), 'warning')
        if '<MASK>':
            flash('HGNC symbol not found: {}'.format(', '.join(not_found_symbols)), 'warning')
        if '<MASK>':
            flash('Gene not included in clinical list: {}'.format(', '.join(non_clinical_symbols)), 'warning')
        form.hgnc_symbols.data = hgnc_symbols
        log.debug('query {}'.format(form.data))
        variants_query = store.gene_variants(query=form.data, category='snv', variant_type=variant_type)
        data = controllers.gene_variants(store, variants_query, page)
    return dict(institute=institute_obj, form=form, page=page, **data)",False,"[""request.method == 'POST'"", 'form.hgnc_symbols.data and len(form.hgnc_symbols.data) > 0', 'not_found_ids', 'not_found_symbols', 'non_clinical_symbols', 'hgnc_symbol.isdigit()', 'hgnc_gene is None', 'store.hgnc_genes(hgnc_symbol).count() == 0', 'is_clinical and hgnc_symbol not in clinical_symbols']",___________________,0.0
"def finalize(self):
    """"""Finalize the counter

        Once finalized, the counter never be incremented and the callback
        can be invoked once the count reaches zero
        """"""
    with self._lock:
        self._is_finalized = True
        if '<MASK>':
            self._callback()",False,['self._count == 0'],"
        """"""
        """"""
        """"""
        """"""
        """"""
        """"""
",0.0
"def add_subparsers(self, parser):
    """"""
        Adds the subparsers to an argparse.ArgumentParser

        @param parser An argparse.ArgumentParser instance
        """"""
    sgroup = getattr(self, 'subparser_group', None)
    if '<MASK>':
        sgroup.add_to_parser(self)
    if '<MASK>':
        return
    args = self.subparsers_args or self.get_default_subparsers_args()
    kwargs = self.subparsers_kwargs or self.get_default_subparsers_kwargs()
    subs = parser.add_subparsers(*args, **kwargs)
    for subparser in self.subparsers:
        subparser.add_to_parser(subs)",False,"['sgroup', 'not self.subparsers']",___________________,0.0
"def band(self, sf_0, B_sf, force=False):
    """"""
        Returns the radial frequency envelope:

        Selects a preferred spatial frequency ``sf_0`` and a bandwidth ``B_sf``.

        """"""
    if '<MASK>':
        return 1.0
    elif '<MASK>':
        tag = str(sf_0) + '_' + str(B_sf)
        try:
            return self.cache['band'][tag]
        except:
            if '<MASK>':
                print('doing band cache for tag ', tag)
            self.cache['band'][tag] = self.band(sf_0, B_sf, force=True)
            return self.cache['band'][tag]
    else:
        env = 1.0 / self.f * np.exp(-0.5 * np.log(self.f / sf_0) ** 2 / B_sf ** 2)
    return env",False,"['sf_0 == 0.0', 'self.pe.use_cache and (not force)', 'self.pe.verbose > 50']","_0,_0,_0,_0,_0,_0,_",0.0
"def handle_input(self, input):
    """"""Takes a single character string as input and alters the game
        state according to that input. Mostly, this means moving the
        player around. Returns a new game state and boolean indicating
        whether the input had an effect on the state.""""""
    dirs = {'h': (-1, 0), 'j': (0, 1), 'k': (0, -1), 'l': (1, 0), 'y': (-1, -1), 'u': (1, -1), 'n': (1, 1), 'b': (-1, 1)}
    if '<MASK>':
        new_self = (lens.player + dirs[input])(self)
        if '<MASK>':
            return (self, False)
        return (new_self, True)
    elif '<MASK>':
        return (self, True)
    elif '<MASK>':
        return (self.end_game(), False)
    elif '<MASK>':
        self = lens.player.set(Vector.random())(self)
        return (self, True)
    else:
        return (self, False)",False,"['input in dirs', 'not new_self.player.inside()', ""input == '.'"", ""input == 'q'"", ""input == 't'""]","_self,,,,,,,,,,,,,,,,,",0.0
"def toHVal(op: Any, suggestedType: Optional[HdlType]=None):
    """"""Convert python or hdl value/signal object to hdl value/signal object""""""
    if '<MASK>':
        return op
    elif '<MASK>':
        return op._sig
    else:
        if '<MASK>':
            if '<MASK>':
                return suggestedType.fromPy(op)
            if '<MASK>':
                raise TypeError('Number %d is too big to fit in 32 bit integer of HDL use Bits type instead' % op)
            elif '<MASK>':
                raise TypeError('Number %d is too small to fit in 32 bit integer of HDL use Bits type instead' % op)
        try:
            hType = defaultPyConversions[type(op)]
        except KeyError:
            hType = None
        if '<MASK>':
            raise TypeError('Unknown hardware type for %s' % op.__class__)
        return hType.fromPy(op)",False,"['isinstance(op, Value) or isinstance(op, SignalItem)', 'isinstance(op, InterfaceBase)', 'isinstance(op, int)', 'hType is None', 'suggestedType is not None', 'op >= 1 << 31', 'op < -(1 << 31)']",___________________,0.0
"def receive_ack_renewing(self, pkt):
    """"""Receive ACK in RENEWING state.""""""
    logger.debug('C3. Received ACK?, in RENEWING state.')
    if '<MASK>':
        logger.debug('C3: T. Received ACK, in RENEWING state, raise BOUND.')
        raise self.BOUND()",False,['self.process_received_ack(pkt)'],"_,,,,,,,,,,,,,,,,,,",0.0
"def error_uri(self):
    """"""The error page URI.

        When something turns error, it will redirect to this error page.
        You can configure the error page URI with Flask config::

            OAUTH2_PROVIDER_ERROR_URI = '/error'

        You can also define the error page by a named endpoint::

            OAUTH2_PROVIDER_ERROR_ENDPOINT = 'oauth.error'
        """"""
    error_uri = self.app.config.get('OAUTH2_PROVIDER_ERROR_URI')
    if '<MASK>':
        return error_uri
    error_endpoint = self.app.config.get('OAUTH2_PROVIDER_ERROR_ENDPOINT')
    if '<MASK>':
        return url_for(error_endpoint)
    return '/oauth/errors'",False,"['error_uri', 'error_endpoint']",___________________,0.0
"def cache(requires=None, disabled=False, applied_on_method=False, check_param=True, limit=None):
    """""" Avoid to recompute a function if its parameters and its source code doesnt have changed.

        Args:
            requires: list of dependencies (functions or function names)
            disabled (bool): disable the cache mecanism for this function (useful if you
                                 only want to use the dependency mecanism)
            applied_on_method (bool): ignore the first argument (useful to ignore ""self"")
            check_param (True, False or a str): the name of the parameter to check.
                                                    False to not check any of them.
                                                    True (default) to check all of them.
            limit (int or None): number of cache entries to keep (no limit by default)
    """"""
    if '<MASK>':
        requires = []
    elif '<MASK>':
        requires = [requires]
    if '<MASK>':
        raise TypeError(""'check_param' must be a str (name of the param to check) or a bool"")
    if '<MASK>':
        raise TypeError(""'limit' must be an int (number of cache entries to keep) or None"")
    if '<MASK>':
        cache.funcs_references = {}
    if '<MASK>':
        cache.dependencies = {}
    if '<MASK>':
        cache.memories = {}

    def decorator(func):
        """""" This code is executed when the augment module is read (when decorator is applied).
            Here we populate cache.funcs_references and cache.dependencies to use them later. """"""
        cache.funcs_references[func.__name__] = get_orig_function(func)
        dependencies_names = []
        for requirement in requires:
            if '<MASK>':
                req_name = requirement.__name__
                cache.funcs_references[req_name] = get_orig_function(requirement)
            elif '<MASK>':
                req_name = requirement
                cache.funcs_references[req_name] = None
            dependencies_names.append(req_name)
        cache.dependencies[func.__name__] = dependencies_names

        @wraps(func)
        def wrapper(*args, **kwargs):
            """""" This code is executed when a decorated function is actually executed.
                It uses the previously built dependency tree (see above). """"""
            current_memory = cache.memories.get(current_thread().name)
            if '<MASK>':
                return func(*args, **kwargs)
            concatenated_source_code = ''
            dependencies = resolve_dependencies(func.__name__, cache.dependencies)
            for func_name in dependencies:
                function = cache.funcs_references[func_name]
                if '<MASK>':
                    raise Exception(f""Can't get source code of function '{func_name}'"")
                source_code = get_func_sourcecode(function)
                concatenated_source_code += source_code
            md5_hash = md5(str.encode(concatenated_source_code)).hexdigest()
            tmp_extra_kwargs = {'__func_dependencies_hash__': md5_hash, '__original_func_name__': func.__name__}
            if '<MASK>':
                kwargs.update(tmp_extra_kwargs)
                if '<MASK>':
                    self_arg, args = (args[0], args[1:])

                @wraps(func)
                def f(*args, **kwargs):
                    for k in tmp_extra_kwargs.keys():
                        del kwargs[k]
                    if '<MASK>':
                        args = (self_arg,) + args
                    return func(*args, **kwargs)
                f = current_memory.cache(f)
                result = f(*args, **kwargs)
            else:
                if '<MASK>':
                    check_only_param_value = get_param_value_from_func_call(param_name=check_param, func=func, call_args=args, call_kwargs=kwargs)
                    tmp_extra_kwargs['__check_only__'] = check_only_param_value

                @wraps(func)
                def f(**tmp_extra_kwargs):
                    return func(*args, **kwargs)
                f = current_memory.cache(f)
                result = f(**tmp_extra_kwargs)
            if '<MASK>':
                clean_cachedir_old_entries(f.store_backend, func.__name__, limit)
            return result
        return wrapper
    return decorator",False,"['not requires', 'not isinstance(check_param, (bool, str))', 'limit is not None and (not isinstance(limit, int))', ""not hasattr(cache, 'funcs_references')"", ""not hasattr(cache, 'dependencies')"", ""not hasattr(cache, 'memories')"", 'isinstance(requires, collections.Callable)', 'isinstance(requirement, collections.Callable)', 'disabled is True or current_memory is None', 'check_param is True', 'limit is not None', 'requirement not in cache.funcs_references', 'function is None', 'applied_on_method', 'isinstance(check_param, str)', 'applied_on_method']",___________________,0.0
"def wellcome_tip(wx_obj):
    """"""Show a tip message""""""
    msg = 'Close the main window to exit & save.\nDrag & Drop / Click the controls from the ToolBox to create new ones.\nLeft click on the created controls to select them.\nDouble click to edit the default property.\nRight click to pop-up the context menu.\n'
    stt = STT.SuperToolTip(msg)
    stt.SetHeader('Welcome to gui2py designer!')
    stt.SetDrawHeaderLine(True)
    stt.ApplyStyle('Office 2007 Blue')
    stt.SetDropShadow(True)
    stt.SetHeaderBitmap(images.designer.GetBitmap())
    stt.SetEndDelay(15000)
    tip = CustomToolTipWindow(wx_obj, stt)
    tip.CalculateBestSize()
    tip.CalculateBestPosition(wx_obj)
    tip.DropShadow(stt.GetDropShadow())
    if '<MASK>':
        show = lambda: tip.StartAlpha(True)
    else:
        show = lambda: tip.Show()
    wx.CallLater(1000, show)
    wx.CallLater(30000, tip.Destroy)",False,['stt.GetUseFade()'],"(,,,,,,,,,,,,,,,,,,",0.0
"def import_dir(self, dir_path, params={}):
    """"""
        Imports a directory of CSV files.

        https://canvas.instructure.com/doc/api/sis_imports.html#method.sis_imports_api.create
        """"""
    if '<MASK>':
        raise MissingAccountID()
    body = self._build_archive(dir_path)
    params['import_type'] = SISImportModel.CSV_IMPORT_TYPE
    url = SIS_IMPORTS_API.format(self._canvas_account_id) + '.json{}'.format(self._params(params))
    headers = {'Content-Type': 'application/zip'}
    return SISImportModel(data=self._post_resource(url, headers, body))",False,['not self._canvas_account_id'],___________________,0.0
"def invoke_one(self, line):
    """"""Invoke a function given a list of arguments with the function listed first.

        The function is searched for using the current context on the context stack
        and its annotated type information is used to convert all of the string parameters
        passed in line to appropriate python types.

        Args:
            line (list): The list of command line arguments.

        Returns:
            (object, list, bool): A tuple containing the return value of the function, if any,
                a boolean specifying if the function created a new context (False if a new context
                was created) and a list with the remainder of the command line if this function
                did not consume all arguments.
        """"""
    funname = line.pop(0)
    context = self.contexts[-1]
    func = self.find_function(context, funname)
    if '<MASK>':
        self.contexts.append(func)
        self._check_initialize_context()
        return (None, line, False)
    if '<MASK>':
        val = func(line)
        line = []
    else:
        posargs, kwargs, line = self.process_arguments(func, line)
        if '<MASK>':
            raise ValidationError('Not enough parameters specified to call function', function=func.metadata.name, signature=func.metadata.signature())
        val = func(*posargs, **kwargs)
    finished = True
    if '<MASK>':
        self.contexts.pop()
    elif '<MASK>':
        if '<MASK>':
            val = func.metadata.format_returnvalue(val)
        else:
            self.contexts.append(val)
            self._check_initialize_context()
            finished = False
            val = None
    return (val, line, finished)",False,"['isinstance(func, dict)', 'func.takes_cmdline is True', 'func.finalizer is True', 'inspect.isclass(func) and (not func.metadata.spec_filled(posargs, kwargs))', 'val is not None', 'func.metadata.returns_data()']",___________________,0.0
"def next(self):
    while True:
        if '<MASK>':
            line = next(self.follow_generator)
        else:
            line = None
        if '<MASK>':
            if '<MASK>':
                try:
                    is_file_changed = not os.path.isfile(file_path) or os.stat(file_path).st_ino != os.fstat(self.following_file.fileno()).st_ino
                except OSError:
                    is_file_changed = True
                if '<MASK>':
                    self.following_file.close()
                    self.following_file = None
                    self.follow_generator = None
            if '<MASK>':
                try:
                    self.following_file = io.open(file_path, 'rb', buffering)
                    self.follow_generator = Tailer(self.following_file, end=self.follow_from_end_on_open).follow()
                    self.follow_from_end_on_open = False
                except (IOError, OSError) as e:
                    LOG.info('Unable to tail file: %s', e)
                    if '<MASK>':
                        self.following_file.close()
                    self.following_file = None
                    self.follow_generator = None
                    line = None
                else:
                    line = next(self.follow_generator)
        return line.decode(encoding, errors) if line is not None else line",False,"['self.follow_generator', 'line is None', 'self.follow_generator', 'not self.follow_generator and os.path.isfile(file_path)', 'is_file_changed', 'self.following_file']","____________)
    """""".
   ",0.0
"def info_signal(self, args):
    """"""Print information about a signal""""""
    if '<MASK>':
        return None
    signame = args[0]
    if '<MASK>':
        if '<MASK>':
            self.dbgr.core.processor.section(self.header)
            for signame in self.siglist:
                self.print_info_signal_entry(signame)
            return True
        else:
            signame = args[1]
            pass
        pass
    signame = self.is_name_or_number(signame)
    self.dbgr.core.processor.section(self.header)
    self.print_info_signal_entry(signame)
    return True",False,"['len(args) == 0', ""signame in ['handle', 'signal']"", 'len(args) == 1']",",,,,,,,,,,,,,,,,,,,",0.0
"def create_index(idx_url, clean=False):
    """"""Configure the index to work with""""""
    try:
        r = requests.get(idx_url)
    except requests.exceptions.ConnectionError:
        cause = 'Error connecting to Elastic Search (index: %s)' % idx_url
        raise ElasticSearchError(cause=cause)
    if '<MASK>':
        r = requests.put(idx_url)
        if '<MASK>':
            logger.info(""Can't create index %s (%s)"", idx_url, r.status_code)
            cause = 'Error creating Elastic Search index %s' % idx_url
            raise ElasticSearchError(cause=cause)
        logger.info('Index %s created', idx_url)
        return True
    elif '<MASK>':
        requests.delete(idx_url)
        requests.put(idx_url)
        logger.info('Index deleted and created (index: %s)', idx_url)
        return True
    return False",False,"['r.status_code != 200', 'r.status_code != 200', 'r.status_code == 200 and clean']","______url)
                                                                      ",0.0
"def _check_call_func(self, node):
    """"""Checks that function call is not format_string.format().

        Args:
          node (astroid.node_classes.Call):
            Call AST node to be checked.
        """"""
    func = utils.safe_infer(node.func)
    types = ('str', 'unicode')
    methods = ('format',)
    if '<MASK>':
        self.add_message('logging-format-interpolation', node=node)",False,"['is_method_call(func, types, methods) and (not is_complex_format_str(func.bound))']",___________________,0.0
"def delete_lower(script, layer_num=None):
    """""" Delete all layers below the specified one.

    Useful for MeshLab ver 2016.12, whcih will only output layer 0.
    """"""
    if '<MASK>':
        layer_num = script.current_layer()
    if '<MASK>':
        change(script, 0)
    for i in range(layer_num):
        delete(script, 0)
    return None",False,"['layer_num is None', 'layer_num != 0']","_,,,,,,,,,,,,,,,,,,",0.0
"def node_to_text(self, node, prev_node_hint=None):
    """"""
        Return the textual representation of the given `node`.

        If `prev_node_hint` is specified, then the current node is formatted
        suitably as following the node given in `prev_node_hint`.  This might
        affect how much space we keep/discard, etc.
        """"""
    if '<MASK>':
        return ''
    if '<MASK>':
        if '<MASK>':
            return ''
        return node.chars
    if '<MASK>':
        if '<MASK>':
            if '<MASK>':
                return '%' + node.comment + '\n'
            else:
                return '%' + node.comment + node.comment_post_space
        elif '<MASK>':
            return ''
        else:
            return node.comment_post_space
    if '<MASK>':
        contents = self._groupnodecontents_to_text(node)
        if '<MASK>':
            return '{' + contents + '}'
        return contents

    def apply_simplify_repl(node, simplify_repl, nodelistargs, what):
        if '<MASK>':
            if '<MASK>':
                return simplify_repl(node, l2tobj=self)
            return simplify_repl(node)
        if '<MASK>':
            try:
                return simplify_repl % tuple([self._groupnodecontents_to_text(nn) for nn in nodelistargs])
            except (TypeError, ValueError):
                logger.warning('WARNING: Error in configuration: {} failed its substitution!'.format(what))
                return simplify_repl
        return simplify_repl
    if '<MASK>':
        macroname = node.macroname.rstrip('*')
        if '<MASK>':
            mac = self.macro_dict[macroname]
        else:
            mac = self.macro_dict['']

        def get_macro_str_repl(node, macroname, mac):
            if '<MASK>':
                return apply_simplify_repl(node, mac.simplify_repl, node.nodeargs, what=""macro '%s'"" % macroname)
            if '<MASK>':
                return ''
            a = node.nodeargs
            if '<MASK>':
                a.prepend(node.nodeoptarg)
            return ''.join([self._groupnodecontents_to_text(n) for n in a])
        macrostr = get_macro_str_repl(node, macroname, mac)
        return macrostr
    if '<MASK>':
        envname = node.envname.rstrip('*')
        if '<MASK>':
            envdef = self.env_dict[envname]
        else:
            envdef = self.env_dict['']
        if '<MASK>':
            return apply_simplify_repl(node, envdef.simplify_repl, node.nodelist, what=""environment '%s'"" % envname)
        if '<MASK>':
            return ''
        return self._nodelistcontents_to_text(node.nodelist)
    if '<MASK>':
        if '<MASK>':
            return latexwalker.math_node_to_latex(node)
        else:
            with _PushEquationContext(self):
                return self._nodelistcontents_to_text(node.nodelist)
    logger.warning('LatexNodes2Text.node_to_text(): Unknown node: %r', node)
    return ''",False,"['node is None', 'node.isNodeType(latexwalker.LatexCharsNode)', 'node.isNodeType(latexwalker.LatexCommentNode)', 'node.isNodeType(latexwalker.LatexGroupNode)', 'node.isNodeType(latexwalker.LatexMacroNode)', 'node.isNodeType(latexwalker.LatexEnvironmentNode)', 'node.isNodeType(latexwalker.LatexMathNode)', ""not self.strict_latex_spaces['between-latex-constructs'] and len(node.chars.strip()) == 0"", 'self.keep_comments', 'self.keep_braced_groups and len(contents) >= self.keep_braced_groups_minlen', 'callable(simplify_repl)', ""'%' in simplify_repl"", 'macroname in self.macro_dict', 'envname in self.env_dict', 'envdef.simplify_repl', 'envdef.discard', 'self.keep_inline_math', ""self.strict_latex_spaces['after-comment']"", ""self.strict_latex_spaces['after-comment']"", ""'l2tobj' in getfullargspec(simplify_repl)[0]"", 'mac.simplify_repl', 'mac.discard', 'node.nodeoptarg']",___________________,0.0
"def get_import_status(self, sis_import):
    """"""
        Get the status of an already created SIS import.

        https://canvas.instructure.com/doc/api/sis_imports.html#method.sis_imports_api.show
        """"""
    if '<MASK>':
        raise MissingAccountID()
    url = SIS_IMPORTS_API.format(self._canvas_account_id) + '/{}.json'.format(sis_import.import_id)
    return SISImportModel(data=self._get_resource(url))",False,['not self._canvas_account_id'],___________________,0.0
"def add_record(self, schema, _bump_stack_level=False):
    """""" Add record class to record store for retrieval at record load time.

            Can be used as a class decorator
        """"""
    full_name = get_full_name(schema)
    has_namespace = '.' in full_name
    self._force_add(full_name, schema, _bump_stack_level, _raise_on_existing=has_namespace)
    if '<MASK>':
        self._force_add(schema.__name__, schema, _bump_stack_level)
    return schema",False,['has_namespace and schema.__name__ not in self._schema_map'],___________________,0.0
"def update(self, deviceUid, metadata=None, deviceInfo=None, status=None):
    """"""
        Update an existing device
        """"""
    if '<MASK>':
        deviceUid = DeviceUid(**deviceUid)
    deviceUrl = 'api/v0002/device/types/%s/devices/%s' % (deviceUid.typeId, deviceUid.deviceId)
    data = {'status': status, 'deviceInfo': deviceInfo, 'metadata': metadata}
    r = self._apiClient.put(deviceUrl, data)
    if '<MASK>':
        return Device(apiClient=self._apiClient, **r.json())
    else:
        raise ApiException(r)",False,"['not isinstance(deviceUid, DeviceUid) and isinstance(deviceUid, dict)', 'r.status_code == 200']",___________________,0.0
"def _fill_enclosure(self, enclosure: Dict[RtlSignalBase, HdlStatement]) -> None:
    """"""
        :attention: enclosure has to be discoverd first use _discover_enclosure()  method
        """"""
    select = []
    outputs = self._outputs
    for e in enclosure.keys():
        if '<MASK>':
            select.append(e)
    for (_, stms), e in zip(self.cases, self._case_enclosed_for):
        fill_stm_list_with_enclosure(self, e, stms, select, enclosure)
        e.update(select)
    t = self.switchOn._dtype
    default_required = len(self.cases) < t.domain_size()
    if '<MASK>':
        self.default = fill_stm_list_with_enclosure(self, self._default_enclosed_for, self.default, select, enclosure)
        self._default_enclosed_for.update(select)
    self._enclosed_for.update(select)",False,"['self.default is not None or default_required', 'e in outputs']",___________________,0.0
"def get_pull_requests(app, repo_config):
    """"""Last 30 pull requests from a repository.

    :param app: Flask app
    :param repo_config: dict with ``github_repo`` key

    :returns: id for a pull request
    """"""
    response = get_api_response(app, repo_config, '/repos/{repo_name}/pulls')
    if '<MASK>':
        raise Exception('Unable to get pull requests: status code {}'.format(response.status_code))
    return (item for item in response.json)",False,['not response.ok'],___________________,0.0
"def volume_size_total(self, volume, human_readable=True):
    """"""Total size of volume""""""
    volume = self._get_volume(volume)
    if '<MASK>':
        return_data = int(volume['size']['total'])
        if '<MASK>':
            return SynoFormatHelper.bytes_to_readable(return_data)
        else:
            return return_data",False,"['volume is not None', 'human_readable']",___________________,0.0
"def visit_attribute(self, node):
    """"""check if the getattr is an access to a class member
        if so, register it. Also check for access to protected
        class member from outside its class (but ignore __special__
        methods)
        """"""
    if '<MASK>':
        self._accessed.set_accessed(node)
        return
    if '<MASK>':
        return
    self._check_protected_attribute_access(node)",False,"['self._uses_mandatory_method_param(node)', ""not self.linter.is_message_enabled('protected-access')""]",___________________,0.0
"def _add_https(self, q):
    """"""for push, pull, and other api interactions, the user can optionally
           define a custom registry. If the registry name doesn't include http
           or https, add it.
 
           Parameters
           ==========
           q: the parsed image query (names), including the original
        """"""
    if '<MASK>':
        if '<MASK>':
            q['registry'] = 'http://%s' % q['registry']
        elif '<MASK>':
            q['registry'] = 'https://%s' % q['registry']
        else:
            prefix = 'https://'
            nohttps = os.environ.get('SREGISTRY_REGISTRY_NOHTTPS')
            if '<MASK>':
                prefix = 'http://'
            q['registry'] = '%s%s' % (prefix, q['registry'])
    return q",False,"[""not q['registry'].startswith('http')"", ""q['original'].startswith('http:')"", ""q['original'].startswith('https:')"", 'nohttps != None']",___________________,0.0
"def filter_unused_variable(line, previous_line=''):
    """"""Return line if used, otherwise return None.""""""
    if '<MASK>':
        return re.sub(' as \\w+:$', ':', line, count=1)
    elif '<MASK>':
        return line
    elif '<MASK>':
        split_line = line.split('=')
        assert len(split_line) == 2
        value = split_line[1].lstrip()
        if '<MASK>':
            return line
        if '<MASK>':
            value = 'pass' + get_line_ending(line)
        return get_indentation(line) + value
    else:
        return line",False,"['re.match(EXCEPT_REGEX, line)', 'multiline_statement(line, previous_line)', ""line.count('=') == 1"", ""',' in split_line[0]"", 'is_literal_or_name(value)']",___________________,0.0
"def make_logging_handlers_and_tools(self, multiproc=False):
    """"""Creates logging handlers and redirects stdout.""""""
    log_stdout = self.log_stdout
    if '<MASK>':
        log_stdout = False
    if '<MASK>':
        if '<MASK>':
            proc_log_config = self._mp_config
        else:
            proc_log_config = self._sp_config
        if '<MASK>':
            if '<MASK>':
                new_dict = self._handle_dict_config(proc_log_config)
                dictConfig(new_dict)
            else:
                parser = self._handle_config_parsing(proc_log_config)
                memory_file = self._parser_to_string_io(parser)
                fileConfig(memory_file, disable_existing_loggers=False)
    if '<MASK>':
        std_name, std_level = self.log_stdout
        stdout = StdoutToLogger(std_name, log_level=std_level)
        stdout.start()
        self._tools.append(stdout)",False,"['sys.stdout is self._stdout_to_logger', 'self.log_config', 'log_stdout', 'multiproc', 'proc_log_config', 'isinstance(proc_log_config, dict)']",___________________,0.0
"def sign_input_at(self, start_index, private_key):
    """"""
        Signs the input at the specified index.

        :param start_index:
            The index of the first input transaction.

            If necessary, the resulting signature will be split across
            multiple transactions automatically (i.e., if an input has
            ``security_level=2``, you still only need to call
            :py:meth:`sign_input_at` once).

        :param private_key:
            The private key that will be used to generate the signature.

            .. important::
                Be sure that the private key was generated using the
                correct seed, or the resulting signature will be
                invalid!
        """"""
    if '<MASK>':
        raise RuntimeError('Cannot sign inputs until bundle is finalized.')
    private_key.sign_input_transactions(self, start_index)",False,['not self.hash'],___________________,0.0
"def add_deformation(chn_names, data):
    """"""From circularity, compute the deformation

    This method is useful for RT-DC data sets that contain
    the circularity but not the deformation.
    """"""
    if '<MASK>':
        for ii, ch in enumerate(chn_names):
            if '<MASK>':
                chn_names.append('deformation')
                data.append(1 - data[ii])
    return (chn_names, data)",False,"[""'deformation' not in chn_names"", ""ch == 'circularity'""]",(((((((((((((((((((,0.0
"def cli_put_object(context, path):
    """"""
    Performs a PUT on the object.

    See :py:mod:`swiftly.cli.put` for context usage information.

    See :py:class:`CLIPut` for more information.
    """"""
    if '<MASK>':
        raise ReturnCode('context.different will not work properly with context.encrypt since encryption may change the object size')
    put_headers = dict(context.headers)
    if '<MASK>':
        body = ''
        put_headers['content-length'] = '0'
    elif '<MASK>':
        stdin = context.io_manager.get_stdin()
        if '<MASK>':

            def reader():
                while True:
                    chunk = stdin.read(65536)
                    if '<MASK>':
                        yield chunk
                    else:
                        return
            segment_body = FileLikeIter(reader(), context.segment_size)
            prefix = _create_container(context, path, time.time(), 0)
            new_context = context.copy()
            new_context.stdin_segmentation = False
            new_context.stdin = segment_body
            new_context.headers = dict(context.headers)
            segment_n = 0
            path2info = {}
            while not segment_body.is_empty():
                segment_path = _get_segment_path(prefix, segment_n)
                etag = cli_put_object(new_context, segment_path)
                size = segment_body.limit - segment_body.left
                path2info[segment_path] = (size, etag)
                segment_body.reset_limit()
                segment_n += 1
            body = _get_manifest_body(context, prefix, path2info, put_headers)
        elif '<MASK>':
            body = context.stdin
        else:
            body = stdin
    elif '<MASK>':
        if '<MASK>':
            raise ReturnCode('putting object %r: Cannot use encryption and context.seek' % path)
        body = open(context.input_, 'rb')
        body.seek(context.seek)
    else:
        l_mtime = os.path.getmtime(context.input_)
        l_size = os.path.getsize(context.input_)
        put_headers['content-length'] = str(l_size)
        if '<MASK>':
            r_mtime = None
            r_size = None
            with context.client_manager.with_client() as client:
                status, reason, headers, contents = client.head_object(*path.split('/', 1), headers=context.headers, query=context.query, cdn=context.cdn)
                if '<MASK>':
                    contents.read()
            if '<MASK>':
                r_mtime = headers.get('x-object-meta-mtime')
                if '<MASK>':
                    try:
                        r_mtime = float(r_mtime)
                    except ValueError:
                        r_mtime = None
                r_size = headers.get('content-length')
                if '<MASK>':
                    try:
                        r_size = int(r_size)
                    except ValueError:
                        r_size = None
            elif '<MASK>':
                raise ReturnCode('could not head %r for conditional check; skipping put: %s %s' % (path, status, reason))
            if '<MASK>':
                return
            if '<MASK>':
                return
        put_headers['x-object-meta-mtime'] = '%f' % l_mtime
        size = os.path.getsize(context.input_)
        if '<MASK>':
            if '<MASK>':
                raise ReturnCode('putting object %r: Cannot use encryption for objects greater than the segment size' % path)
            prefix = _create_container(context, path, l_mtime, size)
            conc = Concurrency(context.concurrency)
            start = 0
            segment = 0
            path2info = {}
            while start < size:
                new_context = context.copy()
                new_context.headers = dict(context.headers)
                new_context.headers['content-length'] = str(min(size - start, context.segment_size))
                new_context.seek = start
                new_path = _get_segment_path(prefix, segment)
                for ident, (exc_type, exc_value, exc_tb, result) in six.iteritems(conc.get_results()):
                    if '<MASK>':
                        conc.join()
                        raise exc_value
                    path2info[ident] = result
                conc.spawn(new_path, cli_put_object, new_context, new_path)
                segment += 1
                start += context.segment_size
            conc.join()
            for ident, (exc_type, exc_value, exc_tb, result) in six.iteritems(conc.get_results()):
                if '<MASK>':
                    raise exc_value
                path2info[ident] = result
            body = _get_manifest_body(context, prefix, path2info, put_headers)
        else:
            body = open(context.input_, 'rb')
    with context.client_manager.with_client() as client:
        if '<MASK>':
            content_length = put_headers.get('content-length')
            if '<MASK>':
                content_length = int(content_length)
            if '<MASK>':
                body = FileLikeIter(aes_encrypt(context.encrypt, body, preamble=AES256CBC, chunk_size=getattr(client, 'chunk_size', 65536), content_length=content_length))
            else:
                body = FileLikeIter(aes_encrypt(context.encrypt, FileLikeIter([body]), preamble=AES256CBC, chunk_size=getattr(client, 'chunk_size', 65536), content_length=content_length))
            if '<MASK>':
                del put_headers['content-length']
        container, obj = path.split('/', 1)
        status, reason, headers, contents = client.put_object(container, obj, body, headers=put_headers, query=context.query, cdn=context.cdn)
        if '<MASK>':
            contents = contents.read()
    if '<MASK>':
        raise ReturnCode('putting object %r: %s %s %r' % (path, status, reason, contents))
    if '<MASK>':
        content_length = put_headers.get('content-length')
        etag = headers.get('etag')
        if '<MASK>':
            content_length = int(content_length)
        else:
            with context.client_manager.with_client() as client:
                container, obj = path.split('/', 1)
                status, reason, headers, contents = client.head_object(container, obj, cdn=context.cdn)
                if '<MASK>':
                    contents = contents.read()
            if '<MASK>':
                raise ReturnCode('heading object %r: %s %s %r' % (path, status, reason, contents))
            content_length = headers.get('content-length')
            etag = headers.get('etag')
            if '<MASK>':
                content_length = int(content_length)
        return (content_length, etag)
    if '<MASK>':
        return headers.get('etag')",False,"['context.different and context.encrypt', 'context.empty', 'status // 100 != 2', 'context.seek is not None', 'context.stdin is not None', ""not context.input_ or context.input_ == '-'"", 'context.encrypt', ""hasattr(contents, 'read')"", 'content_length and etag', 'context.stdin_segmentation', 'context.seek is not None', 'content_length', ""hasattr(body, 'read')"", ""'content-length' in put_headers"", 'status // 100 != 2', 'content_length', ""hasattr(context, 'stdin')"", 'context.encrypt', 'context.newer or context.different', 'size > context.segment_size', ""hasattr(contents, 'read')"", 'status // 100 == 2', 'context.newer and r_mtime is not None or l_mtime <= r_mtime', 'context.different and r_mtime is not None and (l_mtime == r_mtime) and (r_size is not None) and (l_size == r_size)', 'context.encrypt', 'chunk', ""hasattr(contents, 'read')"", 'r_mtime', 'r_size', 'status != 404', 'exc_value', 'exc_value']",___________________,0.0
"def read_header(filename, return_idxs=False):
    """""" Read blimpy header and return a Python dictionary of key:value pairs

    Args:
        filename (str): name of file to open

    Optional args:
        return_idxs (bool): Default False. If true, returns the file offset indexes
                            for values

    returns

    """"""
    with open(filename, 'rb') as fh:
        header_dict = {}
        header_idxs = {}
        keyword, value, idx = read_next_header_keyword(fh)
        try:
            assert keyword == b'HEADER_START'
        except AssertionError:
            raise RuntimeError('Not a valid blimpy file.')
        while True:
            keyword, value, idx = read_next_header_keyword(fh)
            if '<MASK>':
                break
            else:
                header_dict[keyword] = value
                header_idxs[keyword] = idx
    if '<MASK>':
        return header_idxs
    else:
        return header_dict",False,"['return_idxs', ""keyword == b'HEADER_END'""]",___________________,0.0
"def limited_join(sep, items, max_chars=30, overflow_marker='...'):
    """"""Join a number of strings to one, limiting the length to *max_chars*.

    If the string overflows this limit, replace the last fitting item by
    *overflow_marker*.

    Returns: joined_string
    """"""
    full_str = sep.join(items)
    if '<MASK>':
        return full_str
    n_chars = 0
    n_items = 0
    for j, item in enumerate(items):
        n_chars += len(item) + len(sep)
        if '<MASK>':
            n_items += 1
        else:
            break
    return sep.join(list(items[:n_items]) + [overflow_marker])",False,"['len(full_str) < max_chars', 'n_chars < max_chars - len(overflow_marker)']",___________________,0.0
"def slice_to_SLICE(sliceVals, width):
    """"""convert python slice to value of SLICE hdl type""""""
    if '<MASK>':
        raise NotImplementedError()
    start = sliceVals.start
    stop = sliceVals.stop
    if '<MASK>':
        start = INT.fromPy(width)
    else:
        start = toHVal(sliceVals.start)
    if '<MASK>':
        stop = INT.fromPy(0)
    else:
        stop = toHVal(sliceVals.stop)
    startIsVal = isinstance(start, Value)
    stopIsVal = isinstance(stop, Value)
    indexesAreValues = startIsVal and stopIsVal
    if '<MASK>':
        updateTime = max(start.updateTime, stop.updateTime)
    else:
        updateTime = -1
    return Slice.getValueCls()((start, stop), SLICE, 1, updateTime)",False,"['sliceVals.step is not None', 'sliceVals.start is None', 'sliceVals.stop is None', 'indexesAreValues']",___________________,0.0
"def save(self, **kwargs):
    """"""
        Save the model and commit all child relations.
        """"""
    child_relation_names = [rel.get_accessor_name() for rel in get_all_child_relations(self)]
    child_m2m_field_names = [field.name for field in get_all_child_m2m_relations(self)]
    update_fields = kwargs.pop('update_fields', None)
    if '<MASK>':
        real_update_fields = None
        relations_to_commit = child_relation_names
        m2m_fields_to_commit = child_m2m_field_names
    else:
        real_update_fields = []
        relations_to_commit = []
        m2m_fields_to_commit = []
        for field in update_fields:
            if '<MASK>':
                relations_to_commit.append(field)
            elif '<MASK>':
                m2m_fields_to_commit.append(field)
            else:
                real_update_fields.append(field)
    super(ClusterableModel, self).save(update_fields=real_update_fields, **kwargs)
    for relation in relations_to_commit:
        getattr(self, relation).commit()
    for field in m2m_fields_to_commit:
        getattr(self, field).commit()",False,"['update_fields is None', 'field in child_relation_names', 'field in child_m2m_field_names']",___________________,0.0
"def encode(x):
    for encoded in hex_regex.findall(x):
        if '<MASK>':
            x = x.replace(encoded, bytes(str(encoded).replace('\\\\x', '\\x'), 'utf-8').decode('unicode_escape'))
        else:
            x = x.replace(encoded, str(encoded).replace('\\\\x', '\\x').decode('string_escape'))
    return x",False,"['sys.version_info >= (3, 0)']",(((((((((((((((((((,0.0
"def walk(self, node, _done=None):
    """"""walk on the tree from <node>, getting callbacks from handler""""""
    if '<MASK>':
        _done = set()
    if '<MASK>':
        raise AssertionError((id(node), node, node.parent))
    _done.add(node)
    self.visit(node)
    for child_node in node.get_children():
        assert child_node is not node
        self.walk(child_node, _done)
    self.leave(node)
    assert node.parent is not node",False,"['_done is None', 'node in _done']","__node,_,_,_,_,_,_,_,_",0.0
"def nslookup(cls):
    """"""
        Implementation of UNIX nslookup.
        """"""
    try:
        if '<MASK>':
            if '<MASK>':
                request = PyFunceble.socket.getaddrinfo(PyFunceble.INTERN['to_test'], 80, 0, 0, PyFunceble.socket.IPPROTO_TCP)
                for sequence in request:
                    PyFunceble.INTERN['current_test_data']['nslookup'].append(sequence[-1][0])
            else:
                request = PyFunceble.socket.gethostbyaddr(PyFunceble.INTERN['to_test'])
                PyFunceble.INTERN['current_test_data']['nslookup']['hostname'] = request[0]
                PyFunceble.INTERN['current_test_data']['nslookup']['aliases'] = request[1]
                PyFunceble.INTERN['current_test_data']['nslookup']['ips'] = request[2]
        elif '<MASK>':
            PyFunceble.socket.getaddrinfo(PyFunceble.INTERN['to_test'], 80, 0, 0, PyFunceble.socket.IPPROTO_TCP)
        else:
            PyFunceble.socket.gethostbyaddr(PyFunceble.INTERN['to_test'])
        return True
    except (OSError, PyFunceble.socket.herror, PyFunceble.socket.gaierror):
        return False",False,"[""'current_test_data' in PyFunceble.INTERN"", 'not Check().is_ip_valid()', 'not Check().is_ip_valid()']",___________________,0.0
"def parse_addr_list_cmd(proc, args, listsize=40):
    """"""Parses arguments for the ""list"" command and returns the tuple:
    (filename, first line number, last line number)
    or sets these to None if there was some problem.""""""
    text = proc.current_command[len(args[0]) + 1:].strip()
    if '<MASK>':
        if '<MASK>':
            location = resolve_address_location(proc, '.')
            return (location.path, location.line_number, True, location.line_number + listsize, True, location.method)
        if '<MASK>':
            proc.errmsg(""Don't have previous list location"")
            return INVALID_PARSE_LIST
        filename = proc.list_filename
        if '<MASK>':
            first = max(0, proc.list_offset - 1)
        elif '<MASK>':
            if '<MASK>':
                proc.errmsg('Already at start of %s.' % proc.list_filename)
                return INVALID_PARSE_LIST
            first = max(1, proc.list_lineno - 2 * listsize - 1)
        elif '<MASK>':
            first = proc.list_offset + 1
            last = first + listsize - 1
            return (filename, first, True, last, True, proc.list_object)
        last = first + listsize - 1
        return (filename, first, True, last, True, proc.list_object)
    else:
        try:
            list_range = build_arange(text)
        except LocationError as e:
            proc.errmsg('Error in parsing list range at or around:')
            proc.errmsg(e.text)
            proc.errmsg(e.text_cursor)
            return INVALID_PARSE_LIST
        except ScannerError as e:
            proc.errmsg('Lexical error in parsing list range at or around:')
            proc.errmsg(e.text)
            proc.errmsg(e.text_cursor)
            return INVALID_PARSE_LIST
        if '<MASK>':
            assert isinstance(list_range.last, Location)
            location = resolve_address_location(proc, list_range.last)
            if '<MASK>':
                return INVALID_PARSE_LIST
            last = location.line_number
            if '<MASK>':
                raise RuntimeError(""We don't handle ending offsets"")
            else:
                first = max(1, last - listsize)
            return (location.path, first, False, last, False, location.method)
        elif '<MASK>':
            first = list_range.first
            location = resolve_address_location(proc, list_range.last)
            if '<MASK>':
                return INVALID_PARSE_LIST
            filename = location.path
            last = location.line_number
            if '<MASK>':
                last = first + last
            return (location.path, first, False, last, location.is_address, location.method)
        else:
            assert isinstance(list_range.first, Location)
            location = resolve_address_location(proc, list_range.first)
            if '<MASK>':
                return INVALID_PARSE_LIST
            first = location.line_number
            first_is_addr = location.is_address
            last_is_addr = False
            last = list_range.last
            if '<MASK>':
                assert last[0] in ('+', '*')
                last_is_addr = last[0] == '*'
                if '<MASK>':
                    last = int(last[1:])
                else:
                    last = first + int(last[1:])
            elif '<MASK>':
                last_is_addr = True
                last = first + listsize
            elif '<MASK>':
                last = first + last
            return (location.path, first, first_is_addr, last, last_is_addr, location.method)
        pass
    return",False,"[""text in frozenset(('', '.', '+', '-'))"", ""text == '.'"", 'proc.list_offset is None', ""text == '+'"", 'list_range.first is None', ""text == '-'"", 'not location', 'location.is_address', 'isinstance(list_range.first, int)', 'proc.list_lineno == 1 + listsize', ""text == ''"", 'not location', 'not location.is_address and last < first', 'not location', 'isinstance(last, str)', 'last_is_addr', 'not last', 'last < first']",___________________,0.0
"def tryToMerge(procA: HWProcess, procB: HWProcess):
    """"""
    Try merge procB into procA

    :raise IncompatibleStructure: if merge is not possible
    :attention: procA is now result if merge has succeed
    :return: procA which is now result of merge
    """"""
    if '<MASK>':
        raise IncompatibleStructure()
    procA.statements = HdlStatement._merge_statement_lists(procA.statements, procB.statements)
    procA.outputs.extend(procB.outputs)
    procA.inputs.extend(procB.inputs)
    procA.sensitivityList.extend(procB.sensitivityList)
    return procA",False,"['checkIfIsTooSimple(procA) or checkIfIsTooSimple(procB) or areSetsIntersets(procA.outputs, procB.sensitivityList) or areSetsIntersets(procB.outputs, procA.sensitivityList) or (not HdlStatement._is_mergable_statement_list(procA.statements, procB.statements))']",___________________,0.0
"def get(self, requestId):
    """"""
        Gets details of a device management request.
        It accepts requestId (string) as parameters
        In case of failure it throws APIException
        """"""
    url = MgmtRequests.mgmtSingleRequest % requestId
    r = self._apiClient.get(url)
    if '<MASK>':
        return r.json()
    else:
        raise ApiException(r)",False,['r.status_code == 200'],(((((((((((((((((((,0.0
"def _compute_peaks_or_valleys_of_first_derivative(s, do_peaks=True):
    """"""
    Takes a spectrogram and returns a 2D array of the form:

    0 0 0 1 0 0 1 0 0 0 1   <-- Frequency 0
    0 0 1 0 0 0 0 0 0 1 0   <-- Frequency 1
    0 0 0 0 0 0 1 0 1 0 0   <-- Frequency 2
    *** Time axis *******

    Where a 1 means that the value in that time bin in the spectrogram corresponds to
    a peak/valley in the first derivative.

    This function is used as part of the ASA algorithm and is not meant to be used publicly.
    """"""
    gradient = np.nan_to_num(np.apply_along_axis(np.gradient, 1, s), copy=False)
    threshold = np.squeeze(np.nanmean(gradient, axis=1) + np.nanstd(gradient, axis=1))
    half_window = 4
    if '<MASK>':
        indexes = [signal.argrelextrema(gradient[i, :], np.greater, order=half_window)[0] for i in range(gradient.shape[0])]
    else:
        indexes = [signal.argrelextrema(gradient[i, :], np.less, order=half_window)[0] for i in range(gradient.shape[0])]
    extrema = np.zeros(s.shape)
    for row_index, index_array in enumerate(indexes):
        for col_index in index_array:
            if '<MASK>':
                extrema[row_index, col_index] = 1
            elif '<MASK>':
                extrema[row_index, col_index] = 1
    return (extrema, gradient)",False,"['do_peaks', 'do_peaks and gradient[row_index, col_index] > threshold[row_index]', 'not do_peaks']",___________________,0.0
"def get_courses_for_regid(self, regid, params={}):
    """"""
        Return a list of courses for the passed regid.

        https://canvas.instructure.com/doc/api/courses.html#method.courses.index
        """"""
    self._as_user = regid
    data = self._get_resource('/api/v1/courses', params=params)
    self._as_user = None
    courses = []
    for datum in data:
        if '<MASK>':
            courses.append(CanvasCourse(data=datum))
        else:
            courses.append(self.get_course(datum['id'], params))
    return courses",False,"[""'sis_course_id' in datum""]",___________________,0.0
"def _get_client_creds_from_request(self, request):
    """"""Return client credentials based on the current request.

        According to the rfc6749, client MAY use the HTTP Basic authentication
        scheme as defined in [RFC2617] to authenticate with the authorization
        server. The client identifier is encoded using the
        ""application/x-www-form-urlencoded"" encoding algorithm per Appendix B,
        and the encoded value is used as the username; the client password is
        encoded using the same algorithm and used as the password. The
        authorization server MUST support the HTTP Basic authentication scheme
        for authenticating clients that were issued a client password.
        See `Section 2.3.1`_.

        .. _`Section 2.3.1`: https://tools.ietf.org/html/rfc6749#section-2.3.1
        """"""
    if '<MASK>':
        return (request.client_id, request.client_secret)
    auth = request.headers.get('Authorization')
    if '<MASK>':
        return (auth['username'], auth['password'])
    return (None, None)",False,"['request.client_id is not None', 'isinstance(auth, dict)']","_,,,,,,,,,,,,,,,,,,",0.0
"def add_file(self, filename, patch_name=None, ignore=False):
    """""" Add file to the patch with patch_name.
        If patch_name is None or empty the topmost patch will be used.
        Adding an already added patch will raise an QuiltError if ignore is
        False.
        """"""
    file = File(filename)
    if '<MASK>':
        patch = Patch(patch_name)
    else:
        patch = self.db.top_patch()
        if '<MASK>':
            raise NoAppliedPatch(self.db)
    exists = self._file_in_patch(filename, patch, ignore)
    if '<MASK>':
        return
    self._file_in_next_patches(filename, patch)
    if '<MASK>':
        raise QuiltError('Cannot add symbolic link %s' % filename)
    self._backup_file(file, patch)
    if '<MASK>':
        os.chmod(filename, file.get_mode() | stat.S_IWUSR | stat.S_IRUSR)
    self.file_added(file, patch)",False,"['patch_name', 'exists', 'file.is_link()', 'file.exists()', 'not patch']",___________________,0.0
"def handle_message(self, msg):
    """"""manage message of different types, and colorize output
        using ansi escape codes
        """"""
    if '<MASK>':
        color, style = self._get_decoration('S')
        if '<MASK>':
            modsep = colorize_ansi('************* Module %s' % msg.module, color, style)
        else:
            modsep = colorize_ansi('************* %s' % msg.module, color, style)
        self.writeln(modsep)
        self._modules.add(msg.module)
    color, style = self._get_decoration(msg.C)
    msg = msg._replace(**{attr: colorize_ansi(getattr(msg, attr), color, style) for attr in ('msg', 'symbol', 'category', 'C')})
    self.write_message(msg)",False,"['msg.module not in self._modules', 'msg.module']",___________________,0.0
"def run(parser, args, output_file=sys.stdout):
    """"""Run command line interface.""""""
    result_storage = {}
    if '<MASK>':
        args.store.seek(0)
        try:
            result_storage = pickle.load(args.store)
        except EOFError:
            pass
        args.store.close()
    machine = MachineModel(args.machine.name, args=args)
    if '<MASK>':
        code = str(args.code_file.read())
        code = clean_code(code)
        kernel = KernelCode(code, filename=args.code_file.name, machine=machine, keep_intermediates=not args.clean_intermediates)
    else:
        description = str(args.code_file.read())
        kernel = KernelDescription(yaml.load(description, Loader=yaml.Loader), machine=machine)
    required_consts = [v[1] for v in kernel.variables.values() if v[1] is not None]
    required_consts += [[l['start'], l['stop']] for l in kernel.get_loop_stack()]
    required_consts = [i for l in required_consts for i in l]
    required_consts = set([i for l in required_consts for i in l.free_symbols])
    if '<MASK>':
        define_dict = {}
        for name, values in args.define:
            if '<MASK>':
                define_dict[name] = [[name, v] for v in values]
                continue
            for v in values:
                if '<MASK>':
                    define_dict[name].append([name, v])
        define_product = list(itertools.product(*list(define_dict.values())))
        if '<MASK>':
            raise ValueError('Not all constants have been defined. Required are: {}'.format(required_consts))
    else:
        define_product = [{}]
    for define in define_product:
        kernel.clear_state()
        for k, v in define:
            kernel.set_constant(k, v)
        for model_name in uniquify(args.pmodel):
            print('{:^80}'.format(' kerncraft '), file=output_file)
            print('{:<40}{:>40}'.format(args.code_file.name, '-m ' + args.machine.name), file=output_file)
            print(' '.join(['-D {} {}'.format(k, v) for k, v in define]), file=output_file)
            print('{:-^80}'.format(' ' + model_name + ' '), file=output_file)
            if '<MASK>':
                if '<MASK>':
                    kernel.print_kernel_code(output_file=output_file)
                    print('', file=output_file)
                kernel.print_variables_info(output_file=output_file)
                kernel.print_kernel_info(output_file=output_file)
            if '<MASK>':
                kernel.print_constants_info(output_file=output_file)
            model = getattr(models, model_name)(kernel, machine, args, parser)
            model.analyze()
            model.report(output_file=output_file)
            kernel_name = os.path.split(args.code_file.name)[1]
            if '<MASK>':
                result_storage[kernel_name] = {}
            if '<MASK>':
                result_storage[kernel_name][tuple(kernel.constants.items())] = {}
            result_storage[kernel_name][tuple(kernel.constants.items())][model_name] = model.results
            print('', file=output_file)
        if '<MASK>':
            temp_name = args.store.name + '.tmp'
            with open(temp_name, 'wb+') as f:
                pickle.dump(result_storage, f)
            shutil.move(temp_name, args.store.name)",False,"['args.store', 'not args.kernel_description', 'len(required_consts) > 0', 'set(required_consts).difference(set([symbol_pos_int(k) for k in define_dict.keys()]))', 'args.store', 'name not in define_dict', 'args.verbose > 1', 'args.verbose > 0', 'kernel_name not in result_storage', 'tuple(kernel.constants.items()) not in result_storage[kernel_name]', 'v not in define_dict[name]', 'not args.kernel_description']",___________________,0.0
"def _build_choices(self):
    """"""Build choices list runtime using 'sitetree_tree' tag""""""
    tree_token = u'sitetree_tree from ""%s"" template ""%s""' % (self.tree, self.template)
    context_kwargs = {'current_app': 'admin'}
    context = template.Context(context_kwargs) if VERSION >= (1, 8) else template.Context(**context_kwargs)
    context.update({'request': object()})
    choices_str = sitetree_tree(Parser(None), Token(token_type=TOKEN_BLOCK, contents=tree_token)).render(context)
    tree_choices = [(ITEMS_FIELD_ROOT_ID, self.root_title)]
    for line in choices_str.splitlines():
        if '<MASK>':
            splitted = line.split(':::')
            tree_choices.append((splitted[0], mark_safe(splitted[1])))
    return tree_choices",False,['line.strip()'],___________________,0.0
"def _read_comment(ctx: ReaderContext) -> LispReaderForm:
    """"""Read (and ignore) a single-line comment from the input stream.
    Return the next form after the next line break.""""""
    reader = ctx.reader
    start = reader.advance()
    assert start == ';'
    while True:
        token = reader.peek()
        if '<MASK>':
            reader.advance()
            return _read_next(ctx)
        if '<MASK>':
            return ctx.eof
        reader.advance()",False,"['newline_chars.match(token)', ""token == ''""]",_((((((((((((((((((,0.0
"def _build_non_li_content(el, meta_data):
    w_namespace = get_namespace(el, 'w')
    if '<MASK>':
        new_el, visited_nodes = build_table(el, meta_data)
        return (etree.tostring(new_el), visited_nodes)
    elif '<MASK>':
        return (get_element_content(el, meta_data), [el])
    if '<MASK>':
        raise UnintendedTag('Did not expect %s' % el.tag)",False,"[""el.tag == '%stbl' % w_namespace"", 'has_text(el)', ""el.tag == '%sp' % w_namespace""]",___________________,0.0
"def get_body(self):
    """"""Return ""data"" value on self.data

        :return: data to send
        :rtype: str
        """"""
    if '<MASK>':
        return self.default_body
    data = self.data.get('data')
    if '<MASK>':
        return json.dumps(data)
    return data",False,"['self.default_body', 'isinstance(data, dict)']","_body(self):
    """"""
    """"""
    """"""
    """"""
   ",0.0
"def RENEWING(self):
    """"""RENEWING state.""""""
    logger.debug('In state: RENEWING')
    self.current_state = STATE_RENEWING
    if '<MASK>':
        self.script.script_init(self.client.lease, self.current_state)
        self.script.script_go()
    else:
        set_net(self.client.lease)",False,['self.script is not None'],"___________)
    """"""....",0.0
"def _prfx_setattr_(obj, item, value):
    """"""Replacement of __setattr__""""""
    if '<MASK>':
        return setattr(obj, item[2:], value)
    else:
        return super(obj.__class__, obj).__setattr__(item, value)",False,"[""item.startswith('v_')""]","_setattr,_setattr,_setattr,_setattr,_setattr",0.0
"def print_source_location_info(print_fn, filename, lineno, fn_name=None, f_lasti=None, remapped_file=None):
    """"""Print out a source location , e.g. the first line in
    line in:
        (/tmp.py:2 @21):  <module>
        L -- 2 import sys,os
        (trepan3k)
    """"""
    if '<MASK>':
        mess = '(%s:%s remapped %s' % (remapped_file, lineno, filename)
    else:
        mess = '(%s:%s' % (filename, lineno)
    if '<MASK>':
        mess += ' @%d' % f_lasti
        pass
    mess += '):'
    if '<MASK>':
        mess += ' %s' % fn_name
        pass
    print_fn(mess)
    return",False,"['remapped_file', 'f_lasti and f_lasti != -1', ""fn_name and fn_name != '?'""]",___________________,0.0
"def INIT(self):
    """"""INIT state.

        [:rfc:`2131#section-4.4.1`]::

            The client SHOULD wait a random time between one and ten
            seconds to desynchronize the use of DHCP at startup

        .. todo::
           - The initial delay is implemented, but probably is not in other
             implementations. Check what other implementations do.
        """"""
    logger.debug('In state: INIT')
    if '<MASK>':
        self.reset()
    self.current_state = STATE_INIT
    if '<MASK>':
        if '<MASK>':
            delay_before_selecting = gen_delay_selecting()
        else:
            delay_before_selecting = self.delay_before_selecting
    else:
        delay_before_selecting = 0
    self.set_timeout(self.current_state, self.timeout_delay_before_selecting, delay_before_selecting)
    if '<MASK>':
        self.set_timeout(STATE_SELECTING, self.timeout_selecting, self.timeout_select)",False,"['self.current_state is not STATE_PREINIT', 'self.delay_selecting', 'self.timeout_select is not None', 'self.delay_before_selecting is None']",___________________,0.0
"def compile_global_offsets(self, iteration=0, spacing=0):
    """"""
        Return load and store offsets on a virtual address space.

        :param iteration: controls the inner index counter
        :param spacing: sets a spacing between the arrays, default is 0

        All array variables (non scalars) are laid out linearly starting from 0. An optional
        spacing can be set. The accesses are based on this layout.

        The iteration 0 is the first iteration. All loops are mapped to this linear iteration
        space.

        Accesses to scalars are ignored.

        Returned are load and store byte-offset pairs for each iteration.
        """"""
    global_load_offsets = []
    global_store_offsets = []
    if '<MASK>':
        iteration = numpy.arange(iteration.start, iteration.stop, iteration.step, dtype='O')
    else:
        if '<MASK>':
            iteration = [iteration]
        iteration = numpy.array(iteration, dtype='O')
    base_loop_counters = self.global_iterator_to_indices()
    total_length = self.iteration_length()
    assert iteration.max() < self.subs_consts(total_length), 'Iterations go beyond what is possible in the original code ({} vs {}). One common reason, is that the iteration length are unrealistically small.'.format(iteration.max(), self.subs_consts(total_length))
    var_sizes = self.array_sizes(in_bytes=True, subs_consts=True)
    base_offsets = {}
    base = 0
    for var_name, var_size in sorted(var_sizes.items(), key=lambda v: v[0]):
        base_offsets[var_name] = base
        array_total_size = self.subs_consts(var_size + spacing)
        array_total_size = int(array_total_size) + 63 & ~63
        base += array_total_size
    for var_name, var_size in var_sizes.items():
        element_size = self.datatypes_size[self.variables[var_name][0]]
        for r in self.sources.get(var_name, []):
            offset_expr = self.access_to_sympy(var_name, r)
            if '<MASK>':
                continue
            offset = force_iterable(sympy.lambdify(base_loop_counters.keys(), self.subs_consts(offset_expr * element_size + base_offsets[var_name]), numpy))
            global_load_offsets.append(offset)
        for w in self.destinations.get(var_name, []):
            offset_expr = self.access_to_sympy(var_name, w)
            if '<MASK>':
                continue
            offset = force_iterable(sympy.lambdify(base_loop_counters.keys(), self.subs_consts(offset_expr * element_size + base_offsets[var_name]), numpy))
            global_store_offsets.append(offset)
    counter_per_it = [v(iteration) for v in base_loop_counters.values()]
    load_offsets = []
    for o in global_load_offsets:
        load_offsets.append(o(*counter_per_it))
    load_offsets = numpy.asarray(load_offsets).T
    store_offsets = []
    for o in global_store_offsets:
        store_offsets.append(o(*counter_per_it))
    store_offsets = numpy.asarray(store_offsets).T
    store_width = store_offsets.shape[1] if len(store_offsets.shape) > 1 else 0
    dtype = [('load', load_offsets.dtype, (load_offsets.shape[1],)), ('store', store_offsets.dtype, (store_width,))]
    offsets = numpy.empty(max(load_offsets.shape[0], store_offsets.shape[0]), dtype=dtype)
    offsets['load'] = load_offsets
    offsets['store'] = store_offsets
    return offsets",False,"['isinstance(iteration, range)', 'not isinstance(iteration, collections.Sequence)', 'not any([s in base_loop_counters.keys() for s in offset_expr.free_symbols])', 'not any([s in base_loop_counters.keys() for s in offset_expr.free_symbols])']",___________________,0.0
"def convert_outlook_msg(msg_bytes):
    """"""
    Uses the ``msgconvert`` Perl utility to convert an Outlook MS file to
    standard RFC 822 format

    Args:
        msg_bytes (bytes): the content of the .msg file

    Returns:
        A RFC 822 string
    """"""
    if '<MASK>':
        raise ValueError('The supplied bytes are not an Outlook MSG file')
    orig_dir = os.getcwd()
    tmp_dir = tempfile.mkdtemp()
    os.chdir(tmp_dir)
    with open('sample.msg', 'wb') as msg_file:
        msg_file.write(msg_bytes)
    try:
        subprocess.check_call(['msgconvert', 'sample.msg'], stdout=null_file, stderr=null_file)
        eml_path = 'sample.eml'
        with open(eml_path, 'rb') as eml_file:
            rfc822 = eml_file.read()
    except FileNotFoundError:
        raise EmailParserError('Failed to convert Outlook MSG: msgconvert utility not found')
    finally:
        os.chdir(orig_dir)
        shutil.rmtree(tmp_dir)
    return rfc822",False,['not is_outlook_msg(msg_bytes)'],___________________,0.0
"def _next_rdelim(items, pos):
    """"""Return position of next matching closing delimiter.""""""
    for num, item in enumerate(items):
        if '<MASK>':
            break
    else:
        raise RuntimeError('Mismatched delimiters')
    del items[num]
    return item",False,['item > pos'],___________________,0.0
"def get_matching_kwargs(func, kwargs):
    """"""Takes a function and keyword arguments and returns the ones that can be passed.""""""
    args, uses_startstar = _get_argspec(func)
    if '<MASK>':
        return kwargs.copy()
    else:
        matching_kwargs = dict(((k, kwargs[k]) for k in args if k in kwargs))
        return matching_kwargs",False,['uses_startstar'],___________________,0.0
"def __on_erase_background(self, evt):
    """"""Draw the image as background""""""
    if '<MASK>':
        dc = evt.GetDC()
        if '<MASK>':
            dc = wx.ClientDC(self)
            r = self.wx_obj.GetUpdateRegion().GetBox()
            dc.SetClippingRegion(r.x, r.y, r.width, r.height)
        if '<MASK>':
            self.__tile_background(dc)
        else:
            dc.DrawBitmapPoint(self._bitmap.get_bits(), (0, 0))",False,"['self._bitmap', 'not dc', 'self._background_tiling']",___________________,0.0
"def _process_pong(self):
    """"""
        Process PONG sent by server.
        """"""
    if '<MASK>':
        future = self._pongs.pop(0)
        future.set_result(True)
        self._pongs_received += 1
        self._pings_outstanding -= 1",False,['len(self._pongs) > 0'],"_,,,,,,,,,,,,,,,,,,",0.0
"def _handle_subscribed(self, dtype, data, ts):
    """"""Handles responses to subscribe() commands.

        Registers a channel id with the client and assigns a data handler to it.

        :param dtype:
        :param data:
        :param ts:
        :return:
        """"""
    self.log.debug('_handle_subscribed: %s - %s - %s', dtype, data, ts)
    channel_name = data.pop('channel')
    channel_id = data.pop('chanId')
    config = data
    if '<MASK>':
        symbol = config['pair']
        if '<MASK>':
            symbol = symbol[1:]
    elif '<MASK>':
        symbol = config['symbol']
        if '<MASK>':
            symbol = symbol[1:]
    elif '<MASK>':
        symbol = config['key'].split(':')[2][1:]
    else:
        symbol = None
    if '<MASK>':
        channel_name = 'raw_' + channel_name
    self.channel_handlers[channel_id] = self._data_handlers[channel_name]
    if '<MASK>':
        identifier = (channel_name, symbol, config['key'].split(':')[1])
    else:
        identifier = (channel_name, symbol)
    self.channel_handlers[channel_id] = identifier
    self.channel_directory[identifier] = channel_id
    self.channel_directory[channel_id] = identifier
    self.log.info('Subscription succesful for channel %s', identifier)",False,"[""'pair' in config"", ""'prec' in config and config['prec'].startswith('R')"", ""'key' in config"", ""symbol.startswith('t')"", ""'symbol' in config"", ""symbol.startswith('t')"", ""'key' in config""]",___________________,0.0
"def check_causatives(self, case_obj=None, institute_obj=None):
    """"""Check if there are any variants that are previously marked causative

            Loop through all variants that are marked 'causative' for an
            institute and check if any of the variants are present in the
            current case.

            Args:
                case_obj (dict): A Case object
                institute_obj (dict): check across the whole institute

            Returns:
                causatives(iterable(Variant))
        """"""
    institute_id = case_obj['owner'] if case_obj else institute_obj['_id']
    institute_causative_variant_ids = self.get_causatives(institute_id)
    if '<MASK>':
        return []
    if '<MASK>':
        case_causative_ids = set(case_obj.get('causatives', []))
        institute_causative_variant_ids = list(set(institute_causative_variant_ids).difference(case_causative_ids))
    query = self.variant_collection.find({'_id': {'$in': institute_causative_variant_ids}}, {'variant_id': 1})
    positional_variant_ids = [item['variant_id'] for item in query]
    filters = {'variant_id': {'$in': positional_variant_ids}}
    if '<MASK>':
        filters['case_id'] = case_obj['_id']
    else:
        filters['institute'] = institute_obj['_id']
    return self.variant_collection.find(filters)",False,"['len(institute_causative_variant_ids) == 0', 'case_obj', 'case_obj']",___________________,0.0
"def stripped_lines(lines, ignore_comments, ignore_docstrings, ignore_imports):
    """"""return lines with leading/trailing whitespace and any ignored code
    features removed
    """"""
    if '<MASK>':
        tree = astroid.parse(''.join(lines))
        node_is_import_by_lineno = ((node.lineno, isinstance(node, (astroid.Import, astroid.ImportFrom))) for node in tree.body)
        line_begins_import = {lineno: all((is_import for _, is_import in node_is_import_group)) for lineno, node_is_import_group in groupby(node_is_import_by_lineno, key=lambda x: x[0])}
        current_line_is_import = False
    strippedlines = []
    docstring = None
    for lineno, line in enumerate(lines, start=1):
        line = line.strip()
        if '<MASK>':
            if '<MASK>':
                docstring = line[:3]
                line = line[3:]
            if '<MASK>':
                if '<MASK>':
                    docstring = None
                line = ''
        if '<MASK>':
            current_line_is_import = line_begins_import.get(lineno, current_line_is_import)
            if '<MASK>':
                line = ''
        if '<MASK>':
            line = line.split('#', 1)[0].strip()
        strippedlines.append(line)
    return strippedlines",False,"['ignore_imports', 'ignore_docstrings', 'ignore_imports', 'ignore_comments', 'not docstring and any((line.startswith(i) for i in [\'""""""\', ""\'\'\'"", \'r""""""\', ""r\'\'\'""]))', 'docstring', 'current_line_is_import', 'line.endswith(docstring)']",___________________,0.0
"def render_standalone_response(self, request, fragment, **kwargs):
    """"""
        Renders a standalone page as a response for the specified fragment.
        """"""
    if '<MASK>':
        return HttpResponse(status=204)
    html = self.render_to_standalone_html(request, fragment, **kwargs)
    return HttpResponse(html)",False,['fragment is None'],",,,,,,,,,,,,,,,,,,,",0.0
"def _conflictResolveStrategy(self, newValue: set) -> Tuple[Callable[[Value], bool], bool]:
    """"""
        This functions resolves write conflicts for signal

        :param actionSet: set of actions made by process
        """"""
    invalidate = False
    resLen = len(newValue)
    if '<MASK>':
        val, indexes, isEvDependent = newValue
        return (mkArrayUpdater(val, indexes, invalidate), isEvDependent)
    else:
        val, isEvDependent = newValue
        return (mkUpdater(val, invalidate), isEvDependent)",False,['resLen == 3'],___________________,0.0
"def nest_dictionary(flat_dict, separator):
    """""" Nests a given flat dictionary.

    Nested keys are created by splitting given keys around the `separator`.

    """"""
    nested_dict = {}
    for key, val in flat_dict.items():
        split_key = key.split(separator)
        act_dict = nested_dict
        final_key = split_key.pop()
        for new_key in split_key:
            if '<MASK>':
                act_dict[new_key] = {}
            act_dict = act_dict[new_key]
        act_dict[final_key] = val
    return nested_dict",False,['not new_key in act_dict'],_((((((((((((((((((,0.0
"def _trj_store_trajectory(self, traj, only_init=False, store_data=pypetconstants.STORE_DATA, max_depth=None):
    """""" Stores a trajectory to an hdf5 file

        Stores all groups, parameters and results

        """"""
    if '<MASK>':
        self._logger.info('Start storing Trajectory `%s`.' % self._trajectory_name)
    else:
        self._logger.info('Initialising storage or updating meta data of Trajectory `%s`.' % self._trajectory_name)
        store_data = pypetconstants.STORE_NOTHING
    if '<MASK>':
        raise RuntimeError('You want to store a completely new trajectory with name `%s` but this trajectory is already found in file `%s`.Did you try to accidentally overwrite existing data? If you DO want to override existing data, use `overwrite_file=True`.Note that this deletes the whole HDF5 file not just the particular trajectroy therein! ' % (traj.v_name, self._filename))
    self._srvc_check_hdf_properties(traj)
    if '<MASK>':
        self._trajectory_group = self._hdf5file.create_group(where='/', name=self._trajectory_name, title=self._trajectory_name, filters=self._all_get_filters())
    self._trj_store_meta_data(traj)
    if '<MASK>':
        counter = 0
        maximum_display_other = 10
        name_set = set(['parameters', 'config', 'derived_parameters', 'results'])
        for child_name in traj._children:
            if '<MASK>':
                self._logger.info('Storing branch `%s`.' % child_name)
            else:
                if '<MASK>':
                    self._logger.info('Storing branch/node `%s`.' % child_name)
                elif '<MASK>':
                    self._logger.info('To many branches or nodes at root for display. I will not inform you about storing anymore. Branches are stored silently in the background. Do not worry, I will not freeze! Pinky promise!!!')
                counter += 1
            self._tree_store_sub_branch(traj, child_name, store_data=store_data, with_links=True, recursive=True, max_depth=max_depth, hdf5_group=self._trajectory_group)
        self._logger.info('Finished storing Trajectory `%s`.' % self._trajectory_name)
    else:
        self._logger.info('Finished init or meta data update for `%s`.' % self._trajectory_name)
    traj._stored = True",False,"['not only_init', 'not traj._stored and self._trajectory_group is not None', 'self._trajectory_group is None', 'store_data in (pypetconstants.STORE_DATA_SKIPPING, pypetconstants.STORE_DATA, pypetconstants.OVERWRITE_DATA)', 'child_name in name_set', 'counter < maximum_display_other', 'counter == maximum_display_other']",___________________,0.0
"def _srvc_closing_routine(self, closing):
    """"""Routine to close an hdf5 file

        The file is closed only when `closing=True`. `closing=True` means that
        the file was opened in the current highest recursion level. This prevents re-opening
        and closing of the file if `store` or `load` are called recursively.

        """"""
    if '<MASK>':
        f_fd = self._hdf5file.fileno()
        self._hdf5file.flush()
        try:
            os.fsync(f_fd)
            try:
                self._hdf5store.flush(fsync=True)
            except TypeError:
                f_fd = self._hdf5store._handle.fileno()
                self._hdf5store.flush()
                os.fsync(f_fd)
        except OSError as exc:
            errmsg = 'Encountered OSError while flushing file.If you are using Windows, don`t worry! I will ignore the error and try to close the file. Original error: %s' % repr(exc)
            self._logger.debug(errmsg)
        self._hdf5store.close()
        if '<MASK>':
            self._logger.error('Could not close HDF5 file!')
        self._hdf5file = None
        self._hdf5store = None
        self._trajectory_group = None
        self._trajectory_name = None
        self._trajectory_index = None
        self._overview_group_ = None
        self._logger.debug('Closing HDF5 file')
        return True
    else:
        return False",False,"['not self._keep_open and closing and self.is_open', 'self._hdf5file.isopen']","_,,,,,,,,,,,,,,,,,,",0.0
"def get(self, obj_id, byte_range=None):
    """"""Download and return a file object or a specified byte_range from it.
			See HTTP Range header (rfc2616) for possible byte_range formats,
			Examples: ""0-499"" - byte offsets 0-499 (inclusive), ""-500"" - final 500 bytes.""""""
    kwz = dict()
    if '<MASK>':
        kwz['headers'] = dict(Range='bytes={}'.format(byte_range))
    return self(self._api_url_join(obj_id, 'content'), dict(download='true'), raw=True, **kwz)",False,['byte_range'],(((((((((((((((((((,0.0
"def initialize_archive_manager(self, archive_path):
    """"""Initialize the archive manager.

        :param archive_path: path where the archive manager is located
        """"""
    if '<MASK>':
        raise ValueError('Archive manager path cannot be empty')
    if '<MASK>':
        self.archive_manager = perceval.archive.ArchiveManager(archive_path)",False,"[""archive_path == ''"", 'archive_path']",__))))))))))))))))),0.0
"def disks(self):
    """"""Returns all available (internal) disks""""""
    if '<MASK>':
        disks = []
        for disk in self._data['disks']:
            disks.append(disk['id'])
        return disks",False,['self._data is not None'],___________________,0.0
"def upload_stream(stream, server, account, projname, language=None, username=None, password=None, append=False, stage=False):
    """"""
    Given a file-like object containing a JSON stream, upload it to
    Luminoso with the given account name and project name.
    """"""
    client = LuminosoClient.connect(server, username=username, password=password)
    if '<MASK>':
        info = client.post('/projects/' + account, name=projname)
        project_id = info['project_id']
        print('New project ID:', project_id)
    else:
        projects = client.get('/projects/' + account, name=projname)
        if '<MASK>':
            print('No such project exists!')
            return
        if '<MASK>':
            print('Warning: Multiple projects with name ""%s"".  ' % projname, end='')
        project_id = projects[0]['project_id']
        print('Using existing project with id %s.' % project_id)
    project = client.change_path('/projects/' + account + '/' + project_id)
    counter = 0
    for batch in batches(stream, 1000):
        counter += 1
        documents = list(batch)
        project.upload('docs', documents)
        print('Uploaded batch #%d' % counter)
    if '<MASK>':
        print('Calculating.')
        kwargs = {}
        if '<MASK>':
            kwargs = {'language': language}
        job_id = project.post('docs/recalculate', **kwargs)
        project.wait_for(job_id)",False,"['not append', 'not stage', 'len(projects) == 0', 'len(projects) > 1', 'language is not None']",___________________,0.0
"def _prm_write_into_pytable(self, tablename, data, hdf5_group, fullname, **kwargs):
    """"""Stores data as pytable.

        :param tablename:

            Name of the data table

        :param data:

            Data to store

        :param hdf5_group:

            Group node where to store data in hdf5 file

        :param fullname:

            Full name of the `data_to_store`s original container, only needed for throwing errors.

        """"""
    datasize = data.shape[0]
    try:
        description_dict, data_type_dict = self._prm_make_description(data, fullname)
        description_dicts = [{}]
        if '<MASK>':
            new_table_group = self._hdf5file.create_group(where=hdf5_group, name=tablename, filters=self._all_get_filters(kwargs.copy()))
            count = 0
            for innerkey in description_dict:
                val = description_dict[innerkey]
                if '<MASK>':
                    description_dicts.append({})
                    count = 0
                description_dicts[-1][innerkey] = val
                count += 1
            setattr(new_table_group._v_attrs, HDF5StorageService.STORAGE_TYPE, HDF5StorageService.TABLE)
            setattr(new_table_group._v_attrs, HDF5StorageService.SPLIT_TABLE, 1)
            hdf5_group = new_table_group
        else:
            description_dicts = [description_dict]
        for idx, descr_dict in enumerate(description_dicts):
            if '<MASK>':
                tblname = tablename
            else:
                tblname = tablename + '_%d' % idx
            table = self._hdf5file.create_table(where=hdf5_group, name=tblname, description=descr_dict, title=tblname, expectedrows=datasize, filters=self._all_get_filters(kwargs.copy()))
            row = table.row
            for n in range(datasize):
                for key in descr_dict:
                    row[key] = data[key][n]
                row.append()
            if '<MASK>':
                for field_name in data_type_dict:
                    type_description = data_type_dict[field_name]
                    self._all_set_attr(table, field_name, type_description)
                setattr(table._v_attrs, HDF5StorageService.STORAGE_TYPE, HDF5StorageService.TABLE)
            table.flush()
            self._hdf5file.flush()
        if '<MASK>':
            tblname = tablename + '__' + HDF5StorageService.STORAGE_TYPE
            field_names, data_types = list(zip(*data_type_dict.items()))
            data_type_table_dict = {'field_name': field_names, 'data_type': data_types}
            descr_dict, _ = self._prm_make_description(data_type_table_dict, fullname)
            table = self._hdf5file.create_table(where=hdf5_group, name=tblname, description=descr_dict, title=tblname, expectedrows=len(field_names), filters=self._all_get_filters(kwargs))
            row = table.row
            for n in range(len(field_names)):
                for key in data_type_table_dict:
                    row[key] = data_type_table_dict[key][n]
                row.append()
            setattr(table._v_attrs, HDF5StorageService.DATATYPE_TABLE, 1)
            table.flush()
            self._hdf5file.flush()
    except:
        self._logger.error('Failed storing table `%s` of `%s`.' % (tablename, fullname))
        raise",False,"['len(description_dict) > ptpa.MAX_COLUMNS', 'len(description_dict) > ptpa.MAX_COLUMNS', 'idx == 0', 'idx == 0 and len(description_dict) <= ptpa.MAX_COLUMNS', 'count == ptpa.MAX_COLUMNS']",___________________,0.0
"def decodeLength(encoded):
    """"""
    Decodes a variable length value defined in the MQTT protocol.
    This value typically represents remaining field lengths
    """"""
    value = 0
    multiplier = 1
    for i in encoded:
        value += (i & 127) * multiplier
        multiplier *= 128
        if '<MASK>':
            break
    return value",False,['i & 128 != 128'],___________________,0.0
"def export(self, cert, key, type=FILETYPE_PEM, days=100, digest=_UNSPECIFIED):
    """"""
        Export the CRL as a string.

        :param X509 cert: The certificate used to sign the CRL.
        :param PKey key: The key used to sign the CRL.
        :param int type: The export format, either :data:`FILETYPE_PEM`,
            :data:`FILETYPE_ASN1`, or :data:`FILETYPE_TEXT`.
        :param int days: The number of days until the next update of this CRL.
        :param bytes digest: The name of the message digest to use (eg
            ``b""sha256""``).
        :rtype: bytes
        """"""
    if '<MASK>':
        raise TypeError('cert must be an X509 instance')
    if '<MASK>':
        raise TypeError('key must be a PKey instance')
    if '<MASK>':
        raise TypeError('type must be an integer')
    if '<MASK>':
        raise TypeError('digest must be provided')
    digest_obj = _lib.EVP_get_digestbyname(digest)
    if '<MASK>':
        raise ValueError('No such digest method')
    bio = _lib.BIO_new(_lib.BIO_s_mem())
    _openssl_assert(bio != _ffi.NULL)
    sometime = _lib.ASN1_TIME_new()
    _openssl_assert(sometime != _ffi.NULL)
    _lib.X509_gmtime_adj(sometime, 0)
    _lib.X509_CRL_set_lastUpdate(self._crl, sometime)
    _lib.X509_gmtime_adj(sometime, days * 24 * 60 * 60)
    _lib.X509_CRL_set_nextUpdate(self._crl, sometime)
    _lib.X509_CRL_set_issuer_name(self._crl, _lib.X509_get_subject_name(cert._x509))
    sign_result = _lib.X509_CRL_sign(self._crl, key._pkey, digest_obj)
    if '<MASK>':
        _raise_current_error()
    return dump_crl(type, self)",False,"['not isinstance(cert, X509)', 'not isinstance(key, PKey)', 'not isinstance(type, int)', 'digest is _UNSPECIFIED', 'digest_obj == _ffi.NULL', 'not sign_result']",___________________,0.0
"def s3walk(self, basedir, show_dir=None):
    """"""Walk through a S3 directory. This function initiate a walk with a basedir.
       It also supports multiple wildcards.
    """"""
    if '<MASK>':
        show_dir = self.opt.show_dir
    if '<MASK>':
        basedir = basedir[0:-1]
    s3url = S3URL(basedir)
    result = []
    pool = ThreadPool(ThreadUtil, self.opt)
    pool.s3walk(s3url, s3url.get_fixed_path(), s3url.path, result)
    pool.join()
    if '<MASK>':
        path = result[0]['name']
        s3url = S3URL(path)
        result = []
        pool = ThreadPool(ThreadUtil, self.opt)
        pool.s3walk(s3url, s3url.get_fixed_path(), s3url.path, result)
        pool.join()

    def compare(x, y):
        """"""Comparator for ls output""""""
        result = -cmp(x['is_dir'], y['is_dir'])
        if '<MASK>':
            return result
        return cmp(x['name'], y['name'])
    return sorted(result, key=cmp_to_key(compare))",False,"['not show_dir', 'basedir[-1] == PATH_SEP', ""not show_dir and len(result) == 1 and result[0]['is_dir']"", 'result != 0']",___________________,0.0
"def save(self, full=False, force=False):
    """"""
        Saves the current entity to Redis. Will only save changed data by
        default, but you can force a full save by passing ``full=True``.

        If the underlying entity was deleted and you want to re-save the entity,
        you can pass ``force=True`` to force a full re-save of the entity.
        """"""
    was_new = self._new
    if '<MASK>':
        self._before_insert()
    else:
        self._before_update()
    new = self.to_dict()
    ret, data = self._apply_changes(self._last, new, full or self._new or force, is_new=self._new or force)
    self._last = data
    self._new = False
    self._modified = False
    self._deleted = False
    if '<MASK>':
        self._after_insert()
    else:
        self._after_update()
    return ret",False,"['was_new', 'was_new']",___________________,0.0
"def cancel_job_task(self, task_id):
    """"""Cancel the job related to the given task.""""""
    try:
        self._rwlock.writer_acquire()
        job_id = self._tasks.get(task_id, None)
        if '<MASK>':
            self._cancel_job(job_id)
        else:
            logger.warning('Task %s set to be removed was not found', task_id)
    finally:
        self._rwlock.writer_release()",False,['job_id'],"__id,,_id,,_id,,_id,,_id",0.0
"def copy(self):
    """"""
        Returns a new CLIContext instance that is a shallow copy of
        the original, much like dict's copy method.
        """"""
    context = CLIContext()
    for item in dir(self):
        if '<MASK>':
            setattr(context, item, getattr(self, item))
    return context",False,"[""item[0] != '_' and item not in ('copy', 'write_headers')""]","((self,,,,,,,,,,,,,,,,",0.0
"def plot_images(imgs, loc, title=None, channels=1):
    """"""Plot an array of images.

    We assume that we are given a matrix of data whose shape is (n*n, s*s*c) --
    that is, there are n^2 images along the first axis of the array, and each
    image is c squares measuring s pixels on a side. Each row of the input will
    be plotted as a sub-region within a single image array containing an n x n
    grid of images.
    """"""
    n = int(np.sqrt(len(imgs)))
    assert n * n == len(imgs), 'images array must contain a square number of rows!'
    s = int(np.sqrt(len(imgs[0]) / channels))
    assert s * s == len(imgs[0]) / channels, 'images must be square!'
    img = np.zeros(((s + 1) * n - 1, (s + 1) * n - 1, channels), dtype=imgs[0].dtype)
    for i, pix in enumerate(imgs):
        r, c = divmod(i, n)
        img[r * (s + 1):(r + 1) * (s + 1) - 1, c * (s + 1):(c + 1) * (s + 1) - 1] = pix.reshape((s, s, channels))
    img -= img.min()
    img /= img.max()
    ax = plt.gcf().add_subplot(loc)
    ax.xaxis.set_visible(False)
    ax.yaxis.set_visible(False)
    ax.set_frame_on(False)
    ax.imshow(img.squeeze(), cmap=plt.cm.gray)
    if '<MASK>':
        ax.set_title(title)",False,['title'],___________________,0.0
"def get_output(self, stdin_content, stdin):
    """"""
        Try to get output in a separate thread
        """"""
    try:
        if '<MASK>':
            if '<MASK>':
                self.process.stdin.write(bytes(stdin_content, 'utf-8'))
            else:
                self.process.stdin.write(stdin_content)
        self._out = self.process.communicate()[0]
    except (error, IOError):
        self._out = self._in
        pass",False,"['stdin', 'sys.version_info >= (3, 0)']",(((((((((((((((((((,0.0
"def unwatch(self, alias):
    """"""Stop watching a given rule.""""""
    if '<MASK>':
        raise ValueError('Unknown watch alias %s; current set is %r' % (alias, list(self.descriptors.keys())))
    wd = self.descriptors[alias]
    errno = LibC.inotify_rm_watch(self._fd, wd)
    if '<MASK>':
        raise IOError('Failed to close watcher %d: errno=%d' % (wd, errno))
    del self.descriptors[alias]
    del self.requests[alias]
    del self.aliases[wd]",False,"['alias not in self.descriptors', 'errno != 0']",___________________,0.0
"def from_dict(cls, config):
    """"""Create an configuration object from a dictionary.

        Key,value pairs will be used to initialize a task configuration
        object. If 'config' contains invalid configuration parameters
        a `ValueError` exception will be raised.

        :param config: dictionary used to create an instance of this object

        :returns: a task config instance

        :raises ValueError: when an invalid configuration parameter is found
        """"""
    try:
        obj = cls(**config)
    except TypeError as e:
        m = cls.KW_ARGS_ERROR_REGEX.match(str(e))
        if '<MASK>':
            raise ValueError(""unknown '%s' task config parameter"" % m.group(1))
        else:
            raise e
    else:
        return obj",False,['m'],"_,,,,,,,,,,,,,,,,,,",0.0
"def _match_fronts(onset_fronts, offset_fronts, onsets, offsets, debug=False):
    """"""
    Returns a segmentation mask, which looks like this:
    frequency 1: 0 0 4 4 4 4 4 0 0 5 5 5
    frequency 2: 0 4 4 4 4 4 0 0 0 0 5 5
    frequency 3: 0 4 4 4 4 4 4 4 5 5 5 5

    That is, each item in the array is either a 0 (not part of a segment) or a positive
    integer which indicates which segment the sample in that frequency band belongs to.
    """"""

    def printd(*args, **kwargs):
        if '<MASK>':
            print(*args, **kwargs)
    onset_fronts = np.copy(onset_fronts)
    offset_fronts = np.copy(offset_fronts)
    onsets = np.copy(onsets)
    offsets = np.copy(offsets)
    segmentation_mask = np.zeros_like(onset_fronts)
    resulting_onset_fronts = np.copy(onset_fronts)
    printd('    -> Dealing with onset fronts...')
    for onset_front_id in _get_front_ids_one_at_a_time(onset_fronts):
        printd('      -> Dealing with onset front', int(onset_front_id))
        front_is_complete = False
        while not front_is_complete:
            corresponding_offsets = _get_corresponding_offsets(resulting_onset_fronts, onset_front_id, onsets, offsets)
            if '<MASK>':
                break
            _all_offset_fronts_of_interest, ids_ntimes_seen = _get_all_offset_fronts_from_offsets(offset_fronts, corresponding_offsets)
            ntimes_seen_sorted = sorted([(k, v) for k, v in ids_ntimes_seen.items()], key=lambda tup: (-1 * tup[1], tup[0]))
            assert len(ntimes_seen_sorted) > 0, 'We somehow got an empty dict of offset front IDs'
            offset_front_id, _ntimes_seen = ntimes_seen_sorted[0]
            if '<MASK>':
                offset_front_id, _ntimes_seen = ntimes_seen_sorted[1]
            offset_front_id_most_overlap = offset_front_id
            front_is_complete = _update_segmentation_mask(segmentation_mask, resulting_onset_fronts, offset_fronts, onset_front_id, offset_front_id_most_overlap)
            _remove_overlaps(segmentation_mask, resulting_onset_fronts)
            _remove_overlaps(segmentation_mask, offset_fronts)
    return segmentation_mask",False,"['debug', 'not corresponding_offsets', 'offset_front_id == -1 and len(ntimes_seen_sorted) > 1']",___________________,0.0
"def create(self, request):
    """"""
        Change password for logged in django staff user
        """"""
    password_form = PasswordChangeForm(request.user, data=request.data)
    if '<MASK>':
        raise serializers.ValidationError(password_form.errors)
    password_form.save()
    update_session_auth_hash(request, password_form.user)
    return Response(status=status.HTTP_204_NO_CONTENT)",False,['not password_form.is_valid()'],___________________,0.0
"def destroyObject(self, obj):
    """"""
        C_DestroyObject

        :param obj: object ID
        """"""
    rv = self.lib.C_DestroyObject(self.session, obj)
    if '<MASK>':
        raise PyKCS11Error(rv)",False,['rv != CKR_OK'],"(self,,,,,,,,,,,,,,,,,",0.0
"def build_user(user_info):
    """"""Build a user object
    
    Args:
        user_info(dict): A dictionary with user information
    
    Returns:
        user_obj(scout.models.User)
    """"""
    try:
        email = user_info['email']
    except KeyError as err:
        raise KeyError('A user has to have a email')
    try:
        name = user_info['name']
    except KeyError as err:
        raise KeyError('A user has to have a name')
    user_obj = User(email=email, name=name)
    if '<MASK>':
        user_obj['roles'] = user_info['roles']
    if '<MASK>':
        user_obj['location'] = user_info['location']
    if '<MASK>':
        user_obj['institutes'] = user_info['institutes']
    return user_obj",False,"[""'roles' in user_info"", ""'location' in user_info"", ""'institutes' in user_info""]",___________________,0.0
"def validate_timestamp_and_nonce(self, client_key, timestamp, nonce, request, request_token=None, access_token=None):
    """"""Validate the timestamp and nonce is used or not.""""""
    log.debug('Validate timestamp and nonce %r', client_key)
    nonce_exists = self._noncegetter(client_key=client_key, timestamp=timestamp, nonce=nonce, request_token=request_token, access_token=access_token)
    if '<MASK>':
        return False
    self._noncesetter(client_key=client_key, timestamp=timestamp, nonce=nonce, request_token=request_token, access_token=access_token)
    return True",False,['nonce_exists'],"_,,,,,,,,,,,,,,,,,,",0.0
"def update_dismiss_variant(self, institute, case, user, link, variant, dismiss_variant):
    """"""Create an event for updating the manual dismiss variant entry

          This function will create a event and update the dismiss variant
          field of the variant.

        Arguments:
            institute (dict): A Institute object
            case (dict): Case object
            user (dict): A User object
            link (str): The url to be used in the event
            variant (dict): A variant object
            dismiss_variant (list): The new dismiss variant list

        Return:
            updated_variant

        """"""
    LOG.info('Creating event for updating dismiss variant for variant {0}'.format(variant['display_name']))
    self.create_event(institute=institute, case=case, user=user, link=link, category='variant', verb='dismiss_variant', variant=variant, subject=variant['display_name'])
    if '<MASK>':
        LOG.info('Setting dismiss variant to {0} for variant {1}'.format(dismiss_variant, variant['display_name']))
        action = '$set'
    else:
        LOG.info('Reset dismiss variant from {0} for variant {1}'.format(variant['dismiss_variant'], variant['display_name']))
        action = '$unset'
    updated_variant = self.variant_collection.find_one_and_update({'_id': variant['_id']}, {action: {'dismiss_variant': dismiss_variant}}, return_document=pymongo.ReturnDocument.AFTER)
    LOG.debug('Variant updated')
    return updated_variant",False,['dismiss_variant'],___________________,0.0
"def write(self, output_stream, kmip_version=enums.KMIPVersion.KMIP_1_0):
    """"""
        Write the data encoding the Check response payload to a stream.

        Args:
            output_stream (stream): A data stream in which to encode object
                data, supporting a write method; usually a BytearrayStream
                object.
            kmip_version (KMIPVersion): An enumeration defining the KMIP
                version with which the object will be encoded. Optional,
                defaults to KMIP 1.0.

        Raises:
            ValueError: Raised if the data attribute is not defined.
        """"""
    local_stream = utils.BytearrayStream()
    if '<MASK>':
        self._unique_identifier.write(local_stream, kmip_version=kmip_version)
    if '<MASK>':
        self._usage_limits_count.write(local_stream, kmip_version=kmip_version)
    if '<MASK>':
        self._cryptographic_usage_mask.write(local_stream, kmip_version=kmip_version)
    if '<MASK>':
        self._lease_time.write(local_stream, kmip_version=kmip_version)
    self.length = local_stream.length()
    super(CheckResponsePayload, self).write(output_stream, kmip_version=kmip_version)
    output_stream.write(local_stream.buffer)",False,"['self._unique_identifier', 'self._usage_limits_count', 'self._cryptographic_usage_mask', 'self._lease_time']","_____)
    """"""
    """"""
    """"""
    """"""
",0.0
"def deprecated_capacity_meyerhof_and_hanna_1978(sl_0, sl_1, h0, fd, verbose=0):
    """"""
    Calculates the two-layered foundation capacity according Meyerhof and Hanna (1978)

    :param sl_0: Top Soil object
    :param sl_1: Base Soil object
    :param h0: Height of top soil layer
    :param fd: Foundation object
    :param h_l: Horizontal load parallel to length
    :param h_b: Horizontal load parallel to width
    :param vertical_load: Vertical load
    :param verbose: verbosity
    :return: ultimate bearing stress
    """"""
    sl_0.nq_factor_0 = np.tan(np.pi / 4 + np.deg2rad(sl_0.phi / 2)) ** 2 * np.exp(np.pi * np.tan(np.deg2rad(sl_0.phi)))
    if '<MASK>':
        sl_0.nc_factor_0 = 5.14
    else:
        sl_0.nc_factor_0 = (sl_0.nq_factor_0 - 1) / np.tan(np.deg2rad(sl_0.phi))
    sl_0.ng_factor_0 = (sl_0.nq_factor_0 - 1) * np.tan(1.4 * np.deg2rad(sl_0.phi))
    sl_1.nq_factor_1 = np.tan(np.pi / 4 + np.deg2rad(sl_1.phi / 2)) ** 2 * np.exp(np.pi * np.tan(np.deg2rad(sl_1.phi)))
    if '<MASK>':
        sl_1.nc_factor_1 = 5.14
    else:
        sl_1.nc_factor_1 = (sl_1.nq_factor_1 - 1) / np.tan(np.deg2rad(sl_1.phi))
    sl_1.ng_factor_1 = (sl_1.nq_factor_1 - 1) * np.tan(1.4 * np.deg2rad(sl_1.phi))
    if '<MASK>':
        log('Nc: ', sl_1.nc_factor_1)
        log('Nq: ', sl_1.nq_factor_1)
        log('Ng: ', sl_1.ng_factor_1)
    sl_0.kp_0 = np.tan(np.pi / 4 + np.deg2rad(sl_0.phi / 2)) ** 2
    sl_1.kp_1 = np.tan(np.pi / 4 + np.deg2rad(sl_1.phi / 2)) ** 2
    if '<MASK>':
        sl_0.s_c_0 = 1 + 0.2 * sl_0.kp_0 * (fd.width / fd.length)
        sl_0.s_q_0 = 1.0 + 0.1 * sl_0.kp_0 * (fd.width / fd.length)
    else:
        sl_0.s_c_0 = 1 + 0.2 * (fd.width / fd.length)
        sl_0.s_q_0 = 1.0
    sl_0.s_g_0 = sl_0.s_q_0
    if '<MASK>':
        sl_1.s_c_1 = 1 + 0.2 * sl_1.kp_1 * (fd.width / fd.length)
        sl_1.s_q_1 = 1.0 + 0.1 * sl_1.kp_1 * (fd.width / fd.length)
    else:
        sl_1.s_c_1 = 1 + 0.2 * (fd.width / fd.length)
        sl_1.s_q_1 = 1.0
    sl_1.s_g_1 = sl_1.s_q_1
    '\n    # depth factors\n    d_c = 1 + 0.2 * np.sqrt(kp) * fd.depth / fd.width\n    if sl_0.phi > 10:\n        d_q = 1 + 0.1 * np.sqrt(kp) * fd.depth / fd.width\n    else:\n        d_q = 1.0\n    d_g = d_q\n\n    # inclination factors:\n    theta_load = np.arctan(horizontal_load / vertical_load)\n    i_c = (1 - theta_load / (np.pi * 0.5)) ** 2\n    i_q = i_c\n    if sl_0.phi > 0:\n        i_g = (1 - theta_load / sl_0.phi_r) ** 2\n    else:\n        i_g = 0\n    '
    sl_0.q_0 = sl_0.cohesion * sl_0.nc_factor_0 + 0.5 * sl_0.unit_dry_weight * fd.width * sl_0.ng_factor_0
    sl_1.q_1 = sl_1.cohesion * sl_1.nc_factor_1 + 0.5 * sl_1.unit_dry_weight * fd.width * sl_1.ng_factor_1
    q1_q0 = sl_1.q_1 / sl_0.q_0
    x_0 = np.array([0, 20.08, 22.42, 25.08, 27.58, 30.08, 32.58, 34.92, 37.83, 40.0, 42.67, 45.0, 47.0, 49.75])
    y_0 = np.array([0.93, 0.93, 0.93, 0.93, 1.01, 1.17, 1.32, 1.56, 1.87, 2.26, 2.72, 3.35, 3.81, 4.82])
    x_2 = np.array([0, 20.08, 22.5, 25.08, 27.58, 30.08, 32.5, 35.0, 37.67, 40.17, 42.67, 45.0, 47.5, 50.0])
    y_2 = np.array([1.55, 1.55, 1.71, 1.86, 2.1, 2.33, 2.72, 3.11, 3.81, 4.43, 5.28, 6.14, 7.46, 9.24])
    x_4 = np.array([0, 20.0, 22.51, 25.1, 27.69, 30.11, 32.45, 35.04, 37.88, 40.14, 42.65, 45.07, 47.33, 50.08])
    y_4 = np.array([2.49, 2.49, 2.64, 2.87, 3.34, 3.81, 4.43, 5.2, 6.29, 7.38, 9.01, 11.11, 14.29, 19.34])
    x_10 = np.array([0, 20.0, 22.5, 25.08, 28.0, 30.0, 32.5, 34.92, 37.5, 40.17, 42.42, 45.0, 47.17, 50.08])
    y_10 = np.array([3.27, 3.27, 3.74, 4.44, 5.37, 6.07, 7.16, 8.33, 10.04, 12.3, 15.95, 21.17, 27.47, 40.0])
    x_int = sl_0.phi
    if '<MASK>':
        fd.ks = 0
    elif '<MASK>':
        fd.ks = np.interp(x_int, x_0, y_0)
    elif '<MASK>':
        fd.ks = np.interp(x_int, x_2, y_2)
    elif '<MASK>':
        fd.ks = np.interp(x_int, x_4, y_4)
    elif '<MASK>':
        fd.ks = np.interp(x_int, x_10, y_10)
    elif '<MASK>':
        ks_1 = np.interp(x_int, x_0, y_0)
        ks_2 = np.interp(x_int, x_2, y_2)
        fd.ks = (ks_2 - ks_1) * q1_q0 / 0.2 + ks_1
    elif '<MASK>':
        ks_1 = np.interp(x_int, x_2, y_2)
        ks_2 = np.interp(x_int, x_4, y_4)
        fd.ks = (ks_2 - ks_1) * (q1_q0 - 0.2) / 0.2 + ks_1
    elif '<MASK>':
        ks_1 = np.interp(x_int, x_4, y_4)
        ks_2 = np.interp(x_int, x_10, y_10)
        fd.ks = (ks_2 - ks_1) * (q1_q0 - 0.4) / 0.6 + ks_1
    else:
        raise DesignError(""Cannot compute 'ks', bearing ratio out-of-range (q1_q0 = %.3f) required: 0-1."" % q1_q0)
    if '<MASK>':
        c1_c0 = 0
    else:
        c1_c0 = sl_1.cohesion / sl_0.cohesion
    x = np.array([0.0, 0.082, 0.206, 0.298, 0.404, 0.509, 0.598, 0.685, 0.772])
    y = np.array([0.627, 0.7, 0.794, 0.855, 0.912, 0.948, 0.968, 0.983, 0.997])
    ca_c0 = np.interp(c1_c0, x, y)
    fd.ca = ca_c0 * sl_0.cohesion
    a = 1
    s = 1
    r = 1 + fd.width / fd.length
    q_b1 = sl_1.cohesion * sl_1.nc_factor_1 * sl_1.s_c_1
    q_b2 = sl_0.unit_dry_weight * h0 * sl_1.nq_factor_1 * sl_1.s_q_1
    q_b3 = sl_1.unit_dry_weight * fd.width * sl_1.ng_factor_1 * sl_1.s_g_1 / 2
    fd.q_b = q_b1 + q_b2 + q_b3
    fd.q_ult4 = r * (2 * fd.ca * (h0 - fd.depth) / fd.width) * a
    fd.q_ult5 = r * (sl_0.unit_dry_weight * (h0 - fd.depth) ** 2) * (1 + 2 * fd.depth / (h0 - fd.depth)) * (fd.ks * np.tan(np.deg2rad(sl_0.phi)) / fd.width) * s
    fd.q_ult6 = sl_0.unit_dry_weight * (h0 - fd.depth)
    fd.q_ult = fd.q_b + fd.q_ult4 + fd.q_ult5 - fd.q_ult6
    q_t1 = sl_0.cohesion * sl_0.nc_factor_0 * sl_0.s_c_0
    q_t2 = sl_0.unit_dry_weight * fd.depth * sl_0.nq_factor_0 * sl_0.s_q_0
    q_t3 = sl_0.unit_dry_weight * fd.width * sl_0.ng_factor_0 * sl_0.s_g_0 / 2
    fd.q_t = q_t1 + q_t2 + q_t3
    if '<MASK>':
        fd.q_ult = fd.q_t
    return fd.q_ult",False,"['sl_0.phi == 0', 'sl_1.phi == 0', 'verbose', 'sl_0.phi >= 10', 'sl_1.phi >= 10', 'sl_0.phi < 1', 'sl_0.cohesion == 0', 'fd.q_ult > fd.q_t', 'q1_q0 == 0', 'q1_q0 == 0.2', 'q1_q0 == 0.4', 'q1_q0 == 1.0', '0 < q1_q0 < 0.2', '0.2 < q1_q0 < 0.4', '0.4 < q1_q0 < 1.0']","_0,                ",0.0
"def _check_equals_spacing(self, tokens, i):
    """"""Check the spacing of a single equals sign.""""""
    if '<MASK>':
        self._check_space(tokens, i, (_MUST, _MUST))
    elif '<MASK>':
        self._check_space(tokens, i, (_MUST_NOT, _MUST_NOT))
    else:
        self._check_space(tokens, i, (_MUST, _MUST))",False,"['self._has_valid_type_annotation(tokens, i)', ""self._inside_brackets('(') or self._inside_brackets('lambda')""]",___________________,0.0
"def readToken(self):
    """"""
        Attempt to re-establish a connection using previously acquired tokens.

        If the Skype token is valid but the registration token is invalid, a new endpoint will be registered.

        Raises:
            .SkypeAuthException: if the token file cannot be used to authenticate
        """"""
    if '<MASK>':
        raise SkypeAuthException('No token file specified')
    try:
        with open(self.tokenFile, 'r') as f:
            lines = f.read().splitlines()
    except OSError:
        raise SkypeAuthException(""Token file doesn't exist or not readable"")
    try:
        user, skypeToken, skypeExpiry, regToken, regExpiry, msgsHost = lines
        skypeExpiry = datetime.fromtimestamp(int(skypeExpiry))
        regExpiry = datetime.fromtimestamp(int(regExpiry))
    except ValueError:
        raise SkypeAuthException('Token file is malformed')
    if '<MASK>':
        raise SkypeAuthException('Token file has expired')
    self.userId = user
    self.tokens['skype'] = skypeToken
    self.tokenExpiry['skype'] = skypeExpiry
    if '<MASK>':
        self.tokens['reg'] = regToken
        self.tokenExpiry['reg'] = regExpiry
        self.msgsHost = msgsHost
    else:
        self.getRegToken()",False,"['not self.tokenFile', 'datetime.now() >= skypeExpiry', 'datetime.now() < regExpiry']",___________________,0.0
"def read(self, istream, kmip_version=enums.KMIPVersion.KMIP_1_0):
    """"""
        Read the data encoding the RevocationReason object and decode it
        into its constituent parts.

        Args:
            istream (Stream): A data stream containing encoded object data,
                supporting a read method; usually a BytearrayStream object.
            kmip_version (KMIPVersion): An enumeration defining the KMIP
                version with which the object will be decoded. Optional,
                defaults to KMIP 1.0.
        """"""
    super(RevocationReason, self).read(istream, kmip_version=kmip_version)
    tstream = BytearrayStream(istream.read(self.length))
    self.revocation_code = RevocationReasonCode()
    self.revocation_code.read(tstream, kmip_version=kmip_version)
    if '<MASK>':
        self.revocation_message = TextString()
        self.revocation_message.read(tstream, kmip_version=kmip_version)
    self.is_oversized(tstream)
    self.validate()",False,"['self.is_tag_next(Tags.REVOCATION_MESSAGE, tstream)']",___________________,0.0
"def apply(f, args):
    """"""Apply function f to the arguments provided.
    The last argument must always be coercible to a Seq. Intermediate
    arguments are not modified.
    For example:
        (apply max [1 2 3])   ;=> 3
        (apply max 4 [1 2 3]) ;=> 4""""""
    final = list(args[:-1])
    try:
        last = args[-1]
    except TypeError as e:
        logger.debug('Ignored %s: %s', type(e).__name__, e)
    s = to_seq(last)
    if '<MASK>':
        final.extend(s)
    return f(*final)",False,['s is not None'],(((((((((((((((((((,0.0
"def write(self, output_buffer, kmip_version=enums.KMIPVersion.KMIP_1_3):
    """"""
        Write the ValidationInformation structure encoding to the data stream.

        Args:
            output_buffer (stream): A data stream in which to encode
                ValidationInformation structure data, supporting a write
                method.
            kmip_version (enum): A KMIPVersion enumeration defining the KMIP
                version with which the object will be encoded. Optional,
                defaults to KMIP 2.0.

        Raises:
            InvalidField: Raised if the validation authority type, validation
                version major, validation type, and/or validation level fields
                are not defined.
            VersionNotSupported: Raised when a KMIP version is provided that
                does not support the ValidationInformation structure.
        """"""
    if '<MASK>':
        raise exceptions.VersionNotSupported('KMIP {} does not support the ValidationInformation object.'.format(kmip_version.value))
    local_buffer = BytearrayStream()
    if '<MASK>':
        self._validation_authority_type.write(local_buffer, kmip_version=kmip_version)
    else:
        raise exceptions.InvalidField('The ValidationInformation structure is missing the validation authority type field.')
    if '<MASK>':
        self._validation_authority_country.write(local_buffer, kmip_version=kmip_version)
    if '<MASK>':
        self._validation_authority_uri.write(local_buffer, kmip_version=kmip_version)
    if '<MASK>':
        self._validation_version_major.write(local_buffer, kmip_version=kmip_version)
    else:
        raise exceptions.InvalidField('The ValidationInformation structure is missing the validation version major field.')
    if '<MASK>':
        self._validation_version_minor.write(local_buffer, kmip_version=kmip_version)
    if '<MASK>':
        self._validation_type.write(local_buffer, kmip_version=kmip_version)
    else:
        raise exceptions.InvalidField('The ValidationInformation structure is missing the validation type field.')
    if '<MASK>':
        self._validation_level.write(local_buffer, kmip_version=kmip_version)
    else:
        raise exceptions.InvalidField('The ValidationInformation structure is missing the validation level field.')
    if '<MASK>':
        self._validation_certificate_identifier.write(local_buffer, kmip_version=kmip_version)
    if '<MASK>':
        self._validation_certificate_uri.write(local_buffer, kmip_version=kmip_version)
    if '<MASK>':
        self._validation_vendor_uri.write(local_buffer, kmip_version=kmip_version)
    if '<MASK>':
        for validation_profile in self._validation_profiles:
            validation_profile.write(local_buffer, kmip_version=kmip_version)
    self.length = local_buffer.length()
    super(ValidationInformation, self).write(output_buffer, kmip_version=kmip_version)
    output_buffer.write(local_buffer.buffer)",False,"['kmip_version < enums.KMIPVersion.KMIP_1_3', 'self._validation_authority_type', 'self._validation_authority_country', 'self._validation_authority_uri', 'self._validation_version_major', 'self._validation_version_minor', 'self._validation_type', 'self._validation_level', 'self._validation_certificate_identifier', 'self._validation_certificate_uri', 'self._validation_vendor_uri', 'self._validation_profiles']",___________________,0.0
"def adjust_spines(ax, spines):
    """"""function for removing spines and ticks.

    :param ax: axes object
    :param spines: a list of spines names to keep. e.g [left, right, top, bottom]
                    if spines = []. remove all spines and ticks.

    """"""
    for loc, spine in ax.spines.items():
        if '<MASK>':
            continue
        else:
            spine.set_color('none')
    if '<MASK>':
        ax.yaxis.set_ticks_position('left')
    else:
        ax.yaxis.set_ticks([])
    if '<MASK>':
        ax.xaxis.set_ticks_position('bottom')
    else:
        ax.xaxis.set_ticks([])",False,"[""'left' in spines"", ""'bottom' in spines"", 'loc in spines']",___________________,0.0
"def merge_opt_params(self, method, kargs):
    """"""Combine existing parameters with extra options supplied from command line
       options. Carefully merge special type of parameter if needed.
    """"""
    for key in self.legal_params[method]:
        if '<MASK>':
            continue
        if '<MASK>':
            assert type(getattr(self.opt, key)) == dict
            for k, v in getattr(self.opt, key).iteritems():
                kargs[key][k] = v
        else:
            kargs[key] = getattr(self.opt, key)
    return kargs",False,"['not hasattr(self.opt, key) or getattr(self.opt, key) is None', 'key in kargs and type(kargs[key]) == dict']",___________________,0.0
"def get_token_from_post_data(self, data):
    """"""Get a token response from POST data.

        :param data: POST data containing authorization information.
        :type data: dict
        :rtype: requests.Response
        """"""
    try:
        for x in ['grant_type', 'client_id', 'client_secret']:
            if '<MASK>':
                raise TypeError('Missing required OAuth 2.0 POST param: {0}'.format(x))
        if '<MASK>':
            return self.refresh_token(**data)
        for x in ['redirect_uri', 'code']:
            if '<MASK>':
                raise TypeError('Missing required OAuth 2.0 POST param: {0}'.format(x))
        return self.get_token(**data)
    except TypeError as exc:
        self._handle_exception(exc)
        return self._make_json_error_response('invalid_request')
    except StandardError as exc:
        self._handle_exception(exc)
        return self._make_json_error_response('server_error')",False,"[""'refresh_token' in data"", 'not data.get(x)', 'not data.get(x)']",___________________,0.0
"def f_remove_items(self, iterator, recursive=False):
    """"""Removes parameters, results or groups from the trajectory.

        This function ONLY removes items from your current trajectory and does not delete
        data stored to disk. If you want to delete data from disk, take a look at
        :func:`~pypet.trajectory.Trajectory.f_delete_items`.

        This will also remove all links if items are linked.

        :param iterator:

            A sequence of items you want to remove. Either the instances themselves
            or strings with the names of the items.

        :param recursive:

            In case you want to remove group nodes, if the children should be removed, too.

        """"""
    fetched_items = self._nn_interface._fetch_items(REMOVE, iterator, (), {})
    if '<MASK>':
        for _, item, dummy1, dummy2 in fetched_items:
            self._nn_interface._remove_node_or_leaf(item, recursive=recursive)
    else:
        self._logger.warning('Your removal was not successful, could not find a single item to remove.')",False,['fetched_items'],___________________,0.0
"def _fix_next_url(next_url):
    """"""Remove max=null parameter from URL.

    Patch for Webex Teams Defect: 'next' URL returned in the Link headers of
    the responses contain an errant 'max=null' parameter, which  causes the
    next request (to this URL) to fail if the URL is requested as-is.

    This patch parses the next_url to remove the max=null parameter.

    Args:
        next_url(basestring): The 'next' URL to be parsed and cleaned.

    Returns:
        basestring: The clean URL to be used for the 'next' request.

    Raises:
        AssertionError: If the parameter types are incorrect.
        ValueError: If 'next_url' does not contain a valid API endpoint URL
            (scheme, netloc and path).

    """"""
    next_url = str(next_url)
    parsed_url = urllib.parse.urlparse(next_url)
    if '<MASK>':
        raise ValueError(""'next_url' must be a valid API endpoint URL, minimally containing a scheme, netloc and path."")
    if '<MASK>':
        query_list = parsed_url.query.split('&')
        if '<MASK>':
            query_list.remove('max=null')
            warnings.warn('`max=null` still present in next-URL returned from Webex Teams', RuntimeWarning)
        new_query = '&'.join(query_list)
        parsed_url = list(parsed_url)
        parsed_url[4] = new_query
    return urllib.parse.urlunparse(parsed_url)",False,"['not parsed_url.scheme or not parsed_url.netloc or (not parsed_url.path)', 'parsed_url.query', ""'max=null' in query_list""]",___________________,0.0
"@wraps(f)
def decorated(*args, **kwargs):
    server = self.server
    uri, http_method, body, headers = extract_params()
    if '<MASK>':
        redirect_uri = request.args.get('redirect_uri', self.error_uri)
        log.debug('Found redirect_uri %s.', redirect_uri)
        try:
            ret = server.validate_authorization_request(uri, http_method, body, headers)
            scopes, credentials = ret
            kwargs['scopes'] = scopes
            kwargs.update(credentials)
        except oauth2.FatalClientError as e:
            log.debug('Fatal client error %r', e, exc_info=True)
            return self._on_exception(e, e.in_uri(self.error_uri))
        except oauth2.OAuth2Error as e:
            log.debug('OAuth2Error: %r', e, exc_info=True)
            state = request.values.get('state')
            if '<MASK>':
                e.state = state
            return self._on_exception(e, e.in_uri(redirect_uri))
        except Exception as e:
            log.exception(e)
            return self._on_exception(e, add_params_to_uri(self.error_uri, {'error': str(e)}))
    else:
        redirect_uri = request.values.get('redirect_uri', self.error_uri)
    try:
        rv = f(*args, **kwargs)
    except oauth2.FatalClientError as e:
        log.debug('Fatal client error %r', e, exc_info=True)
        return self._on_exception(e, e.in_uri(self.error_uri))
    except oauth2.OAuth2Error as e:
        log.debug('OAuth2Error: %r', e, exc_info=True)
        state = request.values.get('state')
        if '<MASK>':
            e.state = state
        return self._on_exception(e, e.in_uri(redirect_uri))
    if '<MASK>':
        return rv
    if '<MASK>':
        e = oauth2.AccessDeniedError(state=request.values.get('state'))
        return self._on_exception(e, e.in_uri(redirect_uri))
    return self.confirm_authorization_request()",False,"[""request.method in ('GET', 'HEAD')"", 'not isinstance(rv, bool)', 'not rv', 'state and (not e.state)', 'state and (not e.state)']",___________________,0.0
"def run_profilers(run_object, prof_config, verbose=False):
    """"""Runs profilers on run_object.

    Args:
        run_object: An object (string or tuple) for profiling.
        prof_config: A string with profilers configuration.
        verbose: True if info about running profilers should be shown.
    Returns:
        An ordered dictionary with collected stats.
    Raises:
        AmbiguousConfigurationError: when prof_config is ambiguous.
        BadOptionError: when unknown options are present in configuration.
    """"""
    if '<MASK>':
        raise AmbiguousConfigurationError('Profiler configuration %s is ambiguous' % prof_config)
    available_profilers = {opt for opt, _ in _PROFILERS}
    for option in prof_config:
        if '<MASK>':
            raise BadOptionError('Unknown option: %s' % option)
    run_stats = OrderedDict()
    present_profilers = ((o, p) for o, p in _PROFILERS if o in prof_config)
    for option, prof in present_profilers:
        curr_profiler = prof(run_object)
        if '<MASK>':
            print('Running %s...' % curr_profiler.__class__.__name__)
        run_stats[option] = curr_profiler.run()
    return run_stats",False,"['len(prof_config) > len(set(prof_config))', 'option not in available_profilers', 'verbose']",_((((((((((((((((((,0.0
"def get_context(self, name, value, attrs):
    """"""Get the context to render this widget with.""""""
    if '<MASK>':
        context = super(ClearableFileInputWithImagePreview, self).get_context(name, value, attrs)
    else:
        context = {}
        context['widget'] = {'name': name, 'is_hidden': self.is_hidden, 'required': self.is_required, 'value': self._format_value(value), 'attrs': self.build_attrs(self.attrs, attrs), 'template_name': self.template_name, 'type': self.input_type}
    checkbox_name = self.clear_checkbox_name(name)
    checkbox_id = self.clear_checkbox_id(checkbox_name)
    context['widget'].update({'checkbox_name': checkbox_name, 'checkbox_id': checkbox_id, 'is_initial': self.is_initial(value), 'input_text': self.input_text, 'initial_text': self.initial_text, 'clear_checkbox_label': self.clear_checkbox_label})
    if '<MASK>':
        context['widget'].update({'hidden_field_id': self.get_hidden_field_id(name), 'point_stage_id': self.get_point_stage_id(name), 'ppoi_id': self.get_ppoi_id(name), 'sized_url': self.get_sized_url(value), 'image_preview_id': self.image_preview_id(name)})
    return context",False,"['self.has_template_widget_rendering', ""value and hasattr(value, 'url')""]",___________________,0.0
"def wrapped(*args, **kwargs):
    check_name = func.__name__
    arg_name = None
    if '<MASK>':
        arg_name = args[0]
    try:
        if '<MASK>':
            logger.debug(""Checking '%s' for '%s'"", check_name, arg_name)
        else:
            logger.debug(""Checking '%s'"", check_name)
        response = func(*args, **kwargs)
    except Exception as e:
        message = str(e)
        response = {'ok': False, 'error': message, 'stacktrace': traceback.format_exc()}
        if '<MASK>':
            response = {arg_name: response}
            logger.exception(""Error calling '%s' for '%s': %s"", check_name, arg_name, message)
        else:
            logger.exception(""Error calling '%s': %s"", check_name, message)
    return response",False,"['args', 'arg_name', 'arg_name']","_name,_name,_name,_name,_name,_name,_",0.0
"def _print_header(cls):
    """"""
        Decide if we print or not the header.
        """"""
    if '<MASK>':
        print('\n')
        if '<MASK>':
            Prints(None, 'Less').header()
        else:
            Prints(None, 'Generic').header()
        PyFunceble.CONFIGURATION['header_printed'] = True",False,"[""not PyFunceble.CONFIGURATION['quiet'] and (not PyFunceble.CONFIGURATION['header_printed'])"", ""PyFunceble.CONFIGURATION['less']""]",___________________,0.0
"def os_path_to_client_path(self, os_path):
    """"""
        Converts an operating system path into a client path by
        replacing instances of os.path.sep with '/'.

        Note: If the client path contains any instances of '/'
        already, they will be replaced with '-'.
        """"""
    if '<MASK>':
        return os_path
    return os_path.replace('/', '-').replace(os.path.sep, '/')",False,"[""os.path.sep == '/'""]",_path_path_path_path_path_path_path_path_path_,0.0
"def add_phenotype(self, institute, case, user, link, hpo_term=None, omim_term=None, is_group=False):
    """"""Add a new phenotype term to a case

            Create a phenotype term and event with the given information

            Args:
                institute (Institute): A Institute object
                case (Case): Case object
                user (User): A User object
                link (str): The url to be used in the event
                hpo_term (str): A hpo id
                omim_term (str): A omim id
                is_group (bool): is phenotype term a group?

        """"""
    hpo_results = []
    try:
        if '<MASK>':
            hpo_results = [hpo_term]
        elif '<MASK>':
            LOG.debug('Fetching info for mim term {0}'.format(omim_term))
            disease_obj = self.disease_term(omim_term)
            if '<MASK>':
                for hpo_term in disease_obj.get('hpo_terms', []):
                    hpo_results.append(hpo_term)
        else:
            raise ValueError('Must supply either hpo or omim term')
    except ValueError as e:
        raise e
    existing_terms = set((term['phenotype_id'] for term in case.get('phenotype_terms', [])))
    updated_case = case
    phenotype_terms = []
    for hpo_term in hpo_results:
        LOG.debug('Fetching info for hpo term {0}'.format(hpo_term))
        hpo_obj = self.hpo_term(hpo_term)
        if '<MASK>':
            raise ValueError('Hpo term: %s does not exist in database' % hpo_term)
        phenotype_id = hpo_obj['_id']
        description = hpo_obj['description']
        if '<MASK>':
            phenotype_term = dict(phenotype_id=phenotype_id, feature=description)
            phenotype_terms.append(phenotype_term)
            LOG.info('Creating event for adding phenotype term for case {0}'.format(case['display_name']))
            self.create_event(institute=institute, case=case, user=user, link=link, category='case', verb='add_phenotype', subject=case['display_name'], content=phenotype_id)
        if '<MASK>':
            updated_case = self.case_collection.find_one_and_update({'_id': case['_id']}, {'$addToSet': {'phenotype_terms': {'$each': phenotype_terms}, 'phenotype_groups': {'$each': phenotype_terms}}}, return_document=pymongo.ReturnDocument.AFTER)
        else:
            updated_case = self.case_collection.find_one_and_update({'_id': case['_id']}, {'$addToSet': {'phenotype_terms': {'$each': phenotype_terms}}}, return_document=pymongo.ReturnDocument.AFTER)
    LOG.debug('Case updated')
    return updated_case",False,"['hpo_term', 'hpo_obj is None', 'phenotype_id not in existing_terms', 'is_group', 'omim_term', 'disease_obj']",___________________,0.0
"def get_alpn_proto_negotiated(self):
    """"""
        Get the protocol that was negotiated by ALPN.

        :returns: A bytestring of the protocol name.  If no protocol has been
            negotiated yet, returns an empty string.
        """"""
    data = _ffi.new('unsigned char **')
    data_len = _ffi.new('unsigned int *')
    _lib.SSL_get0_alpn_selected(self._ssl, data, data_len)
    if '<MASK>':
        return b''
    return _ffi.buffer(data[0], data_len[0])[:]",False,['not data_len'],___________________,0.0
"def assemble_to_object(self, in_filename, verbose=False):
    """"""
        Assemble *in_filename* assembly into *out_filename* object.

        If *iaca_marked* is set to true, markers are inserted around the block with most packed
        instructions or (if no packed instr. were found) the largest block and modified file is
        saved to *in_file*.

        *asm_block* controls how the to-be-marked block is chosen. ""auto"" (default) results in
        the largest block, ""manual"" results in interactive and a number in the according block.

        *pointer_increment* is the number of bytes the pointer is incremented after the loop or
           - 'auto': automatic detection, RuntimeError is raised in case of failure
           - 'auto_with_manual_fallback': automatic detection, fallback to manual input
           - 'manual': prompt user

        Returns two-tuple (filepointer, filename) to temp binary file.
        """"""
    file_base_name = os.path.splitext(os.path.basename(in_filename))[0]
    out_filename, already_exists = self._get_intermediate_file(file_base_name + '.o', binary=True, fp=False)
    if '<MASK>':
        pass
    compiler, compiler_args = self._machine.get_compiler()
    compiler_args.append('-c')
    cmd = [compiler] + [in_filename] + compiler_args + ['-o', out_filename]
    if '<MASK>':
        print('Executing (assemble_to_object): ', ' '.join(cmd))
    try:
        subprocess.check_output(cmd)
    except subprocess.CalledProcessError as e:
        print('Assembly failed:', e, file=sys.stderr)
        sys.exit(1)
    return out_filename",False,"['already_exists', 'verbose']",___________________,0.0
"def removeThing(self, thingTypeId, thingId):
    """"""
        Delete an existing thing.
        It accepts thingTypeId (string) and thingId (string) as parameters
        In case of failure it throws APIException
        """"""
    thingUrl = ApiClient.thingUrl % (self.host, thingTypeId, thingId)
    r = requests.delete(thingUrl, auth=self.credentials, verify=self.verify)
    status = r.status_code
    if '<MASK>':
        self.logger.debug('Thing was successfully removed')
        return True
    elif '<MASK>':
        raise ibmiotf.APIException(401, 'The authentication token is empty or invalid', None)
    elif '<MASK>':
        raise ibmiotf.APIException(403, 'The authentication method is invalid or the api key used does not exist', None)
    elif '<MASK>':
        raise ibmiotf.APIException(404, 'A thing type or thing instance with the specified id does not exist.', None)
    elif '<MASK>':
        raise ibmiotf.APIException(409, 'The thing instance is aggregated into another thing instance.', None)
    elif '<MASK>':
        raise ibmiotf.APIException(500, 'Unexpected error', None)
    else:
        raise ibmiotf.APIException(None, 'Unexpected error', None)",False,"['status == 204', 'status == 401', 'status == 403', 'status == 404', 'status == 409', 'status == 500']",___________________,0.0
"def _add_imported_module(self, node, importedmodname):
    """"""notify an imported module, used to analyze dependencies""""""
    module_file = node.root().file
    context_name = node.root().name
    base = os.path.splitext(os.path.basename(module_file))[0]
    try:
        importedmodname = astroid.modutils.get_module_part(importedmodname, module_file)
    except ImportError:
        pass
    if '<MASK>':
        self.add_message('import-self', node=node)
    elif '<MASK>':
        if '<MASK>':
            self._module_pkg[context_name] = context_name.rsplit('.', 1)[0]
        importedmodnames = self.stats['dependencies'].setdefault(importedmodname, set())
        if '<MASK>':
            importedmodnames.add(context_name)
        self.import_graph[context_name].add(importedmodname)
        if '<MASK>':
            self._excluded_edges[context_name].add(importedmodname)",False,"['context_name == importedmodname', 'not astroid.modutils.is_standard_module(importedmodname)', ""base != '__init__' and context_name not in self._module_pkg"", 'context_name not in importedmodnames', ""not self.linter.is_message_enabled('cyclic-import', line=node.lineno)""]",___________________,0.0
"def visit_attribute(self, node):
    """"""Look for removed attributes""""""
    if '<MASK>':
        self.add_message('xreadlines-attribute', node=node)
        return
    exception_message = 'message'
    try:
        for inferred in node.expr.infer():
            if '<MASK>':
                if '<MASK>':
                    if '<MASK>':
                        continue
                    self.add_message('exception-message-attribute', node=node)
            if '<MASK>':
                self._warn_if_deprecated(node, inferred.name, {node.attrname}, report_on_modules=False)
    except astroid.InferenceError:
        return",False,"[""node.attrname == 'xreadlines'"", 'isinstance(inferred, astroid.Instance) and utils.inherit_from_std_ex(inferred)', 'isinstance(inferred, astroid.Module)', 'node.attrname == exception_message', 'exception_message in inferred.instance_attrs']","_,,,,,,,,,,,,,,,,,,",0.0
"def build(self, repo, config=None, name=None, commit=None, tag='latest', recipe='Singularity', preview=False):
    """"""trigger a build on Google Cloud (storage then compute) given a name
       recipe, and Github URI where the recipe can be found.
    
       Parameters
       ==========
       name: should be the complete uri that the user has requested to push.
       commit: a commit to use, not required, and can be parsed from URI
       repo: should correspond to a Github URL or (if undefined) used local repo.
       tag: a user specified tag, to take preference over tag in name
       config: The local config file to use. If the file doesn't exist, then
               we attempt looking up the config based on the name.
       recipe: If defined, limit builder to build a single recipe

    """"""
    bot.debug('BUILD %s' % repo)
    if '<MASK>':
        sys.exit(1)
    config = self._load_build_config(config)
    if '<MASK>':
        name = '/'.join(repo.split('/')[-2:])
    names = parse_image_name(remove_uri(name))
    names['tag'] = tag or names['tag']
    if '<MASK>':
        tag = get_recipe_tag(recipe)
    names = parse_image_name(remove_uri(name), tag=tag)
    commit = commit or names['version']
    config = self._setup_build(name=names['url'], recipe=recipe, repo=repo, config=config, tag=tag, commit=commit)
    if '<MASK>':
        return config
    return self._run_build(config)",False,"['not self._healthy(repo)', 'name is None', ""names['tag'] == 'latest' and recipe != 'Singularity'"", 'preview is True']",___________________,0.0
"def rekey(self, uuid=None, offset=None, template_attribute=None, credential=None):
    """"""
        Check object usage according to specific constraints.

        Args:
            uuid (string): The unique identifier of a managed cryptographic
                object that should be checked. Optional, defaults to None.
            offset (int): An integer specifying, in seconds, the difference
                between the rekeyed objects initialization date and activation
                date. Optional, defaults to None.
            template_attribute (TemplateAttribute): A TemplateAttribute struct
                containing the attributes to set on the newly rekeyed object.
                Optional, defaults to None.
            credential (Credential): A Credential struct containing a set of
                authorization parameters for the operation. Optional, defaults
                to None.

        Returns:
            dict: The results of the check operation, containing the following
                key/value pairs:

                Key                        | Value
                ---------------------------|-----------------------------------
                'unique_identifier'        | (string) The unique ID of the
                                           | checked cryptographic object.
                'template_attribute'       | (TemplateAttribute) A struct
                                           | containing attribute set by the
                                           | server. Optional.
                'result_status'            | (ResultStatus) An enumeration
                                           | indicating the status of the
                                           | operation result.
                'result_reason'            | (ResultReason) An enumeration
                                           | providing context for the result
                                           | status.
                'result_message'           | (string) A message providing
                                           | additional context for the
                                           | operation result.
        """"""
    operation = Operation(OperationEnum.REKEY)
    request_payload = payloads.RekeyRequestPayload(unique_identifier=uuid, offset=offset, template_attribute=template_attribute)
    batch_item = messages.RequestBatchItem(operation=operation, request_payload=request_payload)
    request = self._build_request_message(credential, [batch_item])
    response = self._send_and_receive_message(request)
    batch_item = response.batch_items[0]
    payload = batch_item.response_payload
    result = {}
    if '<MASK>':
        result['unique_identifier'] = payload.unique_identifier
        if '<MASK>':
            result['template_attribute'] = payload.template_attribute
    result['result_status'] = batch_item.result_status.value
    try:
        result['result_reason'] = batch_item.result_reason.value
    except Exception:
        result['result_reason'] = batch_item.result_reason
    try:
        result['result_message'] = batch_item.result_message.value
    except Exception:
        result['result_message'] = batch_item.result_message
    return result",False,"['payload', 'payload.template_attribute is not None']",___________________,0.0
"def parse_docstring(doc):
    """"""Parse a docstring into ParameterInfo and ReturnInfo objects.""""""
    doc = inspect.cleandoc(doc)
    lines = doc.split('\n')
    section = None
    section_indent = None
    params = {}
    returns = None
    for line in lines:
        line = line.rstrip()
        if '<MASK>':
            continue
        elif '<MASK>':
            section = 'args'
            section_indent = None
            continue
        elif '<MASK>':
            section = 'return'
            section_indent = None
            continue
        if '<MASK>':
            stripped = line.lstrip()
            margin = len(line) - len(stripped)
            if '<MASK>':
                section_indent = margin
            if '<MASK>':
                continue
            if '<MASK>':
                param_name, type_info = parse_param(stripped)
                params[param_name] = type_info
            elif '<MASK>':
                returns = parse_return(stripped)
    return (params, returns)",False,"['len(line) == 0', 'section is not None', ""str(line) == 'Args:'"", 'section_indent is None', 'margin != section_indent', ""section == 'args'"", ""str(line) == 'Returns:'"", ""section == 'return'""]",___________________,0.0
"def push(self, path, name, tag=None):
    """"""push an image to an S3 endpoint""""""
    path = os.path.abspath(path)
    image = os.path.basename(path)
    bot.debug('PUSH %s' % path)
    if '<MASK>':
        bot.error('%s does not exist.' % path)
        sys.exit(1)
    names = parse_image_name(remove_uri(name), tag=tag)
    image_size = os.path.getsize(path) >> 20
    metadata = {'sizemb': '%s' % image_size, 'client': 'sregistry'}
    self.bucket.upload_file(path, names['storage_uri'], {'Metadata': metadata})",False,['not os.path.exists(path)'],___________________,0.0
"def tags(self, name=None) -> List['Tag']:
    """"""Return all tags with the given name.""""""
    lststr = self._lststr
    type_to_spans = self._type_to_spans
    if '<MASK>':
        if '<MASK>':
            string = lststr[0]
            return [Tag(lststr, type_to_spans, span, 'ExtensionTag') for span in type_to_spans['ExtensionTag'] if string.startswith('<' + name, span[0])]
        tags = []
    else:
        tags = [Tag(lststr, type_to_spans, span, 'ExtensionTag') for span in type_to_spans['ExtensionTag']]
    tags_append = tags.append
    ss = self._span[0]
    shadow = self._shadow
    if '<MASK>':
        reversed_start_matches = reversed([m for m in regex_compile(START_TAG_PATTERN.replace(b'{name}', b'(?P<name>' + name.encode() + b')')).finditer(shadow)])
        end_search = regex_compile(END_TAG_PATTERN.replace(b'{name}', name.encode())).search
    else:
        reversed_start_matches = reversed([m for m in START_TAG_FINDITER(shadow)])
    shadow_copy = shadow[:]
    spans = type_to_spans.setdefault('Tag', [])
    span_tuple_to_span_get = {(s[0], s[1]): s for s in spans}.get
    spans_append = spans.append
    for start_match in reversed_start_matches:
        if '<MASK>':
            s, e = start_match.span()
            span = [ss + s, ss + e]
        else:
            if '<MASK>':
                end_match = end_search(shadow_copy, start_match.end())
            else:
                end_match = search(END_TAG_PATTERN.replace(b'{name}', start_match['name']), shadow_copy)
            if '<MASK>':
                s, e = end_match.span()
                shadow_copy[s:e] = b'_' * (e - s)
                span = [ss + start_match.start(), ss + e]
            else:
                s, e = start_match.span()
                span = [ss + s, ss + e]
        old_span = span_tuple_to_span_get((span[0], span[1]))
        if '<MASK>':
            spans_append(span)
        else:
            span = old_span
        tags_append(Tag(lststr, type_to_spans, span, 'Tag'))
    return sorted(tags, key=attrgetter('_span'))",False,"['name', 'name', 'name in _tag_extensions', ""start_match['self_closing']"", 'old_span is None', 'name', 'end_match']",___________________,0.0
"def resolve_inputs(self, layers):
    """"""Resolve the names of inputs for this layer into shape tuples.

        Parameters
        ----------
        layers : list of :class:`Layer`
            A list of the layers that are available for resolving inputs.

        Raises
        ------
        theanets.util.ConfigurationError :
            If an input cannot be resolved.
        """"""
    resolved = {}
    for name, shape in self._input_shapes.items():
        if '<MASK>':
            name, shape = self._resolve_shape(name, layers)
        resolved[name] = shape
    self._input_shapes = resolved",False,['shape is None'],___________________,0.0
"def _is_raising(body: typing.List) -> bool:
    """"""Return true if the given statement node raise an exception""""""
    for node in body:
        if '<MASK>':
            return True
    return False",False,"['isinstance(node, astroid.Raise)']",(((((((((((((((((((,0.0
"def _alt(alt):
    """"""Parses the VCF row ALT object.""""""
    if '<MASK>':
        return '.'
    else:
        return str(alt)",False,['not alt'],"(alt)
    """"""
    """"""
    """"""
    """"""
    """"""
",0.0
"def push(self, path, name, tag=None):
    """"""push an image to Globus endpoint. In this case, the name is the
       globus endpoint id and path.

       --name <endpointid>:/path/for/image

    """"""
    endpoint, remote = self._parse_endpoint_name(name)
    path = os.path.abspath(path)
    image = os.path.basename(path)
    bot.debug('PUSH %s' % path)
    q = parse_image_name(image)
    if '<MASK>':
        bot.error('%s does not exist.' % path)
        sys.exit(1)
    if '<MASK>':
        self._init_transfer_client()
    endpoints = self._get_endpoints()
    if '<MASK>':
        bot.error('You must have a personal endpoint to transfer the container')
        sys.exit(1)
    source_endpoint = None
    for eid, contender in endpoints['my-endpoints'].items():
        if '<MASK>':
            source_endpoint = contender
            break
    if '<MASK>':
        bot.error('No activated local endpoints online! Go online to transfer')
        sys.exit(1)
    self._create_endpoint_cache(endpoint)
    added = self.add(image_path=path, image_uri=q['uri'], copy=True)
    label = 'Singularity Registry Transfer for %s' % added.name
    tdata = globus_sdk.TransferData(self.transfer_client, source_endpoint['id'], endpoint, label=label, sync_level='checksum')
    image = '.singularity/shub/%s' % image
    tdata.add_item(added.image, image)
    bot.info('Requesting transfer from local %s to %s:%s' % (SREGISTRY_STORAGE, endpoint, image))
    transfer_result = self.transfer_client.submit_transfer(tdata)
    bot.info(transfer_result['message'])
    return transfer_result",False,"['not os.path.exists(path)', ""not hasattr(self, 'transfer_client')"", ""len(endpoints['my-endpoints']) == 0"", 'source_endpoint is None', ""contender['gcp_connected'] is True""]",___________________,0.0
"def request_writes(self, offset, data):
    """"""Request any available writes given new incoming data.

        You call this method by providing new data along with the
        offset associated with the data.  If that new data unlocks
        any contiguous writes that can now be submitted, this
        method will return all applicable writes.

        This is done with 1 method call so you don't have to
        make two method calls (put(), get()) which acquires a lock
        each method call.

        """"""
    if '<MASK>':
        return []
    writes = []
    if '<MASK>':
        return []
    heapq.heappush(self._writes, (offset, data))
    self._pending_offsets.add(offset)
    while self._writes and self._writes[0][0] == self._next_offset:
        next_write = heapq.heappop(self._writes)
        writes.append({'offset': next_write[0], 'data': next_write[1]})
        self._pending_offsets.remove(next_write[0])
        self._next_offset += len(next_write[1])
    return writes",False,"['offset < self._next_offset', 'offset in self._pending_offsets']",___________________,0.0
"def _trampoline(f):
    """"""Trampoline a function repeatedly until it is finished recurring to help
    avoid stack growth.""""""

    @functools.wraps(f)
    def trampoline(*args, **kwargs):
        while True:
            ret = f(*args, **kwargs)
            if '<MASK>':
                args = ret.args
                kwargs = ret.kwargs
                continue
            return ret
    return trampoline",False,"['isinstance(ret, _TrampolineArgs)']",___________________,0.0
"def checkDir(self, file_path=''):
    """"""
        Checks if a directory exists. If not, it creates one with the specified
        file_path.
        """"""
    if '<MASK>':
        try:
            os.makedirs(os.path.dirname(file_path))
        except OSError as e:
            if '<MASK>':
                raise",False,"['not os.path.exists(os.path.dirname(file_path))', 'e.errno != errno.EEXIST']","_path,_path,_path,_path,_path,_path,_",0.0
"def parse_header(recipe, header='from', remove_header=True):
    """"""take a recipe, and return the complete header, line. If
       remove_header is True, only return the value.

       Parameters
       ==========
       recipe: the recipe file
       headers: the header key to find and parse
       remove_header: if true, remove the key

    """"""
    parsed_header = None
    fromline = [x for x in recipe.split('\n') if '%s:' % header in x.lower()]
    if '<MASK>':
        return ''
    if '<MASK>':
        fromline = fromline[0]
        parsed_header = fromline.strip()
    if '<MASK>':
        parsed_header = fromline.split(':', 1)[-1].strip()
    return parsed_header",False,"['len(fromline) == 0', 'len(fromline) > 0', 'remove_header is True']",__header____________header___,0.0
"def _check_names(self, split_names, parent_node=None):
    """"""Checks if a list contains strings with invalid names.

        Returns a description of the name violations. If names are correct the empty
        string is returned.

        :param split_names: List of strings

        :param parent_node:

            The parental node from where to start (only applicable for node names)

        """"""
    faulty_names = ''
    if '<MASK>':
        faulty_names = '%s `overview` cannot be added directly under the root node this is a reserved keyword,' % faulty_names
    for split_name in split_names:
        if '<MASK>':
            faulty_names = '%s `%s` contains no characters, please use at least 1,' % (faulty_names, split_name)
        elif '<MASK>':
            faulty_names = '%s `%s` starts with a leading underscore,' % (faulty_names, split_name)
        elif '<MASK>':
            faulty_names = '%s `%s` contains non-admissible characters (use only [A-Za-z0-9_-]),' % (faulty_names, split_name)
        elif '<MASK>':
            if '<MASK>':
                faulty_names = '%s `%s` contains `$` but has no associated wildcard function,' % (faulty_names, split_name)
        elif '<MASK>':
            warnings.warn('`%s` is a method/attribute of the trajectory/treenode/naminginterface, you may not be able to access it via natural naming but only by using `[]` square bracket notation. ' % split_name, category=SyntaxWarning)
        elif '<MASK>':
            warnings.warn('`%s` is a python keyword, you may not be able to access it via natural naming but only by using `[]` square bracket notation. ' % split_name, category=SyntaxWarning)
    name = split_names[-1]
    if '<MASK>':
        faulty_names = '%s `%s` is too long the name can only have %d characters but it has %d,' % (faulty_names, name, len(name), pypetconstants.HDF5_STRCOL_MAX_NAME_LENGTH)
    return faulty_names",False,"[""parent_node is not None and parent_node.v_is_root and (split_names[0] == 'overview')"", 'len(name) >= pypetconstants.HDF5_STRCOL_MAX_NAME_LENGTH', 'len(split_name) == 0', ""split_name.startswith('_')"", 're.match(CHECK_REGEXP, split_name) is None', ""'$' in split_name"", 'split_name not in self._root_instance._wildcard_keys', 'split_name in self._not_admissible_names', 'split_name in self._python_keywords']","__names,__)
    """""" """"""
    """"""..
    """"""",0.0
"def get_annotation_data_for_tier(self, id_tier):
    """"""Gives a list of annotations of the form: ``(begin, end, value)``
        When the tier contains reference annotations this will be returned,
        check :func:`get_ref_annotation_data_for_tier` for the format.

        :param str id_tier: Name of the tier.
        :raises KeyError: If the tier is non existent.
        """"""
    if '<MASK>':
        return self.get_ref_annotation_data_for_tier(id_tier)
    a = self.tiers[id_tier][0]
    return [(self.timeslots[a[b][0]], self.timeslots[a[b][1]], a[b][2]) for b in a]",False,['self.tiers[id_tier][1]'],"_____t
        """""" """"""
        """"""
    """"""
    """"""",0.0
"def first(self):
    """"""
        Returns only the first result from the query, if any.
        """"""
    lim = [0, 1]
    if '<MASK>':
        lim[0] = self._limit[0]
    if '<MASK>':
        for ent in self:
            return ent
        return None
    ids = self.limit(*lim)._search()
    if '<MASK>':
        return self._model.get(ids[0])
    return None",False,"['self._limit', 'not self._filters and (not self._order_by)', 'ids']","_,,,,,,,,,,,,,,,,,,",0.0
"def recv(self, bufsiz, flags=None):
    """"""
        Receive data on the connection.

        :param bufsiz: The maximum number of bytes to read
        :param flags: (optional) The only supported flag is ``MSG_PEEK``,
            all other flags are ignored.
        :return: The string read from the Connection
        """"""
    buf = _no_zero_allocator('char[]', bufsiz)
    if '<MASK>':
        result = _lib.SSL_peek(self._ssl, buf, bufsiz)
    else:
        result = _lib.SSL_read(self._ssl, buf, bufsiz)
    self._raise_ssl_error(self._ssl, result)
    return _ffi.buffer(buf, result)[:]",False,['flags is not None and flags & socket.MSG_PEEK'],(((((((((((((((((((,0.0
"def sign_inputs(self, key_generator):
    """"""
        Sign inputs in a finalized bundle.
        """"""
    if '<MASK>':
        raise RuntimeError('Cannot sign inputs until bundle is finalized.')
    i = 0
    while i < len(self):
        txn = self[i]
        if '<MASK>':
            if '<MASK>':
                raise with_context(exc=ValueError('Unable to sign input {input}; ``key_index`` is None (``exc.context`` has more info).'.format(input=txn.address)), context={'transaction': txn})
            if '<MASK>':
                raise with_context(exc=ValueError('Unable to sign input {input}; ``security_level`` is None (``exc.context`` has more info).'.format(input=txn.address)), context={'transaction': txn})
            self.sign_input_at(i, key_generator.get_key_for(txn.address))
            i += txn.address.security_level
        else:
            i += 1",False,"['not self.hash', 'txn.value < 0', 'txn.address.key_index is None', 'txn.address.security_level is None']","_,,_,,_,,_,,_,,_,,_",0.0
"def content(self):
    """"""
        The whole response content.
        """"""
    if '<MASK>':
        content = self.httplib_response.read()
        if '<MASK>':
            self._content = content
        else:
            self._content = content.decode('utf-8')
    return self._content",False,"['not self._content', 'self.is_binary_string(content)']","
        """"""
        """"""
        """"""
        """"""
        """"""
        """"""
",0.0
"def get_all_terms(self):
    """"""
        Return all of the terms in the account.
        https://canvas.instructure.com/doc/api/enrollment_terms.html#method.terms_api.index
        """"""
    if '<MASK>':
        raise MissingAccountID()
    params = {'workflow_state': 'all', 'per_page': 500}
    url = ACCOUNTS_API.format(self._canvas_account_id) + '/terms'
    data_key = 'enrollment_terms'
    terms = []
    response = self._get_paged_resource(url, params, data_key)
    for data in response[data_key]:
        terms.append(CanvasTerm(data=data))
    return terms",False,['not self._canvas_account_id'],____))))))))))))))),0.0
"def progress(msg, *args):
    """"""Show current progress message to stderr.
     This function will remember the previous message so that next time,
     it will clear the previous message before showing next one.
  """"""
    if '<MASK>':
        return
    text = msg % args
    if '<MASK>':
        sys.stderr.write(' ' * len(progress.prev_message) + '\r')
    sys.stderr.write(text + '\r')
    progress.prev_message = text",False,"['not (sys.stdout.isatty() and sys.stderr.isatty())', 'progress.prev_message']",___________________,0.0
"def select_best_block(blocks):
    """"""Return best block selected based on simple heuristic.""""""
    if '<MASK>':
        raise ValueError('No suitable blocks were found in assembly.')
    best_block = max(blocks, key=lambda b: b[1]['packed_instr'])
    if '<MASK>':
        best_block = max(blocks, key=lambda b: (b[1]['ops'] + b[1]['packed_instr'] + b[1]['avx_instr'], b[1]['ZMM'], b[1]['YMM'], b[1]['XMM']))
    return best_block[0]",False,"['not blocks', ""best_block[1]['packed_instr'] == 0""]",___________________,0.0
"def plot_labels(all_labels, gt_times, est_file, algo_ids=None, title=None, output_file=None):
    """"""Plots all the labels.

    Parameters
    ----------
    all_labels: list
        A list of np.arrays containing the labels of the boundaries, one array
        for each algorithm.
    gt_times: np.array
        Array with the ground truth boundaries.
    est_file: str
        Path to the estimated file (JSON file)
    algo_ids : list
        List of algorithm ids to to read boundaries from.
        If None, all algorithm ids are read.
    title : str
        Title of the plot. If None, the name of the file is printed instead.
    """"""
    import matplotlib.pyplot as plt
    N = len(all_labels)
    if '<MASK>':
        algo_ids = io.get_algo_ids(est_file)
    for i, algo_id in enumerate(algo_ids):
        algo_ids[i] = translate_ids[algo_id]
    algo_ids = ['GT'] + algo_ids
    for i, labels in enumerate(all_labels):
        all_labels[i] = mir_eval.util.index_labels(labels)[0]
    cm = plt.get_cmap('gist_rainbow')
    max_label = max((max(labels) for labels in all_labels))
    gt_inters = utils.times_to_intervals(gt_times)
    figsize = (6, 4)
    plt.figure(1, figsize=figsize, dpi=120, facecolor='w', edgecolor='k')
    for i, labels in enumerate(all_labels):
        for label, inter in zip(labels, gt_inters):
            plt.axvspan(inter[0], inter[1], ymin=i / float(N), ymax=(i + 1) / float(N), alpha=0.6, color=cm(label / float(max_label)))
        plt.axhline(i / float(N), color='k', linewidth=1)
    for bound in gt_times:
        plt.axvline(bound, color='g')
    _plot_formatting(title, est_file, algo_ids, gt_times[-1], N, output_file)",False,['algo_ids is None'],___________________,0.0
"def get_url_from_image_key(image_instance, image_key):
    """"""Build a URL from `image_key`.""""""
    img_key_split = image_key.split('__')
    if '<MASK>':
        size_key = img_key_split.pop(-1)
    else:
        size_key = None
    img_url = reduce(getattr, img_key_split, image_instance)
    if '<MASK>':
        img_url = img_url[size_key].url
    return img_url",False,"[""'x' in img_key_split[-1]"", 'size_key']",_key_key_key_key_key_key_key_key_key_,0.0
"def parsed_aggregate_reports_to_csv(reports):
    """"""
    Converts one or more parsed aggregate reports to flat CSV format, including
    headers

    Args:
        reports: A parsed aggregate report or list of parsed aggregate reports

    Returns:
        str: Parsed aggregate report data in flat CSV format, including headers
    """"""

    def to_str(obj):
        return str(obj).lower()
    fields = ['xml_schema', 'org_name', 'org_email', 'org_extra_contact_info', 'report_id', 'begin_date', 'end_date', 'errors', 'domain', 'adkim', 'aspf', 'p', 'sp', 'pct', 'fo', 'source_ip_address', 'source_country', 'source_reverse_dns', 'source_base_domain', 'count', 'disposition', 'dkim_alignment', 'spf_alignment', 'policy_override_reasons', 'policy_override_comments', 'envelope_from', 'header_from', 'envelope_to', 'dkim_domains', 'dkim_selectors', 'dkim_results', 'spf_domains', 'spf_scopes', 'spf_results']
    csv_file_object = StringIO(newline='\n')
    writer = DictWriter(csv_file_object, fields)
    writer.writeheader()
    if '<MASK>':
        reports = [reports]
    for report in reports:
        xml_schema = report['xml_schema']
        org_name = report['report_metadata']['org_name']
        org_email = report['report_metadata']['org_email']
        org_extra_contact = report['report_metadata']['org_extra_contact_info']
        report_id = report['report_metadata']['report_id']
        begin_date = report['report_metadata']['begin_date']
        end_date = report['report_metadata']['end_date']
        errors = '|'.join(report['report_metadata']['errors'])
        domain = report['policy_published']['domain']
        adkim = report['policy_published']['adkim']
        aspf = report['policy_published']['aspf']
        p = report['policy_published']['p']
        sp = report['policy_published']['sp']
        pct = report['policy_published']['pct']
        fo = report['policy_published']['fo']
        report_dict = dict(xml_schema=xml_schema, org_name=org_name, org_email=org_email, org_extra_contact_info=org_extra_contact, report_id=report_id, begin_date=begin_date, end_date=end_date, errors=errors, domain=domain, adkim=adkim, aspf=aspf, p=p, sp=sp, pct=pct, fo=fo)
        for record in report['records']:
            row = report_dict
            row['source_ip_address'] = record['source']['ip_address']
            row['source_country'] = record['source']['country']
            row['source_reverse_dns'] = record['source']['reverse_dns']
            row['source_base_domain'] = record['source']['base_domain']
            row['count'] = record['count']
            row['disposition'] = record['policy_evaluated']['disposition']
            row['spf_alignment'] = record['policy_evaluated']['spf']
            row['dkim_alignment'] = record['policy_evaluated']['dkim']
            policy_override_reasons = list(map(lambda r: r['type'], record['policy_evaluated']['policy_override_reasons']))
            policy_override_comments = list(map(lambda r: r['comment'] or 'none', record['policy_evaluated']['policy_override_reasons']))
            row['policy_override_reasons'] = ','.join(policy_override_reasons)
            row['policy_override_comments'] = '|'.join(policy_override_comments)
            row['envelope_from'] = record['identifiers']['envelope_from']
            row['header_from'] = record['identifiers']['header_from']
            envelope_to = record['identifiers']['envelope_to']
            row['envelope_to'] = envelope_to
            dkim_domains = []
            dkim_selectors = []
            dkim_results = []
            for dkim_result in record['auth_results']['dkim']:
                dkim_domains.append(dkim_result['domain'])
                if '<MASK>':
                    dkim_selectors.append(dkim_result['selector'])
                dkim_results.append(dkim_result['result'])
            row['dkim_domains'] = ','.join(map(to_str, dkim_domains))
            row['dkim_selectors'] = ','.join(map(to_str, dkim_selectors))
            row['dkim_results'] = ','.join(map(to_str, dkim_results))
            spf_domains = []
            spf_scopes = []
            spf_results = []
            for spf_result in record['auth_results']['spf']:
                spf_domains.append(spf_result['domain'])
                spf_scopes.append(spf_result['scope'])
                spf_results.append(spf_result['result'])
            row['spf_domains'] = ','.join(map(to_str, spf_domains))
            row['spf_scopes'] = ','.join(map(to_str, spf_scopes))
            row['spf_results'] = ','.join(map(to_str, dkim_results))
            writer.writerow(row)
            csv_file_object.flush()
    return csv_file_object.getvalue()",False,"['type(reports) == OrderedDict', ""'selector' in dkim_result""]",___________________,0.0
"def partition(coll, n: int):
    """"""Partition coll into groups of size n.""""""
    assert n > 0
    start = 0
    stop = n
    while stop <= len(coll):
        yield tuple((e for e in coll[start:stop]))
        start += n
        stop += n
    if '<MASK>':
        stop = len(coll)
        yield tuple((e for e in coll[start:stop]))",False,['start < len(coll) < stop'],(((((((((((((((((((,0.0
"@self.synchronize(wait=query.wait)
def assert_style():
    if '<MASK>':
        raise ExpectationNotMet(query.failure_message)
    return True",False,['not query.resolves_for(self)'],(((((((((((((((((((,0.0
"def valid(self):
    """"""
        ``True`` if credentials are valid, ``False`` if expired.
        """"""
    if '<MASK>':
        return self.expiration_time > int(time.time())
    else:
        return True",False,['self.expiration_time'],"(self):
    """"""
    """"""
    """"""
    """"""
    """"""
",0.0
"def _basename_in_blacklist_re(base_name, black_list_re):
    """"""Determines if the basename is matched in a regex blacklist

    :param str base_name: The basename of the file
    :param list black_list_re: A collection of regex patterns to match against.
        Successful matches are blacklisted.

    :returns: `True` if the basename is blacklisted, `False` otherwise.
    :rtype: bool
    """"""
    for file_pattern in black_list_re:
        if '<MASK>':
            return True
    return False",False,['file_pattern.match(base_name)'],"_,,,,,,,,,,,,,,,,,,",0.0
"def read(self, input_buffer, kmip_version=enums.KMIPVersion.KMIP_1_0):
    """"""
        Read the data encoding the Create request payload and decode it into
        its constituent parts.

        Args:
            input_buffer (stream): A data buffer containing encoded object
                data, supporting a read method.
            kmip_version (KMIPVersion): An enumeration defining the KMIP
                version with which the object will be decoded. Optional,
                defaults to KMIP 1.0.

        Raises:
            InvalidKmipEncoding: Raised if the object type or template
                attribute is missing from the encoded payload.
        """"""
    super(CreateRequestPayload, self).read(input_buffer, kmip_version=kmip_version)
    local_buffer = utils.BytearrayStream(input_buffer.read(self.length))
    if '<MASK>':
        self._object_type = primitives.Enumeration(enums.ObjectType, tag=enums.Tags.OBJECT_TYPE)
        self._object_type.read(local_buffer, kmip_version=kmip_version)
    else:
        raise exceptions.InvalidKmipEncoding('The Create request payload encoding is missing the object type.')
    if '<MASK>':
        if '<MASK>':
            self._template_attribute = objects.TemplateAttribute()
            self._template_attribute.read(local_buffer, kmip_version=kmip_version)
        else:
            raise exceptions.InvalidKmipEncoding('The Create request payload encoding is missing the template attribute.')
    elif '<MASK>':
        attributes = objects.Attributes()
        attributes.read(local_buffer, kmip_version=kmip_version)
        value = objects.convert_attributes_to_template_attribute(attributes)
        self._template_attribute = value
    else:
        raise exceptions.InvalidKmipEncoding('The Create request payload encoding is missing the attributes structure.')
    self.is_oversized(local_buffer)",False,"['self.is_tag_next(enums.Tags.OBJECT_TYPE, local_buffer)', 'kmip_version < enums.KMIPVersion.KMIP_2_0', 'self.is_tag_next(enums.Tags.TEMPLATE_ATTRIBUTE, local_buffer)', 'self.is_tag_next(enums.Tags.ATTRIBUTES, local_buffer)']",___________________,0.0
"def _unlock(self, name, client_id, request_id):
    """"""Handles unlocking""""""
    if '<MASK>':
        other_client_id, other_request_id, lock_time = self._locks[name]
        if '<MASK>':
            response = self.RELEASE_ERROR + self.DELIMITER + 'Lock `%s` was acquired by `%s` (old request id `%s`) and not by `%s` (request id `%s`)' % (name, other_client_id, other_request_id, client_id, request_id)
            self._logger.error(response)
            return response
        else:
            del self._locks[name]
            return self.RELEASED
    elif '<MASK>':
        other_request_id, lock_time = self._timeout_locks[name, client_id]
        timeout = time.time() - lock_time - self._timeout
        response = self.RELEASE_ERROR + self.DELIMITER + 'Lock `%s` timed out %f seconds ago (client id `%s`, old request id `%s`)' % (name, timeout, client_id, other_request_id)
        return response
    else:
        response = self.RELEASE_ERROR + self.DELIMITER + 'Lock `%s` cannot be found in database (client id `%s`, request id `%s`)' % (name, client_id, request_id)
        self._logger.warning(response)
        return response",False,"['name in self._locks', 'other_client_id != client_id', '(name, client_id) in self._timeout_locks']","__id,__id,__id,__id,__id",0.0
"def results_equal(a, b):
    """"""Compares two result instances

    Checks full name and all data. Does not consider the comment.

    :return: True or False

    :raises: ValueError if both inputs are no result instances

    """"""
    if '<MASK>':
        raise ValueError('Both inputs are not results.')
    if '<MASK>':
        return False
    if '<MASK>':
        return False
    if '<MASK>':
        return False
    if '<MASK>':
        akeyset = set(a._data.keys())
        bkeyset = set(b._data.keys())
        if '<MASK>':
            return False
        for key in a._data:
            val = a._data[key]
            bval = b._data[key]
            if '<MASK>':
                return False
    return True",False,"['a.v_is_parameter and b.v_is_parameter', 'a.v_is_parameter or b.v_is_parameter', 'a.v_full_name != b.v_full_name', ""hasattr(a, '_data') and (not hasattr(b, '_data'))"", ""hasattr(a, '_data')"", 'akeyset != bkeyset', 'not nested_equal(val, bval)']",(((((((((((((((((((,0.0
"def parse_geometry(ml_log, log=None, ml_version='2016.12', print_output=False):
    """"""Parse the ml_log file generated by the measure_geometry function.

    Warnings: Not all keys may exist if mesh is not watertight or manifold

    Args:
        ml_log (str): MeshLab log file to parse
        log (str): filename to log output
    """"""
    aabb = {}
    geometry = {'aabb': aabb}
    with open(ml_log) as fread:
        for line in fread:
            if '<MASK>':
                geometry['aabb']['min'] = line.split()[4:7]
                geometry['aabb']['min'] = [util.to_float(val) for val in geometry['aabb']['min']]
            if '<MASK>':
                geometry['aabb']['max'] = line.split()[4:7]
                geometry['aabb']['max'] = [util.to_float(val) for val in geometry['aabb']['max']]
            if '<MASK>':
                geometry['aabb']['size'] = line.split()[4:7]
                geometry['aabb']['size'] = [util.to_float(val) for val in geometry['aabb']['size']]
            if '<MASK>':
                geometry['aabb']['diagonal'] = util.to_float(line.split()[4])
            if '<MASK>':
                geometry['volume_mm3'] = util.to_float(line.split()[3])
                geometry['volume_cm3'] = geometry['volume_mm3'] * 0.001
            if '<MASK>':
                if '<MASK>':
                    geometry['area_mm2'] = util.to_float(line.split()[3])
                else:
                    geometry['area_mm2'] = util.to_float(line.split()[4])
                geometry['area_cm2'] = geometry['area_mm2'] * 0.01
            if '<MASK>':
                if '<MASK>':
                    geometry['total_edge_length_incl_faux'] = util.to_float(line.split()[7])
                else:
                    geometry['total_edge_length'] = util.to_float(line.split()[7])
            if '<MASK>':
                geometry['barycenter'] = line.split()[3:6]
                geometry['barycenter'] = [util.to_float(val) for val in geometry['barycenter']]
            if '<MASK>':
                geometry['barycenter'] = line.split()[4:7]
                geometry['barycenter'] = [util.to_float(val) for val in geometry['barycenter']]
            if '<MASK>':
                geometry['vert_barycenter'] = line.split()[2:5]
                geometry['vert_barycenter'] = [util.to_float(val) for val in geometry['vert_barycenter']]
            if '<MASK>':
                geometry['center_of_mass'] = line.split()[4:7]
                geometry['center_of_mass'] = [util.to_float(val) for val in geometry['center_of_mass']]
            if '<MASK>':
                geometry['inertia_tensor'] = []
                for val in range(3):
                    row = next(fread, val).split()[1:4]
                    row = [util.to_float(b) for b in row]
                    geometry['inertia_tensor'].append(row)
            if '<MASK>':
                geometry['principal_axes'] = []
                for val in range(3):
                    row = next(fread, val).split()[1:4]
                    row = [util.to_float(b) for b in row]
                    geometry['principal_axes'].append(row)
            if '<MASK>':
                geometry['axis_momenta'] = next(fread).split()[1:4]
                geometry['axis_momenta'] = [util.to_float(val) for val in geometry['axis_momenta']]
                break
    for key, value in geometry.items():
        if '<MASK>':
            log_file = open(log, 'a')
            log_file.write('{:27} = {}\n'.format(key, value))
            log_file.close()
        elif '<MASK>':
            print('{:27} = {}'.format(key, value))
    return geometry",False,"['log is not None', ""'Mesh Bounding Box min' in line"", ""'Mesh Bounding Box max' in line"", ""'Mesh Bounding Box Size' in line"", ""'Mesh Bounding Box Diag' in line"", ""'Mesh Volume' in line"", ""'Mesh Surface' in line"", ""'Mesh Total Len of' in line"", ""'Thin shell barycenter' in line"", ""'Thin shell (faces) barycenter' in line"", ""'Vertices barycenter' in line"", ""'Center of Mass' in line"", ""'Inertia Tensor' in line"", ""'Principal axes' in line"", ""'axis momenta' in line"", 'print_output', ""ml_version == '1.3.4BETA'"", ""'including faux edges' in line""]",___________________,0.0
"def override_span_name(self, name):
    """"""Overrides the current span name.

        This is useful if you don't know the span name yet when you create the
        zipkin_span object. i.e. pyramid_zipkin doesn't know which route the
        request matched until the function wrapped by the context manager
        completes.

        :param name: New span name
        :type name: str
        """"""
    self.span_name = name
    if '<MASK>':
        self.logging_context.span_name = name",False,['self.logging_context'],__name_name_name_name_name_name_name_name_name,0.0
"def _clean(self, rmConnetions=True, lockNonExternal=True):
    """"""
        Remove all signals from this interface (used after unit is synthesized
        and its parent is connecting its interface to this unit)
        """"""
    if '<MASK>':
        for i in self._interfaces:
            i._clean(rmConnetions=rmConnetions, lockNonExternal=lockNonExternal)
    else:
        self._sigInside = self._sig
        del self._sig
    if '<MASK>':
        self._isAccessible = False",False,"['self._interfaces', 'lockNonExternal and (not self._isExtern)']",___________________,0.0
"def _evaluate_convolution(t_val, f, g, n_integral=100, inverse_time=None, return_log=False):
    """"""
    Calculate convolution F(t) = int { f(tau)g(t-tau) } dtau
    """"""
    FG = _convolution_integrand(t_val, f, g, inverse_time, return_log)
    if '<MASK>':
        res = ttconf.BIG_NUMBER
    else:
        res = -FG.integrate(a=FG.xmin, b=FG.xmax, n=n_integral, return_log=True)
    if '<MASK>':
        return (res, -1)
    else:
        return (np.exp(-res), -1)",False,"['return_log and FG == ttconf.BIG_NUMBER or (not return_log and FG == 0.0)', 'return_log']",___________________,0.0
"def __call_api(self, verb, path, **kargs):
    """"""
        Make a HTTP request to the Dominos UK API with the given parameters for
        the current session.

        :param verb func: HTTP method on the session.
        :param string path: The API endpoint path.
        :params list kargs: A list of arguments.
        :return: A response from the Dominos UK API.
        :rtype: response.Response
        """"""
    response = verb(self.__url(path), **kargs)
    if '<MASK>':
        raise ApiError('{}: {}'.format(response.status_code, response))
    return response",False,['response.status_code != 200'],___________________,0.0
"def album(self, spotify_id, market='US'):
    """"""Get a spotify album by its ID.

        Parameters
        ----------
        spotify_id : str
            The spotify_id to search by.
        market : Optional[str]
            An ISO 3166-1 alpha-2 country code.

        Returns
        -------
        album : Dict
            The album object.
        """"""
    route = Route('GET', '/albums/{spotify_id}', spotify_id=spotify_id)
    payload = {}
    if '<MASK>':
        payload['market'] = market
    return self.request(route, params=payload)",False,['market'],"_id,,,,,,,,,,,,,,,,,",0.0
"def _construct_schema(elements, nsmap):
    """"""Consruct fiona schema based on given elements

    :param list Element: list of elements
    :param dict nsmap: namespace map

    :return dict: schema
    """"""
    schema = {'properties': {}, 'geometry': None}
    schema_key = None
    gml_key = None
    if '<MASK>':
        for key in nsmap:
            if '<MASK>':
                schema_key = key
            if '<MASK>':
                gml_key = key
    else:
        gml_key = 'gml'
        schema_key = 'xsd'
    mappings = {'PointPropertyType': 'Point', 'PolygonPropertyType': 'Polygon', 'LineStringPropertyType': 'LineString', 'MultiPointPropertyType': 'MultiPoint', 'MultiLineStringPropertyType': 'MultiLineString', 'MultiPolygonPropertyType': 'MultiPolygon', 'MultiGeometryPropertyType': 'MultiGeometry', 'GeometryPropertyType': 'GeometryCollection', 'SurfacePropertyType': '3D Polygon', 'MultiSurfacePropertyType': '3D MultiPolygon'}
    for element in elements:
        data_type = element.attrib['type'].replace(gml_key + ':', '')
        name = element.attrib['name']
        if '<MASK>':
            schema['geometry'] = mappings[data_type]
            schema['geometry_column'] = name
        else:
            schema['properties'][name] = data_type.replace(schema_key + ':', '')
    if '<MASK>':
        return schema
    else:
        return None",False,"['nsmap', ""schema['properties'] or schema['geometry']"", 'data_type in mappings', 'nsmap[key] == XS_NAMESPACE', 'nsmap[key] in GML_NAMESPACES']",___________________,0.0
"def get_system_config_dirs(app_name, app_author, force_xdg=True):
    """"""Returns a list of system-wide config folders for the application.

    For an example application called ``""My App""`` by ``""Acme""``,
    something like the following folders could be returned:

    macOS (non-XDG):
      ``['/Library/Application Support/My App']``
    Mac OS X (XDG):
      ``['/etc/xdg/my-app']``
    Unix:
      ``['/etc/xdg/my-app']``
    Windows 7:
      ``['C:\\ProgramData\\Acme\\My App']``

    :param app_name: the application name. This should be properly capitalized
                     and can contain whitespace.
    :param app_author: The app author's name (or company). This should be
                       properly capitalized and can contain whitespace.
    :param force_xdg: if this is set to `True`, then on macOS the XDG Base
                      Directory Specification will be followed. Has no effect
                      on non-macOS systems.

    """"""
    if '<MASK>':
        folder = os.environ.get('PROGRAMDATA')
        return [os.path.join(folder, app_author, app_name)]
    if '<MASK>':
        return [os.path.join('/Library/Application Support', app_name)]
    dirs = os.environ.get('XDG_CONFIG_DIRS', '/etc/xdg')
    paths = [os.path.expanduser(x) for x in dirs.split(os.pathsep)]
    return [os.path.join(d, _pathify(app_name)) for d in paths]",False,"['WIN', 'MAC and (not force_xdg)']",___________________,0.0
"def worker_execute(self, nick, message, channel, task_id, command, workers=None):
    """"""        Work on a task from the BotnetBot
        """"""
    if '<MASK>':
        nicks = workers.split(',')
        do_task = self.conn.nick in nicks
    else:
        do_task = True
    if '<MASK>':
        self.task_queue.put((int(task_id), command))
        return '!task-received %s' % task_id",False,"['workers', 'do_task']",___________________,0.0
"def priority(self):
    """"""
        Applicable for all platforms, where the schemes, that are integrated
        with your environment, does not fit.
        """"""
    try:
        __import__('argon2.low_level')
    except ImportError:
        raise RuntimeError('argon2_cffi package required')
    try:
        __import__('Crypto.Cipher.AES')
    except ImportError:
        raise RuntimeError('PyCryptodome package required')
    if '<MASK>':
        raise RuntimeError('JSON implementation such as simplejson required.')
    return 2.5",False,['not json'],___________________,0.0
"def getTokenInfo(self, slot):
    """"""
        C_GetTokenInfo

        :param slot: slot number returned by :func:`getSlotList`
        :type slot: integer
        :return: a :class:`CK_TOKEN_INFO` object
        """"""
    tokeninfo = PyKCS11.LowLevel.CK_TOKEN_INFO()
    rv = self.lib.C_GetTokenInfo(slot, tokeninfo)
    if '<MASK>':
        raise PyKCS11Error(rv)
    t = CK_TOKEN_INFO()
    t.label = tokeninfo.GetLabel()
    t.manufacturerID = tokeninfo.GetManufacturerID()
    t.model = tokeninfo.GetModel()
    t.serialNumber = tokeninfo.GetSerialNumber()
    t.flags = tokeninfo.flags
    t.ulMaxSessionCount = tokeninfo.ulMaxSessionCount
    if '<MASK>':
        t.ulMaxSessionCount = -1
    t.ulSessionCount = tokeninfo.ulSessionCount
    if '<MASK>':
        t.ulSessionCount = -1
    t.ulMaxRwSessionCount = tokeninfo.ulMaxRwSessionCount
    if '<MASK>':
        t.ulMaxRwSessionCount = -1
    t.ulRwSessionCount = tokeninfo.ulRwSessionCount
    if '<MASK>':
        t.ulRwSessionCount = -1
    t.ulMaxPinLen = tokeninfo.ulMaxPinLen
    t.ulMinPinLen = tokeninfo.ulMinPinLen
    t.ulTotalPublicMemory = tokeninfo.ulTotalPublicMemory
    if '<MASK>':
        t.ulTotalPublicMemory = -1
    t.ulFreePublicMemory = tokeninfo.ulFreePublicMemory
    if '<MASK>':
        t.ulFreePublicMemory = -1
    t.ulTotalPrivateMemory = tokeninfo.ulTotalPrivateMemory
    if '<MASK>':
        t.ulTotalPrivateMemory = -1
    t.ulFreePrivateMemory = tokeninfo.ulFreePrivateMemory
    if '<MASK>':
        t.ulFreePrivateMemory = -1
    t.hardwareVersion = (tokeninfo.hardwareVersion.major, tokeninfo.hardwareVersion.minor)
    t.firmwareVersion = (tokeninfo.firmwareVersion.major, tokeninfo.firmwareVersion.minor)
    t.utcTime = tokeninfo.GetUtcTime().replace('\x00', ' ')
    return t",False,"['rv != CKR_OK', 't.ulMaxSessionCount == CK_UNAVAILABLE_INFORMATION', 't.ulSessionCount == CK_UNAVAILABLE_INFORMATION', 't.ulMaxRwSessionCount == CK_UNAVAILABLE_INFORMATION', 't.ulRwSessionCount == CK_UNAVAILABLE_INFORMATION', 't.ulTotalPublicMemory == CK_UNAVAILABLE_INFORMATION', 't.ulFreePublicMemory == CK_UNAVAILABLE_INFORMATION', 't.ulTotalPrivateMemory == CK_UNAVAILABLE_INFORMATION', 't.ulFreePrivateMemory == CK_UNAVAILABLE_INFORMATION']","_,,,,,,,,,,,,,,,,,,",0.0
"def rename(script, label='blank', layer_num=None):
    """""" Rename layer label

    Can be useful for outputting mlp files, as the output file names use
    the labels.

    Args:
        script: the mlx.FilterScript object or script filename to write
            the filter to.
        label (str): new label for the mesh layer
        layer_num (int): layer number to rename. Default is the
            current layer. Not supported on the file base API.

    Layer stack:
        Renames a layer

    MeshLab versions:
        2016.12
        1.3.4BETA
    """"""
    filter_xml = ''.join(['  <filter name=""Rename Current Mesh"">\n', '    <Param name=""newName"" ', 'value=""{}"" '.format(label), 'description=""New Label"" ', 'type=""RichString"" ', '/>\n', '  </filter>\n'])
    if '<MASK>':
        if '<MASK>':
            util.write_filter(script, filter_xml)
            script.layer_stack[script.current_layer()] = label
        else:
            cur_layer = script.current_layer()
            change(script, layer_num)
            util.write_filter(script, filter_xml)
            change(script, cur_layer)
            script.layer_stack[layer_num] = label
    else:
        util.write_filter(script, filter_xml)
    return None",False,"['isinstance(script, mlx.FilterScript)', 'layer_num is None or layer_num == script.current_layer()']","_,,,,,,,,,,,,,,,,,,",0.0
"def itertrain(self, train, valid=None, **kwargs):
    """"""Train a model using a training and validation set.

        This method yields a series of monitor values to the caller. After every
        iteration, a pair of monitor dictionaries is generated: one evaluated on
        the training dataset, and another evaluated on the validation dataset.
        The validation monitors might not be updated during every training
        iteration; in this case, the most recent validation monitors will be
        yielded along with the training monitors.

        Parameters
        ----------
        train : :class:`Dataset <theanets.dataset.Dataset>`
            A set of training data for computing updates to model parameters.
        valid : :class:`Dataset <theanets.dataset.Dataset>`
            A set of validation data for computing monitor values and
            determining when the loss has stopped improving.

        Yields
        ------
        training : dict
            A dictionary mapping monitor names to values, evaluated on the
            training dataset.
        validation : dict
            A dictionary containing monitor values evaluated on the validation
            dataset.
        """"""
    from . import feedforward
    original_layer_names = set((l.name for l in self.network.layers[:-1]))
    layers_ = list((l.to_spec() for l in self.network.layers[:-1]))
    for i, l in enumerate(layers_[::-1][:-2]):
        layers_.append(dict(form='tied', partner=l['name'], activation=l['activation']))
    layers_.append(dict(form='tied', partner=layers_[1]['name'], activation='linear'))
    util.log('creating shadow network')
    ae = feedforward.Autoencoder(layers=layers_)
    pre = SupervisedPretrainer(self.algo, ae)
    for monitors in pre.itertrain(train, valid, **kwargs):
        yield monitors
    for param in ae.params:
        l, p = param.name.split('.')
        if '<MASK>':
            util.log('copying pretrained parameter {}', param.name)
            self.network.find(l, p).set_value(param.get_value())
    util.log('completed unsupervised pretraining')",False,['l in original_layer_names'],",,,,,,,,,,,,,,,,,,,",0.0
"def put(self, item):
    """"""Put an item into the queue. Items should be comparable, eg. tuples.
        True - if item placed in queue.
        False - if queue is full and item can not be placed.""""""
    if '<MASK>':
        return False
    heapq.heappush(self.__data, item)
    return True",False,['self.__maxsize and len(self.__data) >= self.__maxsize'],___________________,0.0
"def ancestral_likelihood(self):
    """"""
        Calculate the likelihood of the given realization of the sequences in
        the tree

        Returns
        -------

         log_lh : float
            The tree likelihood given the sequences
        """"""
    log_lh = np.zeros(self.multiplicity.shape[0])
    for node in self.tree.find_clades(order='postorder'):
        if '<MASK>':
            profile = seq2prof(node.cseq, self.gtr.profile_map)
            profile *= self.gtr.Pi
            profile = profile.sum(axis=1)
            log_lh += np.log(profile)
            continue
        t = node.branch_length
        indices = np.array([(np.argmax(self.gtr.alphabet == a), np.argmax(self.gtr.alphabet == b)) for a, b in zip(node.up.cseq, node.cseq)])
        logQt = np.log(self.gtr.expQt(t))
        lh = logQt[indices[:, 1], indices[:, 0]]
        log_lh += lh
    return log_lh",False,['node.up is None'],",,,,,,,,,,,,,,,,,,,",0.0
"def get(self, request, *args, **kwargs):
    """"""
        Render a fragment to HTML or return JSON describing it, based on the request.
        """"""
    fragment = self.render_to_fragment(request, **kwargs)
    response_format = request.GET.get('format') or request.POST.get('format') or 'html'
    if '<MASK>':
        return JsonResponse(fragment.to_dict())
    else:
        return self.render_standalone_response(request, fragment, **kwargs)",False,"[""response_format == 'json' or WEB_FRAGMENT_RESPONSE_TYPE in request.META.get('HTTP_ACCEPT', 'text/html')""]",",,,,,,,,,,,,,,,,,,,",0.0
"def _add_leaf(leaf):
    """"""Adds a leaf to the trajectory""""""
    leaf_full_name = leaf.v_full_name
    try:
        found_leaf = self.f_get(leaf_full_name, with_links=False, shortcuts=False, auto_load=False)
        if '<MASK>':
            found_leaf.__setstate__(leaf.__getstate__())
        return found_leaf
    except AttributeError:
        pass
    if '<MASK>':
        new_leaf = self.f_add_leaf(cp.copy(leaf))
    else:
        new_leaf = self.f_add_leaf(leaf)
    if '<MASK>':
        self._explored_parameters[new_leaf.v_full_name] = new_leaf
    return new_leaf",False,"[""copy_leaves is True or (copy_leaves == 'explored' and leaf.v_is_parameter and leaf.v_explored)"", 'new_leaf.v_is_parameter and new_leaf.v_explored', 'overwrite']",___________________,0.0
"def wrapper(func):

    @functools.wraps(func)
    def new_func(traj, *args, **kwargs):
        do_wrap = not traj._run_by_environment
        if '<MASK>':
            traj.f_start_run(turn_into_run=turn_into_run)
        result = func(traj, *args, **kwargs)
        if '<MASK>':
            traj.f_finalize_run(store_meta_data=store_meta_data, clean_up=clean_up)
        return result
    return new_func",False,"['do_wrap', 'do_wrap']",___________________,0.0
"def groupby(df, *, group_cols: Union[str, List[str]], aggregations: Dict[str, Union[str, List[str]]]):
    """"""
    Aggregate values by groups.

    ---

    ### Parameters

    *mandatory :*
    - `group_cols` (*list*): list of columns used to group data
    - `aggregations` (*dict*): dictionnary of values columns to group as keys and aggregation
      function to use as values (See the [list of aggregation functions](
      https://pandas.pydata.org/pandas-docs/stable/user_guide/groupby.html#aggregation))

    ---

    ### Example

    **Input**

    | ENTITY | YEAR | VALUE_1 | VALUE_2 |
    |:------:|:----:|:-------:|:-------:|
    |    A   | 2017 |    10   |    3    |
    |    A   | 2017 |    20   |    1    |
    |    A   | 2018 |    10   |    5    |
    |    A   | 2018 |    30   |    4    |
    |    B   | 2017 |    60   |    4    |
    |    B   | 2017 |    40   |    3    |
    |    B   | 2018 |    50   |    7    |
    |    B   | 2018 |    60   |    6    |

    ```cson
    groupby:
      group_cols: ['ENTITY', 'YEAR']
      aggregations:
        'VALUE_1': 'sum',
        'VALUE_2': 'mean'
    ```

    **Output**

    | ENTITY | YEAR | VALUE_1 | VALUE_2 |
    |:------:|:----:|:-------:|:-------:|
    |    A   | 2017 |    30   |   2.0   |
    |    A   | 2018 |    40   |   4.5   |
    |    B   | 2017 |   100   |   3.5   |
    |    B   | 2018 |   110   |   6.5   |

    """"""
    df = df.groupby(group_cols, as_index=False).agg(aggregations)
    if '<MASK>':
        level_0 = df.columns.get_level_values(0)
        level_1 = df.columns.get_level_values(1)
        new_columns = [f'{x}_{y}' if x else y for x, y in zip(level_1, level_0)]
        df.columns = new_columns
    return df",False,['df.columns.nlevels == 2'],___________________,0.0
"def send(self, message):
    """""" Send a message object

            :type message: data.OutgoingMessage
            :param message: The message to send
            :rtype: data.OutgoingMessage
            :returns: The sent message with populated fields
            :raises AssertionError: wrong provider name encountered (returned by the router, or provided to OutgoingMessage)
            :raises MessageSendError: generic errors
            :raises AuthError: provider authentication failed
            :raises LimitsError: sending limits exceeded
            :raises CreditError: not enough money on the account
        """"""
    provider_name = self._default_provider
    if '<MASK>':
        assert message.provider in self._providers, 'Unknown provider specified in OutgoingMessage.provideer: {}'.format(provider_name)
        provider = self.get_provider(message.provider)
    else:
        if '<MASK>':
            provider_name = self.router(message, *message.routing_values) or self._default_provider
            assert provider_name in self._providers, 'Routing function returned an unknown provider name: {}'.format(provider_name)
        provider = self.get_provider(provider_name)
    message.provider = provider.name
    message = provider.send(message)
    self.onSend(message)
    return message",False,"['message.provider is not None', 'message.routing_values is not None']",___________________,0.0
"def get_by_id(self, user_id='', record_ids=(None,)):
    """""" Function to get vlrs by user_id and/or record_ids.
        Always returns a list even if only one vlr matches the user_id and record_id

        >>> import pylas
        >>> from pylas.vlrs.known import ExtraBytesVlr, WktCoordinateSystemVlr
        >>> las = pylas.read(""pylastests/extrabytes.las"")
        >>> las.vlrs
        [<ExtraBytesVlr(extra bytes structs: 5)>]
        >>> las.vlrs.get(WktCoordinateSystemVlr.official_user_id())
        []
        >>> las.vlrs.get(WktCoordinateSystemVlr.official_user_id())[0]
        Traceback (most recent call last):
        IndexError: list index out of range
        >>> las.vlrs.get_by_id(ExtraBytesVlr.official_user_id())
        [<ExtraBytesVlr(extra bytes structs: 5)>]
        >>> las.vlrs.get_by_id(ExtraBytesVlr.official_user_id())[0]
        <ExtraBytesVlr(extra bytes structs: 5)>

        Parameters
        ----------
        user_id: str, optional
                 the user id
        record_ids: iterable of int, optional
                    THe record ids of the vlr(s) you wish to get

        Returns
        -------
        :py:class:`list`
            a list of vlrs matching the user_id and records_ids

        """"""
    if '<MASK>':
        return [vlr for vlr in self.vlrs if vlr.user_id == user_id and vlr.record_id in record_ids]
    else:
        return [vlr for vlr in self.vlrs if vlr.user_id == user_id or vlr.record_id in record_ids]",False,"[""user_id != '' and record_ids != (None,)""]",___________________,0.0
"def create_app():
    """"""Create Flask app.""""""
    config = load_config()
    app = Flask(__name__)
    app.config.from_object(config)
    app.wsgi_app = ProxyFix(app.wsgi_app)
    CsrfProtect(app)
    if '<MASK>':
        DebugToolbarExtension(app)
        app.wsgi_app = SharedDataMiddleware(app.wsgi_app, {'/pages': os.path.join(app.config.get('PROJECT_PATH'), 'application/pages')})
    else:
        app.logger.addHandler(logging.StreamHandler())
        app.logger.setLevel(logging.ERROR)
        if '<MASK>':
            from .utils.sentry import sentry
            sentry.init_app(app, dsn=app.config.get('SENTRY_DSN'))
        app.wsgi_app = SharedDataMiddleware(app.wsgi_app, {'/static': os.path.join(app.config.get('PROJECT_PATH'), 'output/static'), '/pkg': os.path.join(app.config.get('PROJECT_PATH'), 'output/pkg'), '/pages': os.path.join(app.config.get('PROJECT_PATH'), 'output/pages')})
    register_db(app)
    register_routes(app)
    register_jinja(app)
    register_error_handle(app)
    register_hooks(app)
    return app",False,"['app.debug or app.testing', ""app.config.get('SENTRY_DSN')""]","_,,,,,,,,,,,,,,,,,,",0.0
"def register_checker(self, checker):
    """"""register a new checker

        checker is an object implementing IRawChecker or / and IAstroidChecker
        """"""
    assert checker.priority <= 0, ""checker priority can't be >= 0""
    self._checkers[checker.name].append(checker)
    for r_id, r_title, r_cb in checker.reports:
        self.register_report(r_id, r_title, r_cb, checker)
    self.register_options_provider(checker)
    if '<MASK>':
        self.msgs_store.register_messages_from_checker(checker)
    checker.load_defaults()
    if '<MASK>':
        self.disable(checker.name)",False,"[""hasattr(checker, 'msgs')"", ""not getattr(checker, 'enabled', True)""]",___________________,0.0
"def append(self, values):
    """"""Insert a number of rows into the grid (and associated table)""""""
    if '<MASK>':
        row = GridRow(self, **values)
    else:
        row = GridRow(self, *values)
    list.append(self, row)
    self._grid_view.wx_obj.AppendRows(numRows=1)",False,"['isinstance(values, dict)']","(self,,,,,,,,,,,,,,,,,",0.0
"def itertrain(self, train, valid=None, **kwargs):
    """"""Train a model using a training and validation set.

        This method yields a series of monitor values to the caller. After every
        iteration, a pair of monitor dictionaries is generated: one evaluated on
        the training dataset, and another evaluated on the validation dataset.
        The validation monitors might not be updated during every training
        iteration; in this case, the most recent validation monitors will be
        yielded along with the training monitors.

        Parameters
        ----------
        train : :class:`Dataset <theanets.dataset.Dataset>`
            A set of training data for computing updates to model parameters.
        valid : :class:`Dataset <theanets.dataset.Dataset>`
            A set of validation data for computing monitor values and
            determining when the loss has stopped improving.

        Yields
        ------
        training : dict
            A dictionary mapping monitor names to values, evaluated on the
            training dataset.
        validation : dict
            A dictionary containing monitor values evaluated on the validation
            dataset.
        """"""
    ifci = itertools.chain.from_iterable

    def first(x):
        return x[0] if isinstance(x, (tuple, list)) else x

    def last(x):
        return x[-1] if isinstance(x, (tuple, list)) else x
    odim = idim = None
    for t in train:
        idim = first(t).shape[-1]
        odim = last(t).shape[-1]
    rng = kwargs.get('rng')
    if '<MASK>':
        rng = np.random.RandomState(rng)
    samples = ifci((last(t) for t in train))
    for param in self.network.layers[-1].params:
        shape = param.get_value(borrow=True).shape
        if '<MASK>':
            arr = np.vstack(SampleTrainer.reservoir(samples, shape[0], rng))
            util.log('setting {}: {}', param.name, shape)
            param.set_value(arr / np.sqrt((arr * arr).sum(axis=1))[:, None])
    samples = ifci((first(t) for t in train))
    for layer in self.network.layers:
        for param in layer.params:
            shape = param.get_value(borrow=True).shape
            if '<MASK>':
                arr = np.vstack(SampleTrainer.reservoir(samples, shape[1], rng)).T
                util.log('setting {}: {}', param.name, shape)
                param.set_value(arr / np.sqrt((arr * arr).sum(axis=0)))
                samples = ifci((self.network.feed_forward(first(t))[i - 1] for t in train))
    yield (dict(loss=0), dict(loss=0))",False,"['rng is None or isinstance(rng, int)', 'len(shape) == 2 and shape[1] == odim', 'len(shape) == 2 and shape[0] == idim']",___________________,0.0
"def ingest(event):
    """"""Ingest a finished recording to the Opencast server.
    """"""
    set_service_status(Service.INGEST, ServiceStatus.BUSY)
    notify.notify('STATUS=Uploading')
    recording_state(event.uid, 'uploading')
    update_event_status(event, Status.UPLOADING)
    service = config('service-ingest')
    service = service[randrange(0, len(service))]
    logger.info('Selecting ingest service to use: ' + service)
    logger.info('Creating new mediapackage')
    mediapackage = http_request(service + '/createMediaPackage')
    prop = 'org.opencastproject.capture.agent.properties'
    dcns = 'http://www.opencastproject.org/xsd/1.0/dublincore/'
    for attachment in event.get_data().get('attach'):
        data = attachment.get('data')
        if '<MASK>':
            workflow_def, workflow_config = get_config_params(data)
        elif '<MASK>':
            name = attachment.get('x-apple-filename', '').rsplit('.', 1)[0]
            logger.info('Adding %s DC catalog' % name)
            fields = [('mediaPackage', mediapackage), ('flavor', 'dublincore/%s' % name), ('dublinCore', data.encode('utf-8'))]
            mediapackage = http_request(service + '/addDCCatalog', fields)
    for flavor, track in event.get_tracks():
        logger.info('Adding track ({0} -> {1})'.format(flavor, track))
        track = track.encode('ascii', 'ignore')
        fields = [('mediaPackage', mediapackage), ('flavor', flavor), ('BODY1', (pycurl.FORM_FILE, track))]
        mediapackage = http_request(service + '/addTrack', fields)
    logger.info('Ingest recording')
    fields = [('mediaPackage', mediapackage)]
    if '<MASK>':
        fields.append(('workflowDefinitionId', workflow_def))
    if '<MASK>':
        fields.append(('workflowInstanceId', event.uid.encode('ascii', 'ignore')))
    fields += workflow_config
    mediapackage = http_request(service + '/ingest', fields)
    recording_state(event.uid, 'upload_finished')
    update_event_status(event, Status.FINISHED_UPLOADING)
    notify.notify('STATUS=Running')
    set_service_status_immediate(Service.INGEST, ServiceStatus.IDLE)
    logger.info('Finished ingest')",False,"['workflow_def', 'event.uid', ""attachment.get('x-apple-filename') == prop"", ""attachment.get('fmttype') == 'application/xml' and dcns in data""]",___________________,0.0
"def calc_n_coarse_chan(self, chan_bw=None):
    """""" This makes an attempt to calculate the number of coarse channels in a given file.

            Note:
                This is unlikely to work on non-Breakthrough Listen data, as a-priori knowledge of
                the digitizer system is required.

        """"""
    nchans = int(self.header[b'nchans'])
    if '<MASK>':
        bandwidth = abs(self.f_stop - self.f_start)
        n_coarse_chan = int(bandwidth / chan_bw)
        return n_coarse_chan
    elif '<MASK>':
        if '<MASK>':
            n_coarse_chan = nchans // 2 ** 20
            return n_coarse_chan
        elif '<MASK>':
            coarse_chan_bw = 2.9296875
            bandwidth = abs(self.f_stop - self.f_start)
            n_coarse_chan = int(bandwidth / coarse_chan_bw)
            return n_coarse_chan
        else:
            logger.warning(""Couldn't figure out n_coarse_chan"")
    elif '<MASK>':
        coarse_chan_bw = 2.9296875
        bandwidth = abs(self.f_stop - self.f_start)
        n_coarse_chan = int(bandwidth / coarse_chan_bw)
        return n_coarse_chan
    else:
        logger.warning('This function currently only works for hires BL Parkes or GBT data.')",False,"['chan_bw is not None', 'nchans >= 2 ** 20', 'nchans % 2 ** 20 == 0', ""self.header[b'telescope_id'] == 6 and nchans < 2 ** 20"", ""self.header[b'telescope_id'] == 6""]",___________________,0.0
"def verifyToken(self, auth):
    """"""
        Ensure the authentication token for the given auth method is still valid.

        Args:
            auth (Auth): authentication type to check

        Raises:
            .SkypeAuthException: if Skype auth is required, and the current token has expired and can't be renewed
        """"""
    if '<MASK>':
        if '<MASK>':
            if '<MASK>':
                raise SkypeAuthException('Skype token expired, and no password specified')
            self.getSkypeToken()
    elif '<MASK>':
        if '<MASK>':
            self.getRegToken()",False,"['auth in (self.Auth.SkypeToken, self.Auth.Authorize)', ""'skype' not in self.tokenExpiry or datetime.now() >= self.tokenExpiry['skype']"", 'auth == self.Auth.RegToken', ""not hasattr(self, 'getSkypeToken')"", ""'reg' not in self.tokenExpiry or datetime.now() >= self.tokenExpiry['reg']""]",___________________,0.0
"def pull(self, images, file_name=None, save=True, force=False, **kwargs):
    """"""pull an image from a docker hub. This is a (less than ideal) workaround
       that actually does the following:

       - creates a sandbox folder
       - adds docker layers, metadata folder, and custom metadata to it
       - converts to a squashfs image with build

    the docker manifests are stored with registry metadata.
 
    Parameters
    ==========
    images: refers to the uri given by the user to pull in the format
    <collection>/<namespace>. You should have an API that is able to 
    retrieve a container based on parsing this uri.
    file_name: the user's requested name for the file. It can 
               optionally be None if the user wants a default.
    save: if True, you should save the container to the database
          using self.add()
    
    Returns
    =======
    finished: a single container path, or list of paths
    """"""
    if '<MASK>':
        images = [images]
    bot.debug('Execution of PULL for %s images' % len(images))
    finished = []
    for image in images:
        q = parse_image_name(remove_uri(image), default_collection='aws')
        image_file = self._pull(file_name=file_name, save=save, force=force, names=q, kwargs=kwargs)
        finished.append(image_file)
    if '<MASK>':
        finished = finished[0]
    return finished",False,"['not isinstance(images, list)', 'len(finished) == 1']",___________________,0.0
"def add_param(self, name, type_name, validators, desc=None):
    """"""Add type information for a parameter by name.

        Args:
            name (str): The name of the parameter we wish to annotate
            type_name (str): The name of the parameter's type
            validators (list): A list of either strings or n tuples that each
                specify a validator defined for type_name.  If a string is passed,
                the validator is invoked with no extra arguments.  If a tuple is
                passed, the validator will be invoked with the extra arguments.
            desc (str): Optional parameter description.
        """"""
    if '<MASK>':
        raise TypeSystemError('Annotation specified multiple times for the same parameter', param=name)
    if '<MASK>':
        raise TypeSystemError('Annotation specified for unknown parameter', param=name)
    info = ParameterInfo(type_name, validators, desc)
    self.annotated_params[name] = info",False,"['name in self.annotated_params', 'name not in self.arg_names and name != self.varargs and (name != self.kwargs)']","_name,,,,,,,,,,,,,,,,,",0.0
"def write(self, stream, message):
    """"""write will write a message to a stream,
        first checking the encoding
        """"""
    if '<MASK>':
        message = message.decode('utf-8')
    stream.write(message)",False,"['isinstance(message, bytes)']","(self,,,,,,,,,,,,,,,,,",0.0
"def write(self, output_stream, kmip_version=enums.KMIPVersion.KMIP_1_0):
    """"""
        Write the data encoding the UsernamePasswordCredential struct to a
        stream.

        Args:
            output_stream (stream): A data stream in which to encode object
                data, supporting a write method; usually a BytearrayStream
                object.
            kmip_version (KMIPVersion): An enumeration defining the KMIP
                version with which the object will be encoded. Optional,
                defaults to KMIP 1.0.

        Raises:
            ValueError: Raised if the username is not defined.
        """"""
    local_stream = BytearrayStream()
    if '<MASK>':
        self._username.write(local_stream, kmip_version=kmip_version)
    else:
        raise ValueError('Username/password credential struct missing the username.')
    if '<MASK>':
        self._password.write(local_stream, kmip_version=kmip_version)
    self.length = local_stream.length()
    super(UsernamePasswordCredential, self).write(output_stream, kmip_version=kmip_version)
    output_stream.write(local_stream.buffer)",False,"['self._username', 'self._password']","__)
        """"""
        """"""
        """"""
        """"""
        """"""
",0.0
"@functools.wraps(func)
def new_func(*args, **kwargs):
    if '<MASK>':
        if '<MASK>':
            raise ValueError('You cannot specify `%s` and `%s` at the same time, they are mutually exclusive.' % (param1_name, param2_name))
        param2 = kwargs.pop(param2_name)
        if '<MASK>':
            param1 = map2to1(param2)
        else:
            param1 = param2
        kwargs[param1_name] = param1
    return func(*args, **kwargs)",False,"['param2_name in kwargs', 'param1_name in kwargs', 'map2to1 is not None']",_name_name_name_name_name_name_name_name_name_,0.0
"def json_dict(json_data):
    """"""Given a dictionary or JSON string; return a dictionary.

    Args:
        json_data(dict, str): Input JSON object.

    Returns:
        A Python dictionary with the contents of the JSON object.

    Raises:
        TypeError: If the input object is not a dictionary or string.

    """"""
    if '<MASK>':
        return json_data
    elif '<MASK>':
        return json.loads(json_data, object_hook=OrderedDict)
    else:
        raise TypeError(""'json_data' must be a dictionary or valid JSON string; received: {!r}"".format(json_data))",False,"['isinstance(json_data, dict)', 'isinstance(json_data, basestring)']","__data,,,,,,,,,,,,,,,,",0.0
"def fromPy(cls, val, typeObj, vldMask=None):
    """"""
        :param val: None or dictionary {index:value} or iterrable of values
        :param vldMask: if is None validity is resolved from val
            if is 0 value is invalidated
            if is 1 value has to be valid
        """"""
    size = evalParam(typeObj.size)
    if '<MASK>':
        size = int(size)
    elements = {}
    if '<MASK>':
        val = None
    if '<MASK>':
        pass
    elif '<MASK>':
        for k, v in val.items():
            if '<MASK>':
                k = int(k)
            elements[k] = typeObj.elmType.fromPy(v)
    else:
        for k, v in enumerate(val):
            if '<MASK>':
                assert v._dtype == typeObj.elmType
                e = v
            else:
                e = typeObj.elmType.fromPy(v)
            elements[k] = e
    _mask = int(bool(val))
    if '<MASK>':
        vldMask = _mask
    else:
        assert vldMask == _mask
    return cls(elements, typeObj, vldMask)",False,"['isinstance(size, Value)', 'vldMask == 0', 'val is None', 'vldMask is None', 'isinstance(val, dict)', 'not isinstance(k, int)', 'isinstance(v, RtlSignalBase)']",___________________,0.0
"def clinvar_submissions(self, user_id, institute_id):
    """"""Collect all open and closed clinvar submission created by a user for an institute

            Args:
                user_id(str): a user ID
                institute_id(str): an institute ID

            Returns:
                submissions(list): a list of clinvar submission objects
        """"""
    LOG.info(""Retrieving all clinvar submissions for user '%s', institute '%s'"", user_id, institute_id)
    query = dict(user_id=user_id, institute_id=institute_id)
    results = list(self.clinvar_submission_collection.find(query))
    submissions = []
    for result in results:
        submission = {}
        submission['_id'] = result.get('_id')
        submission['status'] = result.get('status')
        submission['user_id'] = result.get('user_id')
        submission['institute_id'] = result.get('institute_id')
        submission['created_at'] = result.get('created_at')
        submission['updated_at'] = result.get('updated_at')
        if '<MASK>':
            submission['clinvar_subm_id'] = result['clinvar_subm_id']
        if '<MASK>':
            submission['variant_data'] = self.clinvar_collection.find({'_id': {'$in': result['variant_data']}})
        if '<MASK>':
            submission['case_data'] = self.clinvar_collection.find({'_id': {'$in': result['case_data']}})
        submissions.append(submission)
    return submissions",False,"[""'clinvar_subm_id' in result"", ""result.get('variant_data')"", ""result.get('case_data')""]",___________________,0.0
"def conv_ast_to_sym(self, math_ast):
    """"""
        Convert mathematical expressions to a sympy representation.

        May only contain paranthesis, addition, subtraction and multiplication from AST.
        """"""
    if '<MASK>':
        return symbol_pos_int(math_ast.name)
    elif '<MASK>':
        return sympy.Integer(math_ast.value)
    else:
        op = {'*': operator.mul, '+': operator.add, '-': operator.sub}
        return op[math_ast.op](self.conv_ast_to_sym(math_ast.left), self.conv_ast_to_sym(math_ast.right))",False,"['type(math_ast) is c_ast.ID', 'type(math_ast) is c_ast.Constant']",_ast_ast_ast_ast_ast_ast_ast_ast_ast_,0.0
"def build_route(relation):
    """"""Extract information of one route.""""""
    if '<MASK>':
        return
    short_name = create_route_short_name(relation)
    color = relation.tags.get('color')
    return Route(relation.id, short_name, create_route_long_name(relation, short_name), map_osm_route_type_to_gtfs(relation.tags.get('route')), 'https://www.openstreetmap.org/relation/{}'.format(relation.id), color.strip('#') if color else '', get_agency_id(relation))",False,"[""relation.tags.get('type') != 'route'""]",___________________,0.0
"def encode_model(obj):
    """"""Encode objects like ndb.Model which have a `.to_dict()` method.""""""
    obj_dict = obj.to_dict()
    for key, val in obj_dict.iteritems():
        if '<MASK>':
            try:
                unicode(val)
            except UnicodeDecodeError:
                obj_dict[key] = base64.b64encode(val)
    return obj_dict",False,"['isinstance(val, types.StringType)']",_((((((((((((((((((,0.0
"def authorize_handler(self, f):
    """"""Authorization handler decorator.

        This decorator will sort the parameters and headers out, and
        pre validate everything::

            @app.route('/oauth/authorize', methods=['GET', 'POST'])
            @oauth.authorize_handler
            def authorize(*args, **kwargs):
                if request.method == 'GET':
                    # render a page for user to confirm the authorization
                    return render_template('oauthorize.html')

                confirm = request.form.get('confirm', 'no')
                return confirm == 'yes'
        """"""

    @wraps(f)
    def decorated(*args, **kwargs):
        server = self.server
        uri, http_method, body, headers = extract_params()
        if '<MASK>':
            redirect_uri = request.args.get('redirect_uri', self.error_uri)
            log.debug('Found redirect_uri %s.', redirect_uri)
            try:
                ret = server.validate_authorization_request(uri, http_method, body, headers)
                scopes, credentials = ret
                kwargs['scopes'] = scopes
                kwargs.update(credentials)
            except oauth2.FatalClientError as e:
                log.debug('Fatal client error %r', e, exc_info=True)
                return self._on_exception(e, e.in_uri(self.error_uri))
            except oauth2.OAuth2Error as e:
                log.debug('OAuth2Error: %r', e, exc_info=True)
                state = request.values.get('state')
                if '<MASK>':
                    e.state = state
                return self._on_exception(e, e.in_uri(redirect_uri))
            except Exception as e:
                log.exception(e)
                return self._on_exception(e, add_params_to_uri(self.error_uri, {'error': str(e)}))
        else:
            redirect_uri = request.values.get('redirect_uri', self.error_uri)
        try:
            rv = f(*args, **kwargs)
        except oauth2.FatalClientError as e:
            log.debug('Fatal client error %r', e, exc_info=True)
            return self._on_exception(e, e.in_uri(self.error_uri))
        except oauth2.OAuth2Error as e:
            log.debug('OAuth2Error: %r', e, exc_info=True)
            state = request.values.get('state')
            if '<MASK>':
                e.state = state
            return self._on_exception(e, e.in_uri(redirect_uri))
        if '<MASK>':
            return rv
        if '<MASK>':
            e = oauth2.AccessDeniedError(state=request.values.get('state'))
            return self._on_exception(e, e.in_uri(redirect_uri))
        return self.confirm_authorization_request()
    return decorated",False,"[""request.method in ('GET', 'HEAD')"", 'not isinstance(rv, bool)', 'not rv', 'state and (not e.state)', 'state and (not e.state)']",___________________,0.0
"def get_policy(self, policyid, bundleid=None):
    """"""**Description**
            Retrieve the policy with the given id in the targeted policy bundle

        **Arguments**
            - policyid: Unique identifier associated with this policy.
            - bundleid: Target bundle. If not specified, the currently active bundle will be used.

        **Success Return Value**
            A JSON object containing the policy description.
        """"""
    url = self.url + '/api/scanning/v1/policies/' + policyid
    if '<MASK>':
        url += '?bundleId=' + bundleid",False,['bundleid'],___________________,0.0
"def validate_versatileimagefield_sizekey_list(sizes):
    """"""
    Validate a list of size keys.

    `sizes`: An iterable of 2-tuples, both strings. Example:
    [
        ('large', 'url'),
        ('medium', 'crop__400x400'),
        ('small', 'thumbnail__100x100')
    ]
    """"""
    try:
        for key, size_key in sizes:
            size_key_split = size_key.split('__')
            if '<MASK>':
                raise InvalidSizeKey(""{0} is an invalid size. All sizes must be either 'url' or made up of at least two segments separated by double underscores. Examples: 'crop__400x400', filters__invert__url"".format(size_key))
    except ValueError:
        raise InvalidSizeKeySet('{} is an invalid size key set. Size key sets must be an iterable of 2-tuples'.format(str(sizes)))
    return list(set(sizes))",False,"[""size_key_split[-1] != 'url' and 'x' not in size_key_split[-1]""]","((s,,,,,,,,,,,,,,,,",0.0
"def deleteSchema(self, schemaId):
    """"""
        Delete a schema.  Parameter: schemaId (string). Throws APIException on failure.
        """"""
    req = ApiClient.oneSchemaUrl % (self.host, '/draft', schemaId)
    resp = requests.delete(req, auth=self.credentials, verify=self.verify)
    if '<MASK>':
        self.logger.debug('Schema deleted')
    else:
        raise ibmiotf.APIException(resp.status_code, 'HTTP error deleting schema', resp)
    return resp",False,['resp.status_code == 204'],___________________,0.0
"def client(self):
    """"""single global client instance""""""
    cls = self.__class__
    if '<MASK>':
        kwargs = {}
        if '<MASK>':
            kwargs['tls'] = docker.tls.TLSConfig(**self.tls_config)
        kwargs.update(kwargs_from_env())
        client = docker.APIClient(version='auto', **kwargs)
        cls._client = client
    return cls._client",False,"['cls._client is None', 'self.tls_config']",___________________,0.0
"def update_wrapper(wrapper, wrapped):
    """""" To be used under python2.4 because functools.update_wrapper() is available only from python2.5+ """"""
    for attr_name in ('__module__', '__name__', '__doc__'):
        attr_value = getattr(wrapped, attr_name, None)
        if '<MASK>':
            setattr(wrapper, attr_name, attr_value)
    wrapper.__dict__.update(getattr(wrapped, '__dict__', {}))
    return wrapper",False,['attr_value is not None'],"_,,,,,,,,,,,,,,,,,,",0.0
"def timer(self, stat, tags=None):
    """"""Contextmanager for easily computing timings.

        :arg string stat: A period delimited alphanumeric key.

        :arg list-of-strings tags: Each string in the tag consists of a key and
            a value separated by a colon. Tags can make it easier to break down
            metrics for analysis.

            For example ``['env:stage', 'compressed:yes']``.

        For example:

        >>> mymetrics = get_metrics(__name__)

        >>> def long_function():
        ...     with mymetrics.timer('long_function'):
        ...         # perform some thing we want to keep metrics on
        ...         pass


        .. Note::

           All timings generated with this are in milliseconds.

        """"""
    if '<MASK>':
        start_time = time.perf_counter()
    else:
        start_time = time.time()
    yield
    if '<MASK>':
        end_time = time.perf_counter()
    else:
        end_time = time.time()
    delta = end_time - start_time
    self.timing(stat, value=delta * 1000.0, tags=tags)",False,"['six.PY3', 'six.PY3']",___________________,0.0
"def seekable(fileobj):
    """"""Backwards compat function to determine if a fileobj is seekable

    :param fileobj: The file-like object to determine if seekable

    :returns: True, if seekable. False, otherwise.
    """"""
    if '<MASK>':
        return fileobj.seekable()
    elif '<MASK>':
        try:
            fileobj.seek(0, 1)
            return True
        except (OSError, IOError):
            return False
    return False",False,"[""hasattr(fileobj, 'seekable')"", ""hasattr(fileobj, 'seek') and hasattr(fileobj, 'tell')""]",",,,,,,,,,,,,,,,,,,,",0.0
"def parse_ped(ped_stream, family_type='ped'):
    """"""Parse out minimal family information from a PED file.

    Args:
        ped_stream(iterable(str))
        family_type(str): Format of the pedigree information

    Returns:
        family_id(str), samples(list[dict])
    """"""
    pedigree = FamilyParser(ped_stream, family_type=family_type)
    if '<MASK>':
        raise PedigreeError('Only one case per ped file is allowed')
    family_id = list(pedigree.families.keys())[0]
    family = pedigree.families[family_id]
    samples = [{'sample_id': ind_id, 'father': individual.father, 'mother': individual.mother, 'sex': SEX_MAP[individual.sex], 'phenotype': PHENOTYPE_MAP[int(individual.phenotype)]} for ind_id, individual in family.individuals.items()]
    return (family_id, samples)",False,['len(pedigree.families) != 1'],___________________,0.0
"def _is_completed(self, name_or_id=None):
    """"""Private function such that it can still be called by the environment during
        a single run""""""
    if '<MASK>':
        return all((runinfo['completed'] for runinfo in self._run_information.values()))
    else:
        return self.f_get_run_information(name_or_id, copy=False)['completed']",False,['name_or_id is None'],___________________,0.0
"def close(self):
    """"""Close this SPK file.""""""
    self.daf.file.close()
    for segment in self.segments:
        if '<MASK>':
            del segment._data
    self.daf._array = None
    self.daf._map = None",False,"[""hasattr(segment, '_data')""]",(((((((((((((((((((,0.0
"def _similar_names(owner, attrname, distance_threshold, max_choices):
    """"""Given an owner and a name, try to find similar names

    The similar names are searched given a distance metric and only
    a given number of choices will be returned.
    """"""
    possible_names = []
    names = _node_names(owner)
    for name in names:
        if '<MASK>':
            continue
        distance = _string_distance(attrname, name)
        if '<MASK>':
            possible_names.append((name, distance))
    picked = [name for name, _ in heapq.nsmallest(max_choices, possible_names, key=operator.itemgetter(1))]
    return sorted(picked)",False,"['name == attrname', 'distance <= distance_threshold']","_,_,_,_,_,_,_,_,_,_",0.0
"def get_cumulative_data(self):
    """"""Get the data as it will be charted.  The first set will be
		the actual first data set.  The second will be the sum of the
		first and the second, etc.""""""
    sets = map(itemgetter('data'), self.data)
    if '<MASK>':
        return
    sum = sets.pop(0)
    yield sum
    while sets:
        sum = map(add, sets.pop(0))
        yield sum",False,['not sets'],___________________,0.0
"def add_gene_info(self, variant_obj, gene_panels=None):
    """"""Add extra information about genes from gene panels

        Args:
            variant_obj(dict): A variant from the database
            gene_panels(list(dict)): List of panels from database
        """"""
    gene_panels = gene_panels or []
    variant_obj['has_refseq'] = False
    extra_info = {}
    for panel_obj in gene_panels:
        for gene_info in panel_obj['genes']:
            hgnc_id = gene_info['hgnc_id']
            if '<MASK>':
                extra_info[hgnc_id] = []
            extra_info[hgnc_id].append(gene_info)
    for variant_gene in variant_obj.get('genes', []):
        hgnc_id = variant_gene['hgnc_id']
        hgnc_gene = self.hgnc_gene(hgnc_id)
        if '<MASK>':
            continue
        transcripts_dict = {}
        for transcript in hgnc_gene.get('transcripts', []):
            tx_id = transcript['ensembl_transcript_id']
            transcripts_dict[tx_id] = transcript
        hgnc_gene['transcripts_dict'] = transcripts_dict
        if '<MASK>':
            variant_gene['omim_penetrance'] = True
        panel_info = extra_info.get(hgnc_id, [])
        disease_associated = set()
        disease_associated_no_version = set()
        manual_penetrance = False
        mosaicism = False
        manual_inheritance = set()
        for gene_info in panel_info:
            for tx in gene_info.get('disease_associated_transcripts', []):
                stripped = re.sub('\\.[0-9]', '', tx)
                disease_associated_no_version.add(stripped)
                disease_associated.add(tx)
            if '<MASK>':
                manual_penetrance = True
            if '<MASK>':
                mosaicism = True
            manual_inheritance.update(gene_info.get('inheritance_models', []))
        variant_gene['disease_associated_transcripts'] = list(disease_associated)
        variant_gene['manual_penetrance'] = manual_penetrance
        variant_gene['mosaicism'] = mosaicism
        variant_gene['manual_inheritance'] = list(manual_inheritance)
        for transcript in variant_gene.get('transcripts', []):
            tx_id = transcript['transcript_id']
            if '<MASK>':
                continue
            hgnc_transcript = transcripts_dict[tx_id]
            if '<MASK>':
                transcript['is_primary'] = True
            if '<MASK>':
                continue
            refseq_id = hgnc_transcript['refseq_id']
            transcript['refseq_id'] = refseq_id
            variant_obj['has_refseq'] = True
            if '<MASK>':
                transcript['is_disease_associated'] = True
            transcript['refseq_identifiers'] = hgnc_transcript.get('refseq_identifiers', [])
        variant_gene['common'] = hgnc_gene
        variant_gene['disease_terms'] = self.disease_terms(hgnc_id)
    return variant_obj",False,"['not hgnc_gene', ""hgnc_gene.get('incomplete_penetrance')"", 'hgnc_id not in extra_info', ""gene_info.get('reduced_penetrance')"", ""gene_info.get('mosaicism')"", 'not tx_id in transcripts_dict', ""hgnc_transcript.get('is_primary')"", ""not hgnc_transcript.get('refseq_id')"", 'refseq_id in disease_associated_no_version']",___________________,0.0
"def align_segmentation(beat_times, song):
    """"""Load a ground-truth segmentation, and align times to the nearest
    detected beats.

    Arguments:
        beat_times -- array
        song -- path to the audio file

    Returns:
        segment_beats -- array
            beat-aligned segment boundaries

        segment_times -- array
            true segment times

        segment_labels -- array
            list of segment labels
    """"""
    try:
        segment_times, segment_labels = msaf.io.read_references(song)
    except:
        return (None, None, None)
    segment_times = np.asarray(segment_times)
    segment_intervals = msaf.utils.times_to_intervals(segment_times)
    beat_intervals = np.asarray(zip(beat_times[:-1], beat_times[1:]))
    beat_segment_ids = librosa.util.match_intervals(beat_intervals, segment_intervals)
    segment_beats = []
    segment_times_out = []
    segment_labels_out = []
    for i in range(segment_times.shape[0]):
        hits = np.argwhere(beat_segment_ids == i)
        if '<MASK>':
            segment_beats.extend(hits[0])
            segment_times_out.append(segment_intervals[i, :])
            segment_labels_out.append(segment_labels[i])
    segment_beats = list(segment_beats)
    segment_times_out = segment_times
    return (segment_beats, segment_times_out, segment_labels_out)",False,['len(hits) > 0 and i < len(segment_intervals) and (i < len(segment_labels))'],___________________,0.0
"def detailed_type(self):
    """"""
        Uses EPlus double approach of type ('type' tag, and/or 'key', 'object-list', 'external-list', 'reference' tags)
        to determine detailed type.
        
        Returns
        -------
        ""integer"", ""real"", ""alpha"", ""choice"", ""reference"", ""object-list"", ""external-list"", ""node""
        """"""
    if '<MASK>':
        if '<MASK>':
            self._detailed_type = 'reference'
        elif '<MASK>':
            self._detailed_type = self.tags['type'][0].lower()
        elif '<MASK>':
            self._detailed_type = 'choice'
        elif '<MASK>':
            self._detailed_type = 'object-list'
        elif '<MASK>':
            self._detailed_type = 'external-list'
        elif '<MASK>':
            self._detailed_type = 'alpha'
        elif '<MASK>':
            self._detailed_type = 'real'
        else:
            raise ValueError(""Can't find detailed type."")
    return self._detailed_type",False,"['self._detailed_type is None', ""'reference' in self.tags or 'reference-class-name' in self.tags"", ""'type' in self.tags"", ""'key' in self.tags"", ""'object-list' in self.tags"", ""'external-list' in self.tags"", ""self.basic_type == 'A'"", ""self.basic_type == 'N'""]",___________________,0.0
"def leave_module(self, node):
    """"""leave module: check globals
        """"""
    assert len(self._to_consume) == 1
    not_consumed = self._to_consume.pop().to_consume
    if '<MASK>':
        self._check_all(node, not_consumed)
    self._check_globals(not_consumed)
    if '<MASK>':
        return
    self._check_imports(not_consumed)",False,"[""'__all__' in node.locals"", 'not self.config.init_import and node.package']",___________________,0.0
"def getResultFromProcess(res, tempname, process):
    """"""Get a value from process, return tuple of value, res if succesful""""""
    if '<MASK>':
        value = getRepresentation(tempname, process)
        return (value, res)
    else:
        return (res, str(res))",False,"['not isinstance(res, (UndefinedValue, Exception))']",",,,,,,,,,,,,,,,,,,,",0.0
"def fromPy(cls, val, typeObj, vldMask=None):
    """"""
        Construct value from pythonic value (int, bytes, enum.Enum member)
        """"""
    assert not isinstance(val, Value)
    if '<MASK>':
        vld = 0
        val = 0
        assert vldMask is None or vldMask == 0
    else:
        allMask = typeObj.all_mask()
        w = typeObj.bit_length()
        if '<MASK>':
            val = int.from_bytes(val, byteorder='little', signed=bool(typeObj.signed))
        else:
            try:
                val = int(val)
            except TypeError as e:
                if '<MASK>':
                    val = int(val.value)
                else:
                    raise e
        if '<MASK>':
            vld = allMask
        else:
            assert vldMask <= allMask and vldMask >= 0
            vld = vldMask
        if '<MASK>':
            assert typeObj.signed
            assert signFix(val & allMask, w) == val, (val, signFix(val & allMask, w))
            val = signFix(val & vld, w)
        else:
            if '<MASK>':
                msb = 1 << w - 1
                if '<MASK>':
                    assert val < 0, val
            if '<MASK>':
                raise ValueError('Not enought bits to represent value', val, val & allMask)
            val = val & vld
    return cls(val, typeObj, vld)",False,"['val is None', 'isinstance(val, bytes)', 'vldMask is None', 'val < 0', 'typeObj.signed', 'val & allMask != val', 'msb & val', 'isinstance(val, enum.Enum)']",___________________,0.0
"def _deserialize(self, value, *args, **kwargs):
    """"""Deserialize a serialized value to a model instance.

        If the parent schema is transient, create a new (transient) instance.
        Otherwise, attempt to find an existing instance in the database.
        :param value: The value to deserialize.
        """"""
    if '<MASK>':
        if '<MASK>':
            self.fail('invalid', value=value, keys=[prop.key for prop in self.related_keys])
        value = {self.related_keys[0].key: value}
    if '<MASK>':
        return self.related_model(**value)
    try:
        result = self._get_existing_instance(self.session.query(self.related_model), value)
    except NoResultFound:
        return self.related_model(**value)
    return result",False,"['not isinstance(value, dict)', 'self.transient', 'len(self.related_keys) != 1']",___________________,0.0
"def setup_build(self, name, repo, config, tag=None, commit=None, recipe='Singularity', startup_script=None):
    """"""setup the build based on the selected configuration file, meaning
       producing the configuration file filled in based on the user's input
 
       Parameters
       ==========
       config: the complete configuration file provided by the client
       template: an optional custom start script to use
       tag: a user specified tag for the build, derived from uri or manual
       recipe: a recipe, if defined, overrides recipe set in config.
       commit: a commit to check out, if needed
       start_script: the start script to use, if not defined 
                     defaults to apt (or manager) base in main/templates/build

    """"""
    manager = self._get_and_update_setting('SREGISTRY_BUILDER_MANAGER', 'apt')
    startup_script = get_build_template(startup_script, manager)
    config = self._load_build_config(config)
    if '<MASK>':
        bot.error('Cannot find config, check path or URI.')
        sys.exit(1)
    self._client_tagged(config['data']['tags'])
    defaults = config['data']['metadata']
    selfLink = config['links']['self']
    builder_repo = config['data']['repo']
    builder_bundle = config['data']['path']
    builder_id = config['data']['id']
    config = config['data']['config']
    image_project = defaults.get('GOOGLE_COMPUTE_PROJECT', 'debian-cloud')
    image_family = defaults.get('GOOGLE_COMPUTE_IMAGE_FAMILY', 'debian-8')
    instance_name = '%s-builder %s' % (name.replace('/', '-'), selfLink)
    robot_name = RobotNamer().generate()
    project = self._get_project()
    zone = self._get_zone()
    machine_type = defaults.get('SREGISTRY_BUILDER_machine_type', 'n1-standard-1')
    machine_type = 'zones/%s/machineTypes/%s' % (zone, machine_type)
    disk_size = defaults.get('SREGISTRY_BUILDER_disk_size', '100')
    image_response = self._compute_service.images().getFromFamily(project=image_project, family=image_family).execute()
    source_disk_image = image_response['selfLink']
    storage_bucket = self._bucket_name
    config['name'] = robot_name
    config['description'] = instance_name
    config['machineType'] = machine_type
    config['disks'].append({'autoDelete': True, 'boot': True, 'initializeParams': {'sourceImage': source_disk_image, 'diskSizeGb': disk_size}})
    metadata = {'items': [{'key': 'startup-script', 'value': startup_script}, {'key': 'SREGISTRY_BUILDER_STORAGE_BUCKET', 'value': self._bucket_name}]}
    defaults = setconfig(defaults, 'SREGISTRY_USER_REPO', repo)
    defaults = setconfig(defaults, 'SREGISTRY_CONTAINER_NAME', name)
    defaults = setconfig(defaults, 'SREGISTRY_USER_COMMIT', commit)
    defaults = setconfig(defaults, 'SREGISTRY_USER_BRANCH', 'master')
    defaults = setconfig(defaults, 'SREGISTRY_USER_TAG', tag)
    defaults = setconfig(defaults, 'SREGISTRY_BUILDER_REPO', builder_repo)
    defaults = setconfig(defaults, 'SREGISTRY_BUILDER_COMMIT')
    defaults = setconfig(defaults, 'SREGISTRY_BUILDER_RUNSCRIPT', 'run.sh')
    defaults = setconfig(defaults, 'SREGISTRY_BUILDER_BRANCH', 'master')
    defaults = setconfig(defaults, 'SREGISTRY_BUILDER_ID', builder_id)
    defaults = setconfig(defaults, 'SREGISTRY_BUILDER_BUNDLE', builder_bundle)
    defaults = setconfig(defaults, 'SREGISTRY_BUILDER_DEBUGHOURS', '4')
    defaults = setconfig(defaults, 'SREGISTRY_BUILDER_KILLHOURS', '10')
    defaults = setconfig(defaults, 'SINGULARITY_RECIPE', recipe)
    defaults = setconfig(defaults, 'SINGULARITY_BRANCH')
    defaults = setconfig(defaults, 'SINGULARITY_COMMIT')
    defaults = setconfig(defaults, 'SINGULARITY_REPO', 'https://github.com/cclerget/singularity.git')
    seen = ['SREGISTRY_BUILDER_STORAGE_BUCKET', 'startup-script']
    for key, value in defaults.items():
        if '<MASK>':
            entry = {'key': key, 'value': value}
            metadata['items'].append(entry)
            seen.append(key)
    config['metadata'] = metadata
    return config",False,"['not config', 'value not in seen']",___________________,0.0
"def __deftype_impls(ctx: ParserContext, form: ISeq) -> Tuple[List[DefTypeBase], List[Method]]:
    """"""Roll up deftype* declared bases and method implementations.""""""
    current_interface_sym: Optional[sym.Symbol] = None
    current_interface: Optional[DefTypeBase] = None
    interfaces = []
    methods: List[Method] = []
    interface_methods: MutableMapping[sym.Symbol, List[Method]] = {}
    for elem in form:
        if '<MASK>':
            if '<MASK>':
                if '<MASK>':
                    raise ParserException(f'deftype* forms may only implement an interface once', form=elem)
                assert current_interface_sym is not None, 'Symbol must be defined with interface'
                interface_methods[current_interface_sym] = methods
            current_interface_sym = elem
            current_interface = _parse_ast(ctx, elem)
            methods = []
            if '<MASK>':
                raise ParserException(f'deftype* interface implementation must be an existing interface', form=elem)
            interfaces.append(current_interface)
        elif '<MASK>':
            if '<MASK>':
                raise ParserException(f'deftype* method cannot be declared without interface', form=elem)
            methods.append(__deftype_method(ctx, elem, current_interface))
        else:
            raise ParserException(f'deftype* must consist of interface or protocol names and methods', form=elem)
    if '<MASK>':
        if '<MASK>':
            if '<MASK>':
                raise ParserException(f'deftype* forms may only implement an interface once', form=current_interface_sym)
            assert current_interface_sym is not None, 'Symbol must be defined with interface'
            interface_methods[current_interface_sym] = methods
        else:
            raise ParserException(f'deftype* may not declare interface without at least one method', form=current_interface_sym)
    return (interfaces, list(chain.from_iterable(interface_methods.values())))",False,"['current_interface is not None', 'isinstance(elem, sym.Symbol)', 'len(methods) > 0', 'current_interface is not None', 'not isinstance(current_interface, (MaybeClass, MaybeHostForm, VarRef))', 'isinstance(elem, ISeq)', 'current_interface_sym in interface_methods', 'current_interface_sym in interface_methods', 'current_interface is None']",___________________,0.0
"def decode_spans(self, spans):
    """"""Decodes an encoded list of spans.

        :param spans: encoded list of spans
        :type spans: bytes
        :return: list of spans
        :rtype: list of Span
        """"""
    decoded_spans = []
    transport = TMemoryBuffer(spans)
    if '<MASK>':
        _, size = read_list_begin(transport)
    else:
        size = 1
    for _ in range(size):
        span = zipkin_core.Span()
        span.read(TBinaryProtocol(transport))
        decoded_spans.append(self._decode_thrift_span(span))
    return decoded_spans",False,['six.byte2int(spans) == TType.STRUCT'],___________________,0.0
"def memory_size(self, human_readable=True):
    """"""Total Memory Size of Synology DSM""""""
    if '<MASK>':
        return_data = int(self._data['memory']['memory_size']) * 1024
        if '<MASK>':
            return SynoFormatHelper.bytes_to_readable(return_data)
        else:
            return return_data",False,"['self._data is not None', 'human_readable']",___________________,0.0
"def get_report_status(self, report):
    """"""
        Returns the status of a report.

        https://canvas.instructure.com/doc/api/account_reports.html#method.account_reports.show
        """"""
    if '<MASK>':
        raise ReportFailureException(report)
    url = ACCOUNTS_API.format(report.account_id) + '/reports/{}/{}'.format(report.type, report.report_id)
    data = self._get_resource(url)
    data['account_id'] = report.account_id
    return Report(data=data)",False,['report.account_id is None or report.type is None or report.report_id is None'],(((((((((((((((((((,0.0
"def save_file(title='Save', directory='', filename='', wildcard='All Files (*.*)|*.*', overwrite=False, parent=None):
    """"""Show a dialog to select file to save, return path(s) if accepted""""""
    style = wx.SAVE
    if '<MASK>':
        style |= wx.OVERWRITE_PROMPT
    result = dialogs.fileDialog(parent, title, directory, filename, wildcard, style)
    return result.paths",False,['not overwrite'],___________________,0.0
"def previous_sibling(self, name=None):
    """"""Get the previous sibling in the children list of the parent node.

        If a name is provided, the previous sibling with the given name is
        returned.

        """"""
    if '<MASK>':
        return XMLElement(lib.lsl_previous_sibling(self.e))
    else:
        return XMLElement(lib.lsl_previous_sibling_n(self.e, str.encode(name)))",False,['name is None'],"_s_s
                                                                                                  ",0.0
"def main():
    """""" Main CLI entrypoint. """"""
    options = get_options()
    Windows.enable(auto_colors=True, reset_atexit=True)
    try:
        check_for_virtualenv(options)
        filenames = RequirementsDetector(options.get('<requirements_file>')).get_filenames()
        if '<MASK>':
            print(Color('{{autoyellow}}Found valid requirements file(s):{{/autoyellow}} {{autocyan}}\n{}{{/autocyan}}'.format('\n'.join(filenames))))
        else:
            print(Color('{autoyellow}No requirements files found in current directory. CD into your project or manually specify requirements files as arguments.{/autoyellow}'))
            return
        packages = PackagesDetector(filenames).get_packages()
        packages_status_map = PackagesStatusDetector(packages, options.get('--use-default-index')).detect_available_upgrades(options)
        selected_packages = PackageInteractiveSelector(packages_status_map, options).get_packages()
        upgraded_packages = PackagesUpgrader(selected_packages, filenames, options).do_upgrade()
        print(Color('{{autogreen}}Successfully upgraded (and updated requirements) for the following packages: {}{{/autogreen}}'.format(','.join([package['name'] for package in upgraded_packages]))))
        if '<MASK>':
            print(Color('{automagenta}Actually, no, because this was a simulation using --dry-run{/automagenta}'))
    except KeyboardInterrupt:
        print(Color('\n{autored}Upgrade interrupted.{/autored}'))",False,"['filenames', ""options['--dry-run']""]",___________________,0.0
"def find_cutoff(tt_scores, td_scores, cutoff_fdr, parametric, pfdr, pi0_lambda, pi0_method, pi0_smooth_df, pi0_smooth_log_pi0):
    """""" Finds cut off target score for specified false discovery rate fdr """"""
    error_stat, pi0 = error_statistics(tt_scores, td_scores, parametric, pfdr, pi0_lambda, pi0_method, pi0_smooth_df, pi0_smooth_log_pi0, False)
    if '<MASK>':
        raise click.ClickException('Too little data for calculating error statistcs.')
    i0 = (error_stat.qvalue - cutoff_fdr).abs().idxmin()
    cutoff = error_stat.iloc[i0]['cutoff']
    return cutoff",False,['not len(error_stat)'],___________________,0.0
"def cases(context, institute, display_name, case_id, nr_variants, variants_treshold):
    """"""Display cases from the database""""""
    LOG.info('Running scout view institutes')
    adapter = context.obj['adapter']
    models = []
    if '<MASK>':
        case_obj = adapter.case(case_id=case_id)
        if '<MASK>':
            models.append(case_obj)
    else:
        models = adapter.cases(collaborator=institute, name_query=display_name)
        models = [case_obj for case_obj in models]
    if '<MASK>':
        LOG.info('No cases could be found')
        return
    header = ['case_id', 'display_name', 'institute']
    if '<MASK>':
        LOG.info('Only show cases with more than %s variants', variants_treshold)
        nr_variants = True
    if '<MASK>':
        LOG.info('Displaying number of variants for each case')
        header.append('clinical')
        header.append('research')
    click.echo('#' + '\t'.join(header))
    for model in models:
        output_str = '{:<12}\t{:<12}\t{:<12}'
        output_values = [model['_id'], model['display_name'], model['owner']]
        if '<MASK>':
            output_str += '\t{:<12}\t{:<12}'
            nr_clinical = 0
            nr_research = 0
            variants = adapter.variant_collection.find({'case_id': model['_id']})
            i = 0
            for i, var in enumerate(variants, 1):
                if '<MASK>':
                    nr_clinical += 1
                else:
                    nr_research += 1
            output_values.extend([nr_clinical, nr_research])
            if '<MASK>':
                LOG.debug('Case %s had to few variants, skipping', model['_id'])
                continue
        click.echo(output_str.format(*output_values))",False,"['case_id', 'not models', 'variants_treshold', 'nr_variants', 'case_obj', 'nr_variants', 'variants_treshold and i < variants_treshold', ""var['variant_type'] == 'clinical'""]",___________________,0.0
"def report(self, output_file=sys.stdout):
    """"""Print human readable report of model.""""""
    cpu_perf = self.results['cpu bottleneck']['performance throughput']
    if '<MASK>':
        print('{}'.format(pformat(self.results)), file=output_file)
    if '<MASK>':
        print('Bottlenecks:', file=output_file)
        print('  level | a. intensity |   performance   |   peak bandwidth  | peak bandwidth kernel', file=output_file)
        print('--------+--------------+-----------------+-------------------+----------------------', file=output_file)
        print('    CPU |              | {!s:>15} |                   |'.format(cpu_perf[self._args.unit]), file=output_file)
        for b in self.results['mem bottlenecks']:
            if '<MASK>':
                continue
            print('{level:>7} | {arithmetic intensity:>5.2} FLOP/B | {0!s:>15} | {bandwidth!s:>17} | {bw kernel:<8}'.format(b['performance'][self._args.unit], **b), file=output_file)
        print('', file=output_file)
        print('IACA analisys:', file=output_file)
        print('{!s}'.format({k: v for k, v in list(self.results['cpu bottleneck'].items()) if k not in ['IACA output']}), file=output_file)
    if '<MASK>':
        print('CPU bound. {!s} due to CPU bottleneck'.format(cpu_perf[self._args.unit]), file=output_file)
    else:
        print('Cache or mem bound.', file=output_file)
        bottleneck = self.results['mem bottlenecks'][self.results['bottleneck level']]
        print('{!s} due to {} transfer bottleneck (with bw from {} benchmark)'.format(bottleneck['performance'][self._args.unit], bottleneck['level'], bottleneck['bw kernel']), file=output_file)
        print('Arithmetic Intensity: {:.2f} FLOP/B'.format(bottleneck['arithmetic intensity']), file=output_file)",False,"['self.verbose >= 3', 'self.verbose >= 1', ""self.results['min performance']['FLOP/s'] > cpu_perf['FLOP/s']"", 'b is None']",___________________,0.0
"def _deserialize(self, value):
    """"""
        Deserializes and verifies the value created by :meth:`._serialize`.

        :param str value:
            The serialized value.

        :returns:
            Deserialized object.

        """"""
    encoded, timestamp, signature = value.split('|')
    if '<MASK>':
        raise SessionError('Invalid signature ""{0}""!'.format(signature))
    if '<MASK>':
        return None
    decoded = parse.unquote(encoded)
    deserialized = pickle.loads(decoded.encode('latin-1'))
    return deserialized",False,"['not signature == self._signature(self.name, encoded, timestamp)', 'int(timestamp) < int(time.time()) - self.max_age']","_,,,,,,,,,,,,,,,,,,",0.0
"def _mkdir_p(path):
    """"""mkdir -p path""""""
    try:
        os.makedirs(path)
    except OSError as exc:
        if '<MASK>':
            pass
        else:
            raise
    else:
        logger.info('New: %s%s', path, os.path.sep)",False,['exc.errno == errno.EEXIST and os.path.isdir(path)'],"(path):
    """""".............",0.0
"def current_app_is_admin(self):
    """"""Returns boolean whether current application is Admin contrib.

        :rtype: bool
        """"""
    is_admin = self._current_app_is_admin
    if '<MASK>':
        context = self.current_page_context
        current_app = getattr(getattr(context.get('request', None), 'resolver_match', None), 'app_name', getattr(context, 'current_app', None))
        if '<MASK>':
            current_app = context.get('current_app', '')
        is_admin = current_app == ADMIN_APP_NAME
        self._current_app_is_admin = is_admin
    return is_admin",False,"['is_admin is None', 'current_app is None']",___________________,0.0
"def __refer_all(cls, refers: lmap.Map, other_ns_interns: lmap.Map) -> lmap.Map:
    """"""Refer all _public_ interns from another namespace.""""""
    final_refers = refers
    for entry in other_ns_interns:
        s: sym.Symbol = entry.key
        var: Var = entry.value
        if '<MASK>':
            final_refers = final_refers.assoc(s, var)
    return final_refers",False,['not var.is_private'],___________________,0.0
"def insert_rollbar_console(request, html):
    item_uuid = request.environ.get('rollbar.uuid')
    if '<MASK>':
        return html
    url = '%s/item/uuid/?uuid=%s' % (rollbar_web_base, item_uuid)
    link = '<a style=""color:white;"" href=""%s"">View in Rollbar</a>' % url
    new_data = '<h2>Rollbar: %s</h2>' % link
    insertion_marker = '</h1>'
    replacement = insertion_marker + new_data
    return html.replace(insertion_marker, replacement, 1)",False,['not item_uuid'],___________________,0.0
"def expire_soon(self, seconds):
    """"""
        Returns ``True`` if credentials expire sooner than specified.

        :param int seconds:
            Number of seconds.

        :returns:
            ``True`` if credentials expire sooner than specified,
            else ``False``.

        """"""
    if '<MASK>':
        return self.expiration_time < int(time.time()) + int(seconds)
    else:
        return False",False,['self.expiration_time'],___________________,0.0
"def send_command(self, command):
    """"""Send a command for FastAGI request:

        :param command: Command to launch on FastAGI request. Ex: 'EXEC StartMusicOnHolds'
        :type command: String

        :Example:

        ::

            @asyncio.coroutine
            def call_waiting(request):
                print(['AGI variables:', request.headers])
                yield from request.send_command('ANSWER')
                yield from request.send_command('EXEC StartMusicOnHold')
                yield from request.send_command('EXEC Wait 10')

        """"""
    command += '\n'
    self.writer.write(command.encode(self.encoding))
    yield from self.writer.drain()
    agi_result = (yield from self._read_result())
    while agi_result.get('status_code') == 100:
        agi_result = (yield from self._read_result())
    if '<MASK>':
        buff_usage_error = (yield from self.reader.readline())
        agi_result['msg'] += buff_usage_error.decode(self.encoding)
    return agi_result",False,"[""'error' in agi_result and agi_result['error'] == 'AGIUsageError'""]",___________________,0.0
"def parse_individuals(samples):
    """"""Parse the individual information

        Reformat sample information to proper individuals

        Args:
            samples(list(dict))

        Returns:
            individuals(list(dict))
    """"""
    individuals = []
    if '<MASK>':
        raise PedigreeError('No samples could be found')
    ind_ids = set()
    for sample_info in samples:
        parsed_ind = parse_individual(sample_info)
        individuals.append(parsed_ind)
        ind_ids.add(parsed_ind['individual_id'])
    for parsed_ind in individuals:
        father = parsed_ind['father']
        if '<MASK>':
            if '<MASK>':
                raise PedigreeError('father %s does not exist in family' % father)
        mother = parsed_ind['mother']
        if '<MASK>':
            if '<MASK>':
                raise PedigreeError('mother %s does not exist in family' % mother)
    return individuals",False,"['len(samples) == 0', ""father and father != '0'"", ""mother and mother != '0'"", 'father not in ind_ids', 'mother not in ind_ids']",___________________,0.0
"def datetime_removed(self):
    """"""Returns file's remove aware *datetime* in UTC format.

        It might do API request once because it depends on ``info()``.

        """"""
    if '<MASK>':
        return dateutil.parser.parse(self.info()['datetime_removed'])",False,"[""self.info().get('datetime_removed')""]","_rem,,,,,,,,,,,,,,,,,",0.0
"def apply_hook(self, items, sender):
    """"""Applies item processing hook, registered with ``register_item_hook()``
        to items supplied, and returns processed list.

        Returns initial items list if no hook is registered.

        :param list items:
        :param str|unicode sender: menu, breadcrumbs, sitetree, {type}.children, {type}.has_children
        :rtype: list
        """"""
    processor = _ITEMS_PROCESSOR
    if '<MASK>':
        return items
    if '<MASK>':
        return processor(tree_items=items, tree_sender=sender)
    return processor(tree_items=items, tree_sender=sender, context=self.current_page_context)",False,"['processor is None', '_ITEMS_PROCESSOR_ARGS_LEN == 2']",___________________,0.0
"def send_request(self):
    """"""Send request.

        [:rfc:`2131#section-3.1`]::

        a client retransmitting as described in section 4.1 might retransmit
        the DHCPREQUEST message four times, for a total delay of 60 seconds

        .. todo::

           - The maximum number of retransmitted REQUESTs is per state or in
             total?
           - Are the retransmitted REQUESTs independent to the retransmitted
             DISCOVERs?

        """"""
    assert self.client
    if '<MASK>':
        pkt = self.client.gen_request_unicast()
    else:
        pkt = self.client.gen_request()
    sendp(pkt)
    logger.debug('Modifying FSM obj, setting time_sent_request.')
    self.time_sent_request = nowutc()
    logger.info('DHCPREQUEST of %s on %s to %s port %s', self.client.iface, self.client.client_ip, self.client.server_ip, self.client.server_port)
    if '<MASK>':
        self.request_attempts *= 2
        logger.debug('Increased request attempts to %s', self.request_attempts)
    if '<MASK>':
        timeout_renewing = gen_timeout_request_renew(self.client.lease)
        self.set_timeout(self.current_state, self.timeout_request_renewing, timeout_renewing)
    elif '<MASK>':
        timeout_rebinding = gen_timeout_request_rebind(self.client.lease)
        self.set_timeout(self.current_state, self.timeout_request_rebinding, timeout_rebinding)
    else:
        timeout_requesting = gen_timeout_resend(self.request_attempts)
        self.set_timeout(self.current_state, self.timeout_requesting, timeout_requesting)",False,"['self.current_state == STATE_BOUND', 'self.request_attempts < MAX_ATTEMPTS_REQUEST', 'self.current_state == STATE_RENEWING', 'self.current_state == STATE_REBINDING']",___________________,0.0
"def hierarchical(cls, element):
    """"""
        The idea behind this method is to sort a list of domain hierarchicaly.

        :param element: The element we are currently reading.
        :type element: str

        :return: The formatted element.
        :rtype: str

        .. note::
            For a domain like :code:`aaa.bbb.ccc.tdl`.

            A normal sorting is done in the following order:
                1. :code:`aaa`
                2. :code:`bbb`
                3. :code:`ccc`
                4. :code:`tdl`

            This method allow the sorting to be done in the following order:
                1. :code:`tdl`
                2. :code:`ccc`
                3. :code:`bbb`
                4. :code:`aaa`

        """"""
    to_sort = ''
    full_extension = ''
    element = element.lower()
    url_base = Check().is_url_valid(element, return_base=True)
    if '<MASK>':
        if '<MASK>':
            extension_index = element.rindex('.') + 1
            extension = element[extension_index:]
            if '<MASK>':
                for suffix in PyFunceble.INTERN['psl_db'][extension]:
                    formatted_suffix = '.' + suffix
                    if '<MASK>':
                        suffix_index = element.rindex(formatted_suffix)
                        to_sort = element[:suffix_index]
                        full_extension = suffix
                        break
            if '<MASK>':
                full_extension = element[extension_index:]
                to_sort = element[:extension_index - 1]
            full_extension += '.'
            tros_ot = to_sort[::-1]
            if '<MASK>':
                full_extension = tros_ot[:tros_ot.index('.')][::-1] + '.' + full_extension
                tros_ot = tros_ot[tros_ot.index('.') + 1:]
                reversion = full_extension + '.'.join([x[::-1] for x in tros_ot.split('.')])
                return Regex(reversion, cls.regex_replace, replace_with='@funilrys').replace().replace('@funilrys', '')
            return Regex(to_sort + full_extension, cls.regex_replace, replace_with='@funilrys').replace().replace('@funilrys', '')
        return element
    protocol_position = element.rindex(url_base)
    protocol = element[:protocol_position]
    return protocol + cls.hierarchical(url_base)",False,"['not isinstance(url_base, str)', ""'.' in element"", ""extension in PyFunceble.INTERN['psl_db']"", 'not full_extension', ""'.' in tros_ot"", 'element.endswith(formatted_suffix)']",___________________,0.0
"def _is_version_greater(self):
    """"""
        Check if the current version is greater as the older older one.
        """"""
    checked = Version(True).check_versions(self.current_version[0], self.version_yaml)
    if '<MASK>':
        return True
    return False",False,['checked is not None and (not checked)'],_version_version_version_version_version_version_version_version_version_,0.0
"def getSlotList(self, tokenPresent=False):
    """"""
        C_GetSlotList

        :param tokenPresent: `False` (default) to list all slots,
          `True` to list only slots with present tokens
        :type tokenPresent: bool
        :return: a list of available slots
        :rtype: list
        """"""
    slotList = PyKCS11.LowLevel.ckintlist()
    rv = self.lib.C_GetSlotList(CK_TRUE if tokenPresent else CK_FALSE, slotList)
    if '<MASK>':
        raise PyKCS11Error(rv)
    s = []
    for x in range(len(slotList)):
        s.append(slotList[x])
    return s",False,['rv != CKR_OK'],",,,,,,,,,,,,,,,,,,,",0.0
"def new_panel(store, institute_id, panel_name, display_name, csv_lines):
    """"""Create a new gene panel.

    Args:
        store(scout.adapter.MongoAdapter)
        institute_id(str)
        panel_name(str)
        display_name(str)
        csv_lines(iterable(str)): Stream with genes

    Returns:
        panel_id: the ID of the new panel document created or None

    """"""
    institute_obj = store.institute(institute_id)
    if '<MASK>':
        flash('{}: institute not found'.format(institute_id))
        return None
    panel_obj = store.gene_panel(panel_name)
    if '<MASK>':
        flash('panel already exists: {} - {}'.format(panel_obj['panel_name'], panel_obj['display_name']))
        return None
    log.debug('parse genes from CSV input')
    try:
        new_genes = parse_genes(csv_lines)
    except SyntaxError as error:
        flash(error.args[0], 'danger')
        return None
    log.debug('build new gene panel')
    panel_id = None
    try:
        panel_data = build_panel(dict(panel_name=panel_name, institute=institute_obj['_id'], version=1.0, date=dt.datetime.now(), display_name=display_name, genes=new_genes), store)
        panel_id = store.add_gene_panel(panel_data)
    except Exception as err:
        log.error('An error occurred while adding the gene panel {}'.format(err))
    return panel_id",False,"['institute_obj is None', 'panel_obj']",____name__name_name_name_name_name_name_,0.0
"def get_first_name_last_name(self):
    """"""
        :rtype: str
        """"""
    names = []
    if '<MASK>':
        names += self._get_first_names()
    if '<MASK>':
        names += self._get_additional_names()
    if '<MASK>':
        names += self._get_last_names()
    if '<MASK>':
        return helpers.list_to_string(names, ' ')
    else:
        return self.get_full_name()",False,"['self._get_first_names()', 'self._get_additional_names()', 'self._get_last_names()', 'names']",___________________,0.0
"def _set_ocsp_callback(self, helper, data):
    """"""
        This internal helper does the common work for
        ``set_ocsp_server_callback`` and ``set_ocsp_client_callback``, which is
        almost all of it.
        """"""
    self._ocsp_helper = helper
    self._ocsp_callback = helper.callback
    if '<MASK>':
        self._ocsp_data = _ffi.NULL
    else:
        self._ocsp_data = _ffi.new_handle(data)
    rc = _lib.SSL_CTX_set_tlsext_status_cb(self._context, self._ocsp_callback)
    _openssl_assert(rc == 1)
    rc = _lib.SSL_CTX_set_tlsext_status_arg(self._context, self._ocsp_data)
    _openssl_assert(rc == 1)",False,['data is None'],___________________,0.0
"def transform(self, Z):
    """"""Transform documents to document-term matrix.

        Extract token counts out of raw text documents using the vocabulary
        fitted with fit or the one provided to the constructor.

        Parameters
        ----------
        raw_documents : iterable
            An iterable which yields either str, unicode or file objects.

        Returns
        -------
        X : sparse matrix, [n_samples, n_features]
            Document-term matrix.
        """"""
    if '<MASK>':
        self._validate_vocabulary()
    self._check_vocabulary()
    analyze = self.build_analyzer()
    mapper = self.broadcast(self._count_vocab, Z.context)
    Z = Z.transform(lambda X: list(map(analyze, X)), column='X').transform(mapper, column='X', dtype=sp.spmatrix)
    return Z",False,"[""not hasattr(self, 'vocabulary_')""]",___________________,0.0
"def set(self, key, val):
    """"""
        Sets a header field with the given value, removing
        previous values.

        Usage::

            headers = HTTPHeaderDict(foo='bar')
            headers.set('Foo', 'baz')
            headers['foo']
            > 'baz'
        """"""
    key_lower = key.lower()
    new_vals = (key, val)
    vals = self._container.setdefault(key_lower, new_vals)
    if '<MASK>':
        self._container[key_lower] = [vals[0], vals[1], val]",False,['new_vals is not vals'],_((((((((((((((((((,0.0
"def groupByWordIndex(self, transaction: 'TransTmpl', offset: int):
    """"""
        Group transaction parts splited on words to words

        :param transaction: TransTmpl instance which parts
            should be grupped into words
        :return: generator of tuples (wordIndex, list of transaction parts
            in this word)
        """"""
    actualW = None
    partsInWord = []
    wordWidth = self.wordWidth
    for item in self.splitOnWords(transaction, offset):
        _actualW = item.startOfPart // wordWidth
        if '<MASK>':
            actualW = _actualW
            partsInWord.append(item)
        elif '<MASK>':
            yield (actualW, partsInWord)
            actualW = _actualW
            partsInWord = [item]
        else:
            partsInWord.append(item)
    if '<MASK>':
        yield (actualW, partsInWord)",False,"['partsInWord', 'actualW is None', '_actualW > actualW']",___________________,0.0
"def from_sphinx(cls, app):
    """"""Class method to create a :class:`Gallery` instance from the
        configuration of a sphinx application""""""
    app.config.html_static_path.append(os.path.join(os.path.dirname(__file__), '_static'))
    config = app.config.example_gallery_config
    insert_bokeh = config.get('insert_bokeh')
    if '<MASK>':
        if '<MASK>':
            import bokeh
            insert_bokeh = bokeh.__version__
        app.add_stylesheet(NotebookProcessor.BOKEH_STYLE_SHEET.format(version=insert_bokeh))
        app.add_javascript(NotebookProcessor.BOKEH_JS.format(version=insert_bokeh))
    insert_bokeh_widgets = config.get('insert_bokeh_widgets')
    if '<MASK>':
        if '<MASK>':
            import bokeh
            insert_bokeh_widgets = bokeh.__version__
        app.add_stylesheet(NotebookProcessor.BOKEH_WIDGETS_STYLE_SHEET.format(version=insert_bokeh_widgets))
        app.add_javascript(NotebookProcessor.BOKEH_WIDGETS_JS.format(version=insert_bokeh_widgets))
    if '<MASK>':
        return
    cls(**app.config.example_gallery_config).process_directories()",False,"['insert_bokeh', 'insert_bokeh_widgets', 'not app.config.process_examples', 'not isstring(insert_bokeh)', 'not isstring(insert_bokeh_widgets)']",___________________,0.0
"def render_node(node):
    """"""Renders node as JSON""""""
    if '<MASK>':
        return node
    else:
        return {'type': node['type'], 'data': embeds.to_json(node['type'], node['data'])}",False,"[""node['type'] == 'paragraph'""]","_json(node):
    """"""...........",0.0
"def requirements(work_dir, hive_root, with_requirements, with_dockerfile, active_module, active_module_file):
    """"""编译全新依赖文件""""""
    import sys
    sys.path.insert(0, hive_root)
    hive_root = os.path.abspath(os.path.expanduser(hive_root))
    work_dir = work_dir or os.path.join(os.environ.get('FANTASY_APP_PATH', os.getcwd()))
    work_dir = os.path.expanduser(work_dir)
    requirements_root = os.path.join(work_dir, 'requirements')
    migrate_root = os.path.join(work_dir, 'migrations')
    active_module_paths = []
    active_module_list = []
    if '<MASK>':
        with open(active_module_file, 'r') as fp:
            for l in fp:
                pkg = l.split('#')[0].strip()
                if '<MASK>':
                    active_module_list.append(l.strip('\n'))
            pass
    active_module_list += active_module
    for m in active_module_list:
        try:
            mod = importlib.import_module(m)
            active_module_paths.append(os.path.dirname(mod.__file__))
        except ImportError:
            click.echo('module ""%s"" not found.' % m, color='yellow')
            pass
        pass

    def build_requirements():
        """"""构造requirements文件

        requirements文件共分为两份：

        - hive.txt  从hive项目中直接复制
        - hive-modules.txt 从指定的模块中装载依赖项

        .. note::
            requirements要求必须是顺序无关的
            因为我们会使用set来去重，并按照value排序

        """"""
        if '<MASK>':
            os.makedirs(requirements_root)
            pass
        click.echo(click.style('Generate hive requirements...', fg='yellow'))
        shutil.copy(os.path.join(hive_root, 'requirements.txt'), os.path.join(requirements_root, 'hive.txt'))
        click.echo(click.style('Generate hive-module requirements...', fg='yellow'))
        requirements_files = []
        for m in active_module_paths:
            t = os.path.join(m, 'requirements.txt')
            if '<MASK>':
                requirements_files.append(t)
            pass
        module_packages = set()
        with fileinput.input(requirements_files) as fp:
            for line in fp:
                pkg = line.split('#')[0].strip()
                if '<MASK>':
                    module_packages.add(pkg)
            pass
        with click.open_file(os.path.join(requirements_root, 'hive-modules.txt'), 'w') as fp:
            for p in module_packages:
                fp.write('%s\n' % p)
            pass
        pass

    def build_dockerfile():
        """"""构造Dockerfile""""""
        modules_in_hive = map(lambda x: x.replace(hive_root, '').lstrip('/'), filter(lambda x: x.startswith(hive_root), active_module_paths))
        modules_path = ' '.join(modules_in_hive)
        docker_file = os.path.join(os.path.dirname(requirements_root), 'Dockerfile')
        if '<MASK>':
            click.echo(click.style('Found Dockerfile,try update...', fg='yellow'))
            with open(docker_file, 'r') as fp:
                buffer = fp.read()
                pass
            import re
            replaced = re.sub('ARG HIVE_PACKAGES="".*""', 'ARG HIVE_PACKAGES=""%s""' % modules_path, buffer)
            with open(docker_file, 'w') as fp:
                fp.write(replaced)
                pass
            pass
        pass

    def build_migrations():
        models_pairs = filter(lambda pair: os.path.exists(pair[0]), map(lambda x: (os.path.join(x[0], 'models.py'), x[1]), [(v, active_module_list[i]) for i, v in enumerate(active_module_paths)]))
        try:
            _, models = zip(*models_pairs)
        except ValueError:
            click.echo(click.style('No models found,is it include in your PYTHONPATH?\nModules: %s' % ','.join(active_module_list), fg='yellow'))
            return
        click.echo(click.style('Found models.txt,try update...', fg='yellow'))
        with open(os.path.join(migrate_root, 'models.txt'), 'w') as fp:
            for p in models:
                fp.write('%s\n' % p)
            pass
        pass

    def build_tasks():
        tasks_pairs = filter(lambda pair: os.path.exists(pair[0]), map(lambda x: (os.path.join(x[0], 'tasks.py'), x[1]), [(v, active_module_list[i]) for i, v in enumerate(active_module_paths)]))
        try:
            _, tasks = zip(*tasks_pairs)
        except ValueError:
            click.echo(click.style('No tasks found,is it include in your PYTHONPATH?\nModules: %s' % ','.join(active_module_list), fg='yellow'))
            return
        click.echo(click.style('Found tasks.txt,try update...', fg='yellow'))
        with open(os.path.join(migrate_root, 'tasks.txt'), 'w') as fp:
            for p in tasks:
                fp.write('%s\n' % p)
        pass
    if '<MASK>':
        build_requirements()
    if '<MASK>':
        build_dockerfile()
    if '<MASK>':
        build_migrations()
    if '<MASK>':
        build_tasks()
    click.echo(click.style('Generate done...', fg='yellow'))
    pass",False,"['active_module_file', 'with_requirements', 'with_dockerfile', 'os.path.exists(migrate_root)', 'os.path.exists(migrate_root)', 'not os.path.exists(requirements_root)', 'os.path.exists(docker_file)', 'os.path.exists(t)', 'pkg', 'pkg']",___________________,0.0
"def get_table_content(self, table):
    """"""trick to get table content without actually writing it

        return an aligned list of lists containing table cells values as string
        """"""
    result = [[]]
    cols = table.cols
    for cell in self.compute_content(table):
        if '<MASK>':
            result.append([])
            cols = table.cols
        cols -= 1
        result[-1].append(cell)
    while len(result[-1]) < cols:
        result[-1].append('')
    return result",False,['cols == 0'],"_,,,,,,,,,,,,,,,,,,",0.0
"def adjust_frame(proc_obj, name, pos, absolute_pos):
    """"""Adjust stack frame by pos positions. If absolute_pos then
    pos is an absolute number. Otherwise it is a relative number.

    A negative number indexes from the other end.""""""
    if '<MASK>':
        proc_obj.errmsg('No stack.')
        return
    if '<MASK>':
        if '<MASK>':
            pos = frame_num(proc_obj, pos)
        else:
            pos = -pos - 1
            pass
    else:
        pos += proc_obj.curindex
        pass
    if '<MASK>':
        proc_obj.errmsg('Adjusting would put us beyond the oldest frame.')
        return
    elif '<MASK>':
        proc_obj.errmsg('Adjusting would put us beyond the newest frame.')
        return
    proc_obj.curindex = pos
    proc_obj.curframe = proc_obj.stack[proc_obj.curindex][0]
    proc_obj.location()
    proc_obj.list_lineno = None
    proc_obj.list_offset = proc_obj.curframe.f_lasti
    proc_obj.list_object = proc_obj.curframe
    proc_obj.list_filename = proc_obj.curframe.f_code.co_filename
    return",False,"['not proc_obj.curframe', 'absolute_pos', 'pos < 0', 'pos >= 0', 'pos >= len(proc_obj.stack)']",___________________,0.0
"def wrap_key(self, key_material, wrapping_method, key_wrap_algorithm, encryption_key):
    """"""
        Args:
            key_material (bytes): The bytes of the key to wrap. Required.
            wrapping_method (WrappingMethod): A WrappingMethod enumeration
                specifying what wrapping technique to use to wrap the key
                material. Required.
            key_wrap_algorithm (BlockCipherMode): A BlockCipherMode
                enumeration specifying the key wrapping algorithm to use to
                wrap the key material. Required.
            encryption_key (bytes): The bytes of the encryption key to use
                to encrypt the key material. Required.

        Returns:
            bytes: the bytes of the wrapped key

        Raises:
            CryptographicFailure: Raised when an error occurs during key
                wrapping.
            InvalidField: Raised when an unsupported wrapping or encryption
                algorithm is specified.

        Example:
            >>> engine = CryptographyEngine()
            >>> result = engine.wrap_key(
            ...     key_material=(
            ...         b'\x00\x11""3DUfw'
            ...         b'\x88\x99ª»ÌÝîÿ'
            ...     )
            ...     wrapping_method=enums.WrappingMethod.ENCRYPT,
            ...     key_wrap_algorithm=enums.BlockCipherMode.NIST_KEY_WRAP,
            ...     encryption_key=(
            ...         b'\x00\x01\x02\x03\x04\x05\x06\x07'
            ...         b'\x08	
\x0b\x0c\r\x0e\x0f'
            ...     )
            ... )
            >>> result
            b'\x1f¦\x8b
\x81\x12´G®óKØûZ{\x82\x9d>\x86#q
            ÒÏå'
        """"""
    if '<MASK>':
        if '<MASK>':
            try:
                wrapped_key = keywrap.aes_key_wrap(encryption_key, key_material, default_backend())
                return wrapped_key
            except Exception as e:
                raise exceptions.CryptographicFailure(str(e))
        else:
            raise exceptions.InvalidField(""Encryption algorithm '{0}' is not a supported key wrapping algorithm."".format(key_wrap_algorithm))
    else:
        raise exceptions.InvalidField(""Wrapping method '{0}' is not a supported key wrapping method."".format(wrapping_method))",False,"['wrapping_method == enums.WrappingMethod.ENCRYPT', 'key_wrap_algorithm == enums.BlockCipherMode.NIST_KEY_WRAP']","_,,_,,_,,_,,_,,_,,_",0.0
"def show_progress(self, iteration, total, length=40, min_level=0, prefix=None, carriage_return=True, suffix=None, symbol=None):
    """"""create a terminal progress bar, default bar shows for verbose+
 
           Parameters
           ==========
           iteration: current iteration (Int)
           total: total iterations (Int)
           length: character length of bar (Int)
        """"""
    if '<MASK>':
        percent = 100 * (iteration / float(total))
        progress = int(length * iteration // total)
        if '<MASK>':
            suffix = ''
        if '<MASK>':
            prefix = 'Progress'
        if '<MASK>':
            percent = 100
            progress = length
        if '<MASK>':
            symbol = '='
        if '<MASK>':
            bar = symbol * progress + '|' + '-' * (length - progress - 1)
        else:
            bar = symbol * progress + '-' * (length - progress)
        if '<MASK>':
            percent = '%5s' % '{0:.1f}'.format(percent)
            output = '\r' + prefix + ' |%s| %s%s %s' % (bar, percent, '%', suffix)
            (sys.stdout.write(output),)
            if '<MASK>':
                sys.stdout.write('\n')
            sys.stdout.flush()",False,"['not self.level == QUIET', 'suffix is None', 'prefix is None', 'percent >= 100', 'symbol is None', 'progress < length', 'self.level > min_level', 'iteration == total and carriage_return']","_,,,,,,,,,,,,,,,,,,",0.0
"def build_executable(self, lflags=None, verbose=False, openmp=False):
    """"""Compile source to executable with likwid capabilities and return the executable name.""""""
    compiler, compiler_args = self._machine.get_compiler()
    kernel_obj_filename = self.compile_kernel(openmp=openmp, verbose=verbose)
    out_filename, already_exists = self._get_intermediate_file(os.path.splitext(os.path.basename(kernel_obj_filename))[0], binary=True, fp=False)
    if '<MASK>':
        main_source_filename = self.get_main_code(as_filename=True)
        if '<MASK>':
            print('Could not find LIKWID_INCLUDE (e.g., ""-I/app/likwid/4.1.2/include"") and LIKWID_LIB (e.g., ""-L/apps/likwid/4.1.2/lib"") environment variables', file=sys.stderr)
            sys.exit(1)
        compiler_args += ['-std=c99', '-I' + reduce_path(os.path.abspath(os.path.dirname(os.path.realpath(__file__))) + '/headers/'), os.environ.get('LIKWID_INCLUDE', ''), os.environ.get('LIKWID_INC', ''), '-llikwid']
        if '<MASK>':
            compiler_args = compiler_args[:-1]
        if '<MASK>':
            lflags = []
        lflags += os.environ['LIKWID_LIB'].split(' ') + ['-pthread']
        compiler_args += os.environ['LIKWID_LIB'].split(' ') + ['-pthread']
        infiles = [reduce_path(os.path.abspath(os.path.dirname(os.path.realpath(__file__))) + '/headers/dummy.c'), kernel_obj_filename, main_source_filename]
        cmd = [compiler] + infiles + compiler_args + ['-o', out_filename]
        cmd = list(filter(bool, cmd))
        if '<MASK>':
            print('Executing (build_executable): ', ' '.join(cmd))
        try:
            subprocess.check_output(cmd)
        except subprocess.CalledProcessError as e:
            print('Build failed:', e, file=sys.stderr)
            sys.exit(1)
    elif '<MASK>':
        print('Executing (build_executable): ', 'using cached', out_filename)
    return out_filename",False,"['not already_exists', ""not (('LIKWID_INCLUDE' in os.environ or 'LIKWID_INC' in os.environ) and 'LIKWID_LIB' in os.environ)"", ""os.environ.get('LIKWID_LIB') == ''"", 'lflags is None', 'verbose', 'verbose']",___________________,0.0
"def _subgraph_parse(self, node, pathnode, extra_blocks):
    """"""parse the body and any `else` block of `if` and `for` statements""""""
    loose_ends = []
    self.tail = node
    self.dispatch_list(node.body)
    loose_ends.append(self.tail)
    for extra in extra_blocks:
        self.tail = node
        self.dispatch_list(extra.body)
        loose_ends.append(self.tail)
    if '<MASK>':
        self.tail = node
        self.dispatch_list(node.orelse)
        loose_ends.append(self.tail)
    else:
        loose_ends.append(node)
    if '<MASK>':
        bottom = '%s' % self._bottom_counter
        self._bottom_counter += 1
        for le in loose_ends:
            self.graph.connect(le, bottom)
        self.tail = bottom",False,"['node.orelse', 'node']",___________________,0.0
"def _get(self, node, name, fast_access, shortcuts, max_depth, auto_load, with_links):
    """"""Searches for an item (parameter/result/group node) with the given `name`.

        :param node: The node below which the search is performed

        :param name: Name of the item (full name or parts of the full name)

        :param fast_access: If the result is a parameter, whether fast access should be applied.

        :param max_depth:

            Maximum search depth relative to start node.

        :param auto_load:

            If data should be automatically loaded

        :param with_links

            If links should be considered

        :return:

            The found instance (result/parameter/group node) or if fast access is True and you
            found a parameter or result that supports fast access, the contained value is returned.

        :raises:

            AttributeError if no node with the given name can be found.
            Raises errors that are raised by the storage service if `auto_load=True` and
            item cannot be found.

        """"""
    if '<MASK>':
        raise ValueError('If you allow auto-loading you mus allow links.')
    if '<MASK>':
        split_name = name
    elif '<MASK>':
        split_name = list(name)
    elif '<MASK>':
        split_name = [name]
    else:
        split_name = name.split('.')
    if '<MASK>':
        if '<MASK>':
            return node
        key = split_name[0]
        _, key = self._translate_shortcut(key)
        if '<MASK>':
            node.f_add_group(key)
    if '<MASK>':
        max_depth = float('inf')
    if '<MASK>':
        raise ValueError('Name of node to search for (%s) is longer thant the maximum depth %d' % (str(name), max_depth))
    try_auto_load_directly1 = False
    try_auto_load_directly2 = False
    wildcard_positions = []
    root = self._root_instance
    for idx, key in enumerate(split_name):
        translated_shortcut, key = self._translate_shortcut(key)
        if '<MASK>':
            split_name[idx] = key
        if '<MASK>':
            raise AttributeError('Leading underscores are not allowed for group or parameter names. Cannot return %s.' % key)
        is_wildcard = self._root_instance.f_is_wildcard(key)
        if '<MASK>':
            try_auto_load_directly1 = True
            try_auto_load_directly2 = True
        if '<MASK>':
            wildcard_positions.append((idx, key))
            if '<MASK>':
                try_auto_load_directly1 = True
            if '<MASK>':
                try_auto_load_directly2 = True
    run_idx = root.v_idx
    wildcard_exception = None
    if '<MASK>':
        for wildcard_pos, wildcard in wildcard_positions:
            split_name[wildcard_pos] = root.f_wildcard(wildcard, run_idx)
        raise AttributeError(""%s is not part of your trajectory or it's tree. "" % str('.'.join(split_name)))
    if '<MASK>':
        with self._disable_logging:
            try:
                for wildcard_pos, wildcard in wildcard_positions:
                    split_name[wildcard_pos] = root.f_wildcard(wildcard, run_idx)
                result = self._perform_get(node, split_name, fast_access, shortcuts, max_depth, auto_load, with_links, try_auto_load_directly1)
                return result
            except (pex.DataNotInStorageError, AttributeError) as exc:
                wildcard_exception = exc
    if '<MASK>':
        for wildcard_pos, wildcard in wildcard_positions:
            split_name[wildcard_pos] = root.f_wildcard(wildcard, -1)
    try:
        return self._perform_get(node, split_name, fast_access, shortcuts, max_depth, auto_load, with_links, try_auto_load_directly2)
    except (pex.DataNotInStorageError, AttributeError):
        if '<MASK>':
            raise wildcard_exception
        else:
            raise",False,"['auto_load and (not with_links)', 'isinstance(name, list)', 'node.v_is_root', 'max_depth is None', 'len(split_name) > max_depth and shortcuts', 'try_auto_load_directly1 and try_auto_load_directly2 and (not auto_load)', 'run_idx > -1', 'wildcard_positions', 'isinstance(name, tuple)', ""len(split_name) == 1 and split_name[0] == ''"", 'key in SUBTREE_MAPPING and key not in node._children', 'translated_shortcut', ""key[0] == '_'"", 'not is_wildcard and key not in self._nodes_and_leaves and (key not in self._links_count)', 'is_wildcard', 'isinstance(name, int)', 'root.f_wildcard(key) not in self._nodes_and_leaves', 'root.f_wildcard(key, -1) not in self._nodes_and_leaves', 'wildcard_exception is not None']",___________________,0.0
"def pick_peaks(nc, L=16):
    """"""Obtain peaks from a novelty curve using an adaptive threshold.""""""
    offset = nc.mean() / 20.0
    nc = filters.gaussian_filter1d(nc, sigma=4)
    th = filters.median_filter(nc, size=L) + offset
    peaks = []
    for i in range(1, nc.shape[0] - 1):
        if '<MASK>':
            if '<MASK>':
                peaks.append(i)
    return peaks",False,"['nc[i - 1] < nc[i] and nc[i] > nc[i + 1]', 'nc[i] > th[i]']",(((((((((((((((((((,0.0
"def branch_length_to_years(self):
    """"""
        This function sets branch length to reflect the date differences between parent and child
        nodes measured in years. Should only be called after :py:meth:`timetree.ClockTree.convert_dates` has been called.

        Returns
        -------
         None
            All manipulations are done in place on the tree

        """"""
    self.logger('ClockTree.branch_length_to_years: setting node positions in units of years', 2)
    if '<MASK>':
        self.logger('ClockTree.branch_length_to_years: infer ClockTree first', 2, warn=True)
    self.tree.root.branch_length = 0.1
    for n in self.tree.find_clades(order='preorder'):
        if '<MASK>':
            n.branch_length = n.numdate - n.up.numdate",False,"[""not hasattr(self.tree.root, 'numdate')"", 'n.up is not None']","_,,,,,,,,,,,,,,,,,,",0.0
"def _check_required_files(self):
    """"""Checks whetner the trace and log files are available
        """"""
    if '<MASK>':
        raise eh.InspectionError('The provided trace file could not be opened: {}'.format(self.trace_file))
    if '<MASK>':
        raise eh.InspectionError('The .nextflow.log files could not be opened. Are you sure you are in a nextflow project directory?')",False,"['not os.path.exists(self.trace_file)', 'not os.path.exists(self.log_file)']","_(self):
    """"""
    """"""
    """"""
    """"""
    """"""",0.0
"def findsource(func):
    file = getfile(func)
    module = inspect.getmodule(func, file)
    lines = linecache.getlines(file, module.__dict__)
    code = func.__code__
    lnum = code.co_firstlineno - 1
    pat = re.compile('^(\\s*def\\s)|(\\s*async\\s+def\\s)|(.*(?<!\\w)lambda(:|\\s))|^(\\s*@)')
    while lnum > 0:
        if '<MASK>':
            break
        lnum = lnum - 1
    return (lines, lnum)",False,['pat.match(lines[lnum])'],",,,,,,,,,,,,,,,,,,,",0.0
"def getAll(self, deviceUid):
    """"""
        Retrieves a list of the last cached message for all events from a specific device.
        """"""
    if '<MASK>':
        deviceUid = DeviceUid(**deviceUid)
    url = 'api/v0002/device/types/%s/devices/%s/events' % (deviceUid.typeId, deviceUid.deviceId)
    r = self._apiClient.get(url)
    if '<MASK>':
        events = []
        for event in r.json():
            events.append(LastEvent(**event))
        return events
    else:
        raise ApiException(r)",False,"['not isinstance(deviceUid, DeviceUid) and isinstance(deviceUid, dict)', 'r.status_code == 200']",___________________,0.0
"def get_single_file(self, pool, source, target):
    """"""Download a single file or a directory by adding a task into queue""""""
    if '<MASK>':
        if '<MASK>':
            basepath = S3URL(source).path
            for f in (f for f in self.s3walk(source) if not f['is_dir']):
                pool.download(f['name'], os.path.join(target, os.path.relpath(S3URL(f['name']).path, basepath)))
        else:
            message('omitting directory ""%s"".' % source)
    else:
        pool.download(source, target)",False,"['source[-1] == PATH_SEP', 'self.opt.recursive']",___________________,0.0
"def vcf_records(self, qualified=False):
    """"""Generates parsed VcfRecord objects.

        Typically called in a for loop to process each vcf record in a
        VcfReader. VcfReader must be opened in advanced and closed when
        complete. Skips all headers.

        Args:
            qualified: When True, sample names are prefixed with file name

        Returns:
            Parsed VcfRecord

        Raises:
            StopIteration: when reader is exhausted.
            TypeError: if reader is closed.
        """"""
    if '<MASK>':
        sample_names = self.qualified_sample_names
    else:
        sample_names = self.sample_names
    for line in self._file_reader.read_lines():
        if '<MASK>':
            continue
        yield VcfRecord.parse_record(line, sample_names)",False,"['qualified', ""line.startswith('#')""]","__,__,_,_,_,_,_,_,_",0.0
"def _size_coverter(s):
    """"""Converts size string into megabytes

        Parameters
        ----------
        s : str
            The size string can be '30KB', '20MB' or '1GB'

        Returns
        -------
        float
            With the size in bytes

        """"""
    if '<MASK>':
        return float(s.rstrip('KB')) / 1024
    elif '<MASK>':
        return float(s.rstrip('B')) / 1024 / 1024
    elif '<MASK>':
        return float(s.rstrip('MB'))
    elif '<MASK>':
        return float(s.rstrip('GB')) * 1024
    elif '<MASK>':
        return float(s.rstrip('TB')) * 1024 * 1024
    else:
        return float(s)",False,"[""s.upper().endswith('KB')"", ""s.upper().endswith(' B')"", ""s.upper().endswith('MB')"", ""s.upper().endswith('GB')"", ""s.upper().endswith('TB')""]","_,,,,,,,,,,,,,,,,,,",0.0
"def infer_gtr(self, print_raw=False, marginal=False, normalized_rate=True, fixed_pi=None, pc=5.0, **kwargs):
    """"""
        Calculates a GTR model given the multiple sequence alignment and the tree.
        It performs ancestral sequence inferrence (joint or marginal), followed by
        the branch lengths optimization. Then, the numbers of mutations are counted
        in the optimal tree and related to the time within the mutation happened.
        From these statistics, the relative state transition probabilities are inferred,
        and the transition matrix is computed.

        The result is used to construct the new GTR model of type 'custom'.
        The model is assigned to the TreeAnc and is used in subsequent analysis.

        Parameters
        -----------

         print_raw : bool
            If True, print the inferred GTR model

         marginal : bool
            If True, use marginal sequence reconstruction

         normalized_rate : bool
            If True, sets the mutation rate prefactor to 1.0.

         fixed_pi : np.array
            Provide the equilibrium character concentrations.
            If None is passed, the concentrations will be inferred from the alignment.

         pc: float
            Number of pseudo counts to use in gtr inference

        Returns
        -------

         gtr : GTR
            The inferred GTR model
        """"""
    if '<MASK>':
        _ml_anc = self._ml_anc_marginal
    else:
        _ml_anc = self._ml_anc_joint
    self.logger('TreeAnc.infer_gtr: inferring the GTR model from the tree...', 1)
    if '<MASK>':
        self.logger('TreeAnc.infer_gtr: ERROR, alignment or tree are missing', 0)
        return ttconf.ERROR
    _ml_anc(final=True, **kwargs)
    alpha = list(self.gtr.alphabet)
    n = len(alpha)
    nij = np.zeros((n, n))
    Ti = np.zeros(n)
    self.logger('TreeAnc.infer_gtr: counting mutations...', 2)
    for node in self.tree.find_clades():
        if '<MASK>':
            for a, pos, d in node.mutations:
                i, j = (alpha.index(d), alpha.index(a))
                nij[i, j] += 1
                Ti[j] += 0.5 * self._branch_length_to_gtr(node)
                Ti[i] -= 0.5 * self._branch_length_to_gtr(node)
            for ni, nuc in enumerate(node.cseq):
                i = alpha.index(nuc)
                Ti[i] += self._branch_length_to_gtr(node) * self.multiplicity[ni]
    self.logger('TreeAnc.infer_gtr: counting mutations...done', 3)
    if '<MASK>':
        print('alphabet:', alpha)
        print('n_ij:', nij, nij.sum())
        print('T_i:', Ti, Ti.sum())
    root_state = np.array([np.sum((self.tree.root.cseq == nuc) * self.multiplicity) for nuc in alpha])
    self._gtr = GTR.infer(nij, Ti, root_state, fixed_pi=fixed_pi, pc=pc, alphabet=self.gtr.alphabet, logger=self.logger, prof_map=self.gtr.profile_map)
    if '<MASK>':
        self.logger('TreeAnc.infer_gtr: setting overall rate to 1.0...', 2)
        self._gtr.mu = 1.0
    return self._gtr",False,"['marginal', 'self.tree is None or self.aln is None', 'print_raw', 'normalized_rate', ""hasattr(node, 'mutations')""]",___________________,0.0
"def getLogicalInterfacesOnThingType(self, thingTypeId, draft=False):
    """"""
        Get all logical interfaces for a thing type.
        Parameters:
          - thingTypeId (string)
          - draft (boolean)
        Returns:
            - list of logical interface ids
            - HTTP response object
        Throws APIException on failure.
        """"""
    if '<MASK>':
        req = ApiClient.allThingTypeLogicalInterfacesUrl % (self.host, '/draft', thingTypeId)
    else:
        req = ApiClient.allThingTypeLogicalInterfacesUrl % (self.host, '', thingTypeId)
    resp = requests.get(req, auth=self.credentials, verify=self.verify)
    if '<MASK>':
        self.logger.debug('All thing type logical interfaces retrieved')
    else:
        raise ibmiotf.APIException(resp.status_code, 'HTTP error getting all thing type logical interfaces', resp)
    return ([appintf['id'] for appintf in resp.json()], resp.json())",False,"['draft', 'resp.status_code == 200']",___________________,0.0
"def render_to_response(self, context, **response_kwargs):
    """"""
        Returns a PDF response with a template rendered with the given context.
        """"""
    filename = response_kwargs.pop('filename', None)
    cmd_options = response_kwargs.pop('cmd_options', None)
    if '<MASK>':
        if '<MASK>':
            filename = self.get_filename()
        if '<MASK>':
            cmd_options = self.get_cmd_options()
        return super(PDFTemplateView, self).render_to_response(context=context, filename=filename, show_content_in_browser=self.show_content_in_browser, header_template=self.header_template, footer_template=self.footer_template, cmd_options=cmd_options, cover_template=self.cover_template, **response_kwargs)
    else:
        return super(PDFTemplateView, self).render_to_response(context=context, **response_kwargs)",False,"['issubclass(self.response_class, PDFTemplateResponse)', 'filename is None', 'cmd_options is None']",___________________,0.0
"def normalize(es, esnull):
    """"""normalize the ES(S,pi) and the observed ES(S), separately rescaling
       the positive and negative scores by dividing the mean of the ES(S,pi).
       
       return: NES, NESnull
    """"""
    nEnrichmentScores = np.zeros(es.shape)
    nEnrichmentNulls = np.zeros(esnull.shape)
    esnull_pos = (esnull * (esnull >= 0)).mean(axis=1)
    esnull_neg = (esnull * (esnull < 0)).mean(axis=1)
    for i in range(esnull.shape[0]):
        if '<MASK>':
            nEnrichmentScores[i] = es[i] / esnull_pos[i]
        else:
            nEnrichmentScores[i] = -es[i] / esnull_neg[i]
        for j in range(esnull.shape[1]):
            if '<MASK>':
                nEnrichmentNulls[i, j] = esnull[i, j] / esnull_pos[i]
            else:
                nEnrichmentNulls[i, j] = -esnull[i, j] / esnull_neg[i]
    return (nEnrichmentScores, nEnrichmentNulls)",False,"['es[i] >= 0', 'esnull[i, j] >= 0']",___________________,0.0
"def tree(self, in_tree):
    """"""
        assigns a tree to the internal self._tree variable. The tree is either
        loaded from file (if in_tree is str) or assigned (if in_tree is a Phylo.tree)
        """"""
    from os.path import isfile
    if '<MASK>':
        self._tree = in_tree
    elif '<MASK>':
        try:
            self._tree = Phylo.read(in_tree, 'newick')
        except:
            fmt = in_tree.split('.')[-1]
            if '<MASK>':
                self._tree = Phylo.read(in_tree, 'nexus')
            else:
                self.logger('TreeAnc: could not load tree, format needs to be nexus or newick! input was ' + str(in_tree), 1)
                self._tree = None
                return ttconf.ERROR
    else:
        self.logger('TreeAnc: could not load tree! input was ' + str(in_tree), 0)
        self._tree = None
        return ttconf.ERROR
    for node in self._tree.find_clades():
        if '<MASK>':
            node.__delattr__('sequence')
        node.original_length = node.branch_length
        node.mutation_length = node.branch_length
    self.prepare_tree()
    return ttconf.SUCCESS",False,"['isinstance(in_tree, Phylo.BaseTree.Tree)', 'type(in_tree) in string_types and isfile(in_tree)', ""hasattr(node, 'sequence')"", ""fmt in ['nexus', 'nex']""]",___________________,0.0
"def _check_name(self, node_type, name, node, confidence=interfaces.HIGH):
    """"""check for a name using the type's regexp""""""

    def _should_exempt_from_invalid_name(node):
        if '<MASK>':
            inferred = utils.safe_infer(node)
            if '<MASK>':
                return True
        return False
    if '<MASK>':
        clobbering, _ = utils.clobber_in_except(node)
        if '<MASK>':
            return
    if '<MASK>':
        return
    if '<MASK>':
        self.stats['badname_' + node_type] += 1
        self.add_message('blacklisted-name', node=node, args=name)
        return
    regexp = self._name_regexps[node_type]
    match = regexp.match(name)
    if '<MASK>':
        name_group = self._find_name_group(node_type)
        bad_name_group = self._bad_names.setdefault(name_group, {})
        warnings = bad_name_group.setdefault(match.lastgroup, [])
        warnings.append((node, node_type, name, confidence))
    if '<MASK>':
        self._raise_name_warning(node, node_type, name, confidence)",False,"['utils.is_inside_except(node)', 'name in self.config.good_names', 'name in self.config.bad_names', '_is_multi_naming_match(match, node_type, confidence)', 'match is None and (not _should_exempt_from_invalid_name(node))', ""node_type == 'variable'"", 'clobbering', 'isinstance(inferred, astroid.ClassDef)']",___________________,0.0
"def write(self, output_stream, kmip_version=enums.KMIPVersion.KMIP_1_0):
    """"""
        Write the data encoding the Nonce struct to a stream.

        Args:
            output_stream (stream): A data stream in which to encode object
                data, supporting a write method; usually a BytearrayStream
                object.
            kmip_version (KMIPVersion): An enumeration defining the KMIP
                version with which the object will be encoded. Optional,
                defaults to KMIP 1.0.

        Raises:
            ValueError: Raised if the nonce ID or nonce value is not defined.
        """"""
    local_stream = BytearrayStream()
    if '<MASK>':
        self._nonce_id.write(local_stream, kmip_version=kmip_version)
    else:
        raise ValueError('Nonce struct is missing the nonce ID.')
    if '<MASK>':
        self._nonce_value.write(local_stream, kmip_version=kmip_version)
    else:
        raise ValueError('Nonce struct is missing the nonce value.')
    self.length = local_stream.length()
    super(Nonce, self).write(output_stream, kmip_version=kmip_version)
    output_stream.write(local_stream.buffer)",False,"['self._nonce_id', 'self._nonce_value']","_,_,_,_,_,_,_,_,_,_",0.0
"def run_exec(self, cmd, start_opts=None, globals_=None, locals_=None):
    """""" Run debugger on string `cmd' which will executed via the
        builtin function exec. Arguments `globals_' and `locals_' are
        the dictionaries to use for local and global variables. By
        default, the value of globals is globals(), the current global
        variables. If `locals_' is not given, it becomes a copy of
        `globals_'.

        Debugger.core.start settings are passed via optional
        dictionary `start_opts'. Overall debugger settings are in
        Debugger.settings which changed after an instance is created
        . Also see `run_eval' if what you want to run is an
        run_eval'able expression have that result returned and
        `run_call' if you want to debug function run_call.
        """"""
    if '<MASK>':
        globals_ = globals()
    if '<MASK>':
        locals_ = globals_
    if '<MASK>':
        cmd = cmd + '\n'
        pass
    self.core.start(start_opts)
    try:
        exec(cmd, globals_, locals_)
    except DebuggerQuit:
        pass
    finally:
        self.core.stop()
    return",False,"['globals_ is None', 'locals_ is None', 'not isinstance(cmd, types.CodeType)']",___________________,0.0
"def factorize(self, niter=10, compute_w=True, compute_h=True, compute_err=True, show_progress=False):
    """""" Factorize s.t. WH = data

            Parameters
            ----------
            niter : int
                    number of iterations.
            show_progress : bool
                    print some extra information to stdout.
            compute_h : bool
                    iteratively update values for H.
            compute_w : bool
                    iteratively update values for W.
            compute_err : bool
                    compute Frobenius norm |data-WH| after each update and store
                    it to .ferr[k].

            Updated Values
            --------------
            .W : updated values for W.
            .H : updated values for H.
            .ferr : Frobenius norm |data-WH| for each iteration.
        """"""
    if '<MASK>':
        self.init_w()
    if '<MASK>':
        self.init_h()

    def separate_positive(m):
        return (np.abs(m) + m) / 2.0

    def separate_negative(m):
        return (np.abs(m) - m) / 2.0
    if '<MASK>':
        self._logger.setLevel(logging.INFO)
    else:
        self._logger.setLevel(logging.ERROR)
    XtX = np.dot(self.data[:, :].T, self.data[:, :])
    XtX_pos = separate_positive(XtX)
    XtX_neg = separate_negative(XtX)
    self.ferr = np.zeros(niter)
    for i in range(niter):
        XtX_neg_x_W = np.dot(XtX_neg, self.G)
        XtX_pos_x_W = np.dot(XtX_pos, self.G)
        if '<MASK>':
            H_x_WT = np.dot(self.H.T, self.G.T)
            ha = XtX_pos_x_W + np.dot(H_x_WT, XtX_neg_x_W)
            hb = XtX_neg_x_W + np.dot(H_x_WT, XtX_pos_x_W) + 10 ** (-9)
            self.H = (self.H.T * np.sqrt(ha / hb)).T
        if '<MASK>':
            HT_x_H = np.dot(self.H, self.H.T)
            wa = np.dot(XtX_pos, self.H.T) + np.dot(XtX_neg_x_W, HT_x_H)
            wb = np.dot(XtX_neg, self.H.T) + np.dot(XtX_pos_x_W, HT_x_H) + 10 ** (-9)
            self.G *= np.sqrt(wa / wb)
            self.W = np.dot(self.data[:, :], self.G)
        if '<MASK>':
            self.ferr[i] = self.frobenius_norm()
            self._logger.info('Iteration ' + str(i + 1) + '/' + str(niter) + ' FN:' + str(self.ferr[i]))
        else:
            self._logger.info('Iteration ' + str(i + 1) + '/' + str(niter))
        if '<MASK>':
            if '<MASK>':
                self.ferr = self.ferr[:i]
                break",False,"[""not hasattr(self, 'W')"", ""not hasattr(self, 'H')"", 'show_progress', 'compute_h', 'compute_w', 'compute_err', 'i > 1 and compute_err', 'self.converged(i)']",___________________,0.0
"def convertBits(self, sigOrVal, toType):
    """"""
    Cast signed-unsigned, to int or bool
    """"""
    if '<MASK>':
        return convertBits__val(self, sigOrVal, toType)
    elif '<MASK>':
        if '<MASK>':
            v = 0 if sigOrVal._dtype.negated else 1
            return sigOrVal._eq(self.getValueCls().fromPy(v, self))
    elif '<MASK>':
        if '<MASK>':
            return sigOrVal._convSign(toType.signed)
    elif '<MASK>':
        return Operator.withRes(AllOps.BitsToInt, [sigOrVal], toType)
    return default_auto_cast_fn(self, sigOrVal, toType)",False,"['isinstance(sigOrVal, Value)', 'isinstance(toType, HBool)', 'self.bit_length() == 1', 'isinstance(toType, Bits)', 'self.bit_length() == toType.bit_length()', 'toType == INT']",___________________,0.0
"def timetree_likelihood(self):
    """"""
        Return the likelihood of the data given the current branch length in the tree
        """"""
    LH = 0
    for node in self.tree.find_clades(order='preorder'):
        if '<MASK>':
            continue
        LH -= node.branch_length_interpolator(node.branch_length)
    if '<MASK>':
        LH += self.gtr.sequence_logLH(self.tree.root.cseq, pattern_multiplicity=self.multiplicity)
    return LH",False,"['self.aln', 'node.up is None']","_,,,,,,,,,,,,,,,,,,",0.0
"def _init(self):
    """"""
        Set up the store context for a subsequent verification operation.

        Calling this method more than once without first calling
        :meth:`_cleanup` will leak memory.
        """"""
    ret = _lib.X509_STORE_CTX_init(self._store_ctx, self._store._store, self._cert._x509, _ffi.NULL)
    if '<MASK>':
        _raise_current_error()",False,['ret <= 0'],___________________,0.0
"def authenticate(self):
    """"""Authenticate with the Bitfinex API.

        :return:
        """"""
    if '<MASK>':
        raise ValueError('Must supply both key and secret key for API!')
    self.channel_configs['auth'] = {'api_key': self.key, 'secret': self.secret}
    self.conn.send(api_key=self.key, secret=self.secret, auth=True)",False,['not self.key and (not self.secret)'],",,,,,,,,,,,,,,,,,,,",0.0
"def _handle_weekly_repeat_in(self):
    """"""
        Handles repeating both weekly and biweekly events, if the
        current year and month are inside it's l_start_date and l_end_date.
        Four possibilites:
            1. The event starts this month and ends repeating this month.
            2. The event starts this month and doesn't finish repeating
            this month.
            3. The event didn't start this month but ends repeating this month.
            4. The event didn't start this month and doesn't end repeating
            this month.
        """"""
    self.day = self.event.l_start_date.day
    self.count_first = False
    repeats = {'WEEKLY': 7, 'BIWEEKLY': 14}
    if '<MASK>':
        for repeat, num in repeats.items():
            self.num = num
            if '<MASK>':
                self.repeat()
                if '<MASK>':
                    self.repeat_chunk(diff=self.event.start_end_diff)",False,"['self.event.starts_same_year_month_as(self.year, self.month)', 'self.event.repeats(repeat)', 'self.event.is_chunk()']",___________________,0.0
"def _equal_values(self, val1, val2):
    """"""Checks if the parameter considers two values as equal.

        This is important for the trajectory in case of merging. In case you want to delete
        duplicate parameter points, the trajectory needs to know when two parameters
        are equal. Since equality is not always implemented by values handled by
        parameters in the same way, the parameters need to judge whether their values are equal.

        The straightforward example here is a numpy array.
        Checking for equality of two numpy arrays yields
        a third numpy array containing truth values of a piecewise comparison.
        Accordingly, the parameter could judge two numpy arrays equal if ALL of the numpy
        array elements are equal.

        In this BaseParameter class values are considered to be equal if they obey
        the function :func:`~pypet.utils.comparisons.nested_equal`.
        You might consider implementing a different equality comparison in your subclass.

        :raises: TypeError: If both values are not supported by the parameter.

        """"""
    if '<MASK>':
        return False
    if '<MASK>':
        raise TypeError('I do not support the types of both inputs (`%s` and `%s`), therefore I cannot judge whether the two are equal.' % (str(type(val1)), str(type(val2))))
    if '<MASK>':
        return False
    return comparisons.nested_equal(val1, val2)",False,"['self.f_supports(val1) != self.f_supports(val2)', 'not self.f_supports(val1) and (not self.f_supports(val2))', 'not self._values_of_same_type(val1, val2)']","_,,,,,,,,,,,,,,,,,,",0.0
"def set_options(self, options):
    """"""
        Add options. Options set before are not cleared!
        This method should be used with the :const:`OP_*` constants.

        :param options: The options to add.
        :return: The new option bitmask.
        """"""
    if '<MASK>':
        raise TypeError('options must be an integer')
    return _lib.SSL_CTX_set_options(self._context, options)",False,"['not isinstance(options, integer_types)']","__))
        :_))
        :_)))
       ",0.0
"def find_asm_blocks(asm_lines):
    """"""Find blocks probably corresponding to loops in assembly.""""""
    blocks = []
    last_labels = OrderedDict()
    packed_ctr = 0
    avx_ctr = 0
    xmm_references = []
    ymm_references = []
    zmm_references = []
    gp_references = []
    mem_references = []
    increments = {}
    for i, line in enumerate(asm_lines):
        zmm_references += re.findall('%zmm[0-9]+', line)
        ymm_references += re.findall('%ymm[0-9]+', line)
        xmm_references += re.findall('%xmm[0-9]+', line)
        gp_references += re.findall('%r[a-z0-9]+', line)
        if '<MASK>':
            m = re.search('(?P<off>[-]?\\d*)\\(%(?P<basep>\\w+)(,%(?P<idx>\\w+))?(?:,(?P<scale>\\d))?\\)(?P<eol>$)?', line)
            mem_references.append((int(m.group('off')) if m.group('off') else 0, m.group('basep'), m.group('idx'), int(m.group('scale')) if m.group('scale') else 1, 'load' if m.group('eol') is None else 'store'))
        if '<MASK>':
            if '<MASK>':
                avx_ctr += 1
            packed_ctr += 1
        elif '<MASK>':
            last_labels[line[0:line.find(':')]] = i
            packed_ctr = 0
            avx_ctr = 0
            xmm_references = []
            ymm_references = []
            zmm_references = []
            gp_references = []
            mem_references = []
            increments = {}
        elif '<MASK>':
            reg_start = line.find('%') + 1
            increments[line[reg_start:]] = 1
        elif '<MASK>':
            const_start = line.find('$') + 1
            const_end = line[const_start + 1:].find(',') + const_start + 1
            reg_start = line.find('%') + 1
            increments[line[reg_start:]] = int(line[const_start:const_end])
        elif '<MASK>':
            reg_start = line.find('%') + 1
            increments[line[reg_start:]] = -1
        elif '<MASK>':
            const_start = line.find('$') + 1
            const_end = line[const_start + 1:].find(',') + const_start + 1
            reg_start = line.find('%') + 1
            increments[line[reg_start:]] = -int(line[const_start:const_end])
        elif '<MASK>':
            last_label = None
            last_label_line = -1
            for label_name, label_line in last_labels.items():
                if '<MASK>':
                    last_label = label_name
                    last_label_line = label_line
            labels = list(last_labels.keys())
            if '<MASK>':
                pointer_increment = None
                possible_idx_regs = None
                if '<MASK>':
                    store_references = [mref for mref in mem_references if mref[4] == 'store']
                    refs = store_references or mem_references
                    possible_idx_regs = list(set(increments.keys()).intersection(set([r[1] for r in refs if r[1] is not None] + [r[2] for r in refs if r[2] is not None])))
                    for mref in refs:
                        for reg in list(possible_idx_regs):
                            if '<MASK>':
                                if '<MASK>':
                                    possible_idx_regs.remove(reg)
                    idx_reg = None
                    if '<MASK>':
                        idx_reg = possible_idx_regs[0]
                    elif '<MASK>':
                        idx_reg = possible_idx_regs[0]
                    if '<MASK>':
                        mem_scales = [mref[3] for mref in refs if idx_reg == mref[2] or idx_reg == mref[1]]
                        if '<MASK>':
                            try:
                                pointer_increment = mem_scales[0] * increments[idx_reg]
                            except:
                                print('labels', pformat(labels[labels.index(last_label):]))
                                print('lines', pformat(asm_lines[last_label_line:i + 1]))
                                print('increments', increments)
                                print('mem_references', pformat(mem_references))
                                print('idx_reg', idx_reg)
                                print('mem_scales', mem_scales)
                                raise
                blocks.append({'first_line': last_label_line, 'last_line': i, 'ops': i - last_label_line, 'labels': labels[labels.index(last_label):], 'packed_instr': packed_ctr, 'avx_instr': avx_ctr, 'XMM': (len(xmm_references), len(set(xmm_references))), 'YMM': (len(ymm_references), len(set(ymm_references))), 'ZMM': (len(zmm_references), len(set(zmm_references))), 'GP': (len(gp_references), len(set(gp_references))), 'regs': (len(xmm_references) + len(ymm_references) + len(zmm_references) + len(gp_references), len(set(xmm_references)) + len(set(ymm_references)) + len(set(zmm_references)) + len(set(gp_references))), 'pointer_increment': pointer_increment, 'lines': asm_lines[last_label_line:i + 1], 'possible_idx_regs': possible_idx_regs, 'mem_references': mem_references, 'increments': increments})
            packed_ctr = 0
            avx_ctr = 0
            xmm_references = []
            ymm_references = []
            zmm_references = []
            gp_references = []
            mem_references = []
            increments = {}
            last_labels = OrderedDict()
    return list(enumerate(blocks))",False,"[""re.search('\\\\d*\\\\(%\\\\w+(,%\\\\w+)?(,\\\\d)?\\\\)', line)"", ""re.match('^[v]?(mul|add|sub|div|fmadd(132|213|231)?)[h]?p[ds]', line)"", ""line.startswith('v')"", ""re.match('^\\\\S+:', line)"", ""re.match('^inc[bwlq]?\\\\s+%[a-z0-9]+', line)"", ""re.match('^add[bwlq]?\\\\s+\\\\$[0-9]+,\\\\s*%[a-z0-9]+', line)"", ""re.match('^dec[bwlq]?', line)"", ""re.match('^sub[bwlq]?\\\\s+\\\\$[0-9]+,', line)"", ""last_labels and re.match('^j[a-z]+\\\\s+\\\\S+\\\\s*', line)"", 'last_label', ""re.match('^j[a-z]+\\\\s+' + re.escape(label_name) + '\\\\s*', line)"", 'mem_references', 'len(possible_idx_regs) == 1', 'idx_reg', 'possible_idx_regs and itemsEqual([increments[pidxreg] for pidxreg in possible_idx_regs])', 'itemsEqual(mem_scales)', 'None not in mref[1:3]', 'not (reg == mref[1] or reg == mref[2])']",___________________,0.0
"def server_random(self):
    """"""
        Retrieve the random value used with the server hello message.

        :return: A string representing the state
        """"""
    session = _lib.SSL_get_session(self._ssl)
    if '<MASK>':
        return None
    length = _lib.SSL_get_server_random(self._ssl, _ffi.NULL, 0)
    assert length > 0
    outp = _no_zero_allocator('unsigned char[]', length)
    _lib.SSL_get_server_random(self._ssl, outp, length)
    return _ffi.buffer(outp, length)[:]",False,['session == _ffi.NULL'],___________________,0.0
"def update_course_enrollment(self, email, course_url, purchase_incomplete, mode, unit_cost=None, course_id=None, currency=None, message_id=None, site_code=None, sku=None):
    """"""Adds/updates Sailthru when a user adds to cart/purchases/upgrades a course

     Args:
        email(str): The user's email address
        course_url(str): Course home page url
        purchase_incomplete(boolean): True if adding to cart
        mode(string): enroll mode (audit, verification, ...)
        unit_cost(decimal): cost if purchase event
        course_id(CourseKey): course id
        currency(str): currency if purchase event - currently ignored since Sailthru only supports USD
        message_id(str): value from Sailthru marketing campaign cookie
        site_code(str): site code

    Returns:
        None
    """"""
    config = get_sailthru_configuration(site_code)
    try:
        sailthru_client = get_sailthru_client(site_code)
    except SailthruError:
        return
    new_enroll = False
    send_template = None
    if '<MASK>':
        if '<MASK>':
            send_template = config.get('SAILTHRU_UPGRADE_TEMPLATE')
        elif '<MASK>':
            new_enroll = True
            send_template = config.get('SAILTHRU_ENROLL_TEMPLATE')
        else:
            new_enroll = True
            send_template = config.get('SAILTHRU_PURCHASE_TEMPLATE')
    cost_in_cents = int(unit_cost * 100)
    if '<MASK>':
        if '<MASK>':
            schedule_retry(self, config)
    course_data = _get_course_content(course_id, course_url, sailthru_client, site_code, config)
    item = _build_purchase_item(course_id, course_url, cost_in_cents, mode, course_data, sku)
    options = {}
    if '<MASK>':
        options['reminder_template'] = config.get('SAILTHRU_ABANDONED_CART_TEMPLATE')
        options['reminder_time'] = '+{} minutes'.format(config.get('SAILTHRU_ABANDONED_CART_DELAY'))
    if '<MASK>':
        options['send_template'] = send_template
    if '<MASK>':
        schedule_retry(self, config)",False,"['not purchase_incomplete', 'new_enroll', ""purchase_incomplete and config.get('SAILTHRU_ABANDONED_CART_TEMPLATE')"", 'send_template', 'not _record_purchase(sailthru_client, email, item, purchase_incomplete, message_id, options)', ""mode == 'verified'"", 'not _update_unenrolled_list(sailthru_client, email, course_url, False)', ""mode == 'audit' or mode == 'honor'""]",___________________,0.0
"def _detect_fork(self):
    """"""Detects if lock client was forked.

        Forking is detected by comparing the PID of the current
        process with the stored PID.

        """"""
    if '<MASK>':
        self._pid = os.getpid()
    if '<MASK>':
        current_pid = os.getpid()
        if '<MASK>':
            self._logger.debug('Fork detected: My pid `%s` != os pid `%s`. Restarting connection.' % (str(self._pid), str(current_pid)))
            self._context = None
            self._pid = current_pid",False,"['self._pid is None', 'self._context is not None', 'current_pid != self._pid']","_):
        """"""
        """"""
        """"""
        """"""
        """"""
       ",0.0
"def read_hdf5(self, filename, f_start=None, f_stop=None, t_start=None, t_stop=None, load_data=True):
    """""" Populate Filterbank instance with data from HDF5 file

        Note:
            This is to be deprecated in future, please use Waterfall() to open files.
        """"""
    print('Warning: this function will be deprecated in the future. Please use Waterfall to open HDF5 files.')
    self.header = {}
    self.filename = filename
    self.h5 = h5py.File(filename)
    for key, val in self.h5[b'data'].attrs.items():
        if '<MASK>':
            key = bytes(key, 'ascii')
        if '<MASK>':
            self.header[key] = Angle(val, unit='hr')
        elif '<MASK>':
            self.header[key] = Angle(val, unit='deg')
        else:
            self.header[key] = val
    self.n_ints_in_file = self.h5[b'data'].shape[0]
    i_start, i_stop, chan_start_idx, chan_stop_idx = self._setup_freqs(f_start=f_start, f_stop=f_stop)
    ii_start, ii_stop, n_ints = self._setup_time_axis(t_start=t_start, t_stop=t_stop)
    if '<MASK>':
        self.data = self.h5[b'data'][ii_start:ii_stop, :, chan_start_idx:chan_stop_idx]
        self.file_size_bytes = os.path.getsize(self.filename)
    else:
        print('Skipping data load...')
        self.data = np.array([0])
        self.n_ints_in_file = 0
        self.file_size_bytes = os.path.getsize(self.filename)",False,"['load_data', 'six.PY3', ""key == b'src_raj'"", ""key == b'src_dej'""]",___________________,0.0
"def describe(self, verbose=True):
    """"""Return a textual description of the segment.""""""
    body = titlecase(target_names.get(self.body, 'Unknown body'))
    text = '{0.start_jd:.2f}..{0.end_jd:.2f} frame={0.frame}  {1} ({0.body})'.format(self, body)
    if '<MASK>':
        text += '\n  data_type={0.data_type} source={1}'.format(self, self.source.decode('ascii'))
    return text",False,['verbose'],"_,,,,,,,,,,,,,,,,,,",0.0
"def _log(self, level, message, context=None):
    """"""
        Sends a message to the instance's logger, if configured.
        """"""
    if '<MASK>':
        self._logger.log(level, message, extra={'context': context or {}})",False,['self._logger'],"_,,,,,,,,,,,,,,,,,,",0.0
"def gabsFromSpt(spt):
    """"""
  Obtain M_G (absolute magnitude in G-band) for the input spectral type.

  Parameters
  ----------
  
  spt - String representing the spectral type of the star.

  Returns
  -------
  
  The value of M_G.
  """"""
    if '<MASK>':
        return vabsFromSpt(spt) + gminvFromVmini(vminiFromSpt(spt))
    else:
        message = 'Unknown spectral type. Allowed values are: '
        for key in _sptToVminiVabsDictionary.keys():
            message += key + ' '
        raise Exception(message)",False,['spt in _sptToVminiVabsDictionary'],(s__))))))))))))))),0.0
"def load(self, data, session=None, instance=None, transient=False, *args, **kwargs):
    """"""Deserialize data to internal representation.

        :param session: Optional SQLAlchemy session.
        :param instance: Optional existing instance to modify.
        :param transient: Optional switch to allow transient instantiation.
        """"""
    self._session = session or self._session
    self._transient = transient or self._transient
    if '<MASK>':
        raise ValueError('Deserialization requires a session')
    self.instance = instance or self.instance
    try:
        return super(ModelSchema, self).load(data, *args, **kwargs)
    finally:
        self.instance = None",False,['not (self.transient or self.session)'],",,,,,,,,,,,,,,,,,,,",0.0
"def translate(self, message):
    """"""
        Translate machine identifiers into human-readable
        """"""
    try:
        user_id = message.pop('user')
        user = self.slack.user_from_id(user_id)
        message[u'user'] = user['name']
    except (KeyError, IndexError, ValueError):
        pass
    try:
        if '<MASK>':
            channel_id = message.pop('channel')
        else:
            channel_id = message.pop('channel')['id']
            self.slack.reload_channels()
        channel = self.slack.channel_from_id(channel_id)
        message[u'channel'] = channel['name']
    except (KeyError, IndexError, ValueError):
        pass
    return message",False,"[""type(message['channel']) == str""]",__id_id_id_id_id_id_id_id_id,0.0
"def user(self, login):
    """"""Get the user information and update the user cache""""""
    user = None
    if '<MASK>':
        return self._users[login]
    url_user = urijoin(self.base_url, 'users', login)
    logging.info('Getting info for %s' % url_user)
    r = self.fetch(url_user)
    user = r.text
    self._users[login] = user
    return user",False,['login in self._users'],___________________,0.0
"def _get_front_ids_one_at_a_time(onset_fronts):
    """"""
    Yields one onset front ID at a time until they are gone. All the onset fronts from a
    frequency channel are yielded, then all of the next channel's, etc., though one at a time.
    """"""
    yielded_so_far = set()
    for row in onset_fronts:
        for id in row:
            if '<MASK>':
                yield id
                yielded_so_far.add(id)",False,['id != 0 and id not in yielded_so_far'],___________________,0.0
"def _merge_slowly(self, other_trajectory, rename_dict):
    """"""Merges trajectories by loading iteratively items of the other trajectory and
        store it into the current trajectory.

        :param rename_dict:

            Dictionary containing mappings from the old result names in the `other_trajectory`
            to the new names in the current trajectory.

        """"""
    for other_key in rename_dict:
        new_key = rename_dict[other_key]
        other_instance = other_trajectory.f_get(other_key)
        if '<MASK>':
            with self._nn_interface._disable_logging:
                other_trajectory.f_load_item(other_instance)
        if '<MASK>':
            class_name = other_instance.f_get_class_name()
            class_ = self._create_class(class_name)
            my_instance = self.f_add_leaf(class_, new_key)
        else:
            my_instance = self.f_get(new_key, shortcuts=False)
        if '<MASK>':
            raise RuntimeError('Something is wrong! Your item `%s` should be empty.' % new_key)
        load_dict = other_instance._store()
        my_instance._load(load_dict)
        my_instance.f_set_annotations(**other_instance.v_annotations.f_to_dict(copy=False))
        my_instance.v_comment = other_instance.v_comment
        self.f_store_item(my_instance)
        if '<MASK>':
            other_instance.f_unlock()
            my_instance.f_unlock()
        other_instance.f_empty()
        my_instance.f_empty()",False,"['other_instance.f_is_empty()', 'not self.f_contains(new_key)', 'not my_instance.f_is_empty()', 'other_instance.v_is_parameter']",___________________,0.0
"def average_within_regions(dataset, regions, masker=None, threshold=None, remove_zero=True):
    """""" Aggregates over all voxels within each ROI in the input image.

    Takes a Dataset and a Nifti image that defines distinct regions, and
    returns a numpy matrix of  ROIs x mappables, where the value at each
    ROI is the proportion of active voxels in that ROI. Each distinct ROI
    must have a unique value in the image; non-contiguous voxels with the
    same value will be assigned to the same ROI.

    Args:
        dataset: Either a Dataset instance from which image data are
            extracted, or a Numpy array containing image data to use. If
            the latter, the array contains voxels in rows and
            features/studies in columns. The number of voxels must be equal
            to the length of the vectorized image mask in the regions
            image.
        regions: An image defining the boundaries of the regions to use.
            Can be one of:
            1) A string name of the NIFTI or Analyze-format image
            2) A NiBabel SpatialImage
            3) A list of NiBabel images
            4) A 1D numpy array of the same length as the mask vector in
                the Dataset's current Masker.
        masker: Optional masker used to load image if regions is not a
            numpy array. Must be passed if dataset is a numpy array.
        threshold: An optional float in the range of 0 - 1 or integer. If
            passed, the array will be binarized, with ROI values above the
            threshold assigned to True and values below the threshold
            assigned to False. (E.g., if threshold = 0.05, only ROIs in
            which more than 5% of voxels are active will be considered
            active.) If threshold is integer, studies will only be
            considered active if they activate more than that number of
            voxels in the ROI.
        remove_zero: An optional boolean; when True, assume that voxels
            with value of 0 should not be considered as a separate ROI, and
            will be ignored.

    Returns:
        A 2D numpy array with ROIs in rows and mappables in columns.
    """"""
    if '<MASK>':
        masker = masker
    elif '<MASK>':
        masker = dataset.masker
    elif '<MASK>':
        raise ValueError('If dataset is a numpy array and regions is not a numpy array, a masker must be provided.')
    if '<MASK>':
        regions = masker.mask(regions)
    if '<MASK>':
        dataset = dataset.get_image_data(dense=False)
    if '<MASK>':
        m = regions
        for i in range(regions.shape[1]):
            _nz = np.nonzero(m[:, i])[0]
            if '<MASK>':
                m[_nz, i] = 1.0
            else:
                m[_nz, i] = 1.0 / np.count_nonzero(m[:, i])
    else:
        labels = np.unique(regions)
        if '<MASK>':
            labels = labels[np.nonzero(labels)]
        n_regions = labels.size
        m = np.zeros((regions.size, n_regions))
        for i in range(n_regions):
            if '<MASK>':
                m[regions == labels[i], i] = 1.0
            else:
                m[regions == labels[i], i] = 1.0 / np.sum(regions == labels[i])
    result = dataset.T.dot(m).T
    if '<MASK>':
        result[result < threshold] = 0.0
        result = result.astype(bool)
    return result",False,"['masker is not None', ""not type(regions).__module__.startswith('numpy')"", 'isinstance(dataset, Dataset)', 'regions.ndim == 2', 'threshold is not None', 'isinstance(dataset, Dataset)', 'remove_zero', ""not type(regions).__module__.startswith('numpy')"", 'isinstance(threshold, int)', 'isinstance(threshold, int)']",___________________,0.0
"def fix_path(self, splited_path=None):
    """"""
        Fix the path of the given path.

        :param splited_path: A list to convert to the right path.
        :type splited_path: list

        :return: The fixed path.
        :rtype: str
        """"""
    if '<MASK>':
        split_path = []
        if '<MASK>':
            if '<MASK>':
                split_path = self.directory.split('/')
            elif '<MASK>':
                split_path = self.directory.split('\\')
            else:
                split_path = [self.directory]
            return self.fix_path(splited_path=[x for x in split_path if x])
        return self.directory
    return directory_separator.join(splited_path) + directory_separator",False,"['not splited_path', 'self.directory', ""'/' in self.directory"", ""'\\\\' in self.directory""]",_path_path_path_path_path_path_path_path_path_,0.0
"def _is_to_ignore(cls, line):
    """"""
        Check if we have to ignore the given line.

        :param line: The line from the file.
        :type line: str
        """"""
    to_ignore = ['(^!|^@@|^\\/|^\\[|^\\.|^-|^_|^\\?|^&)']
    for element in to_ignore:
        if '<MASK>':
            return True
    return False",False,"['Regex(line, element, return_data=False).match()']",___________________,0.0
"def parse_format_method_string(format_string: str) -> Tuple[List[Tuple[str, List[Tuple[bool, str]]]], int, int]:
    """"""
    Parses a PEP 3101 format string, returning a tuple of
    (keyword_arguments, implicit_pos_args_cnt, explicit_pos_args),
    where keyword_arguments is the set of mapping keys in the format string, implicit_pos_args_cnt
    is the number of arguments required by the format string and
    explicit_pos_args is the number of arguments passed with the position.
    """"""
    keyword_arguments = []
    implicit_pos_args_cnt = 0
    explicit_pos_args = set()
    for name in collect_string_fields(format_string):
        if '<MASK>':
            explicit_pos_args.add(str(name))
        elif '<MASK>':
            keyname, fielditerator = split_format_field_names(name)
            if '<MASK>':
                explicit_pos_args.add(str(keyname))
                keyname = int(keyname)
            try:
                keyword_arguments.append((keyname, list(fielditerator)))
            except ValueError:
                raise IncompleteFormatString()
        else:
            implicit_pos_args_cnt += 1
    return (keyword_arguments, implicit_pos_args_cnt, len(explicit_pos_args))",False,"['name and str(name).isdigit()', 'name', 'isinstance(keyname, numbers.Number)']",___________________,0.0
"def merge_left(field, local_task, remote_issue, hamming=False):
    """""" Merge array field from the remote_issue into local_task

    * Local 'left' entries are preserved without modification
    * Remote 'left' are appended to task if not present in local.

    :param `field`: Task field to merge.
    :param `local_task`: `taskw.task.Task` object into which to merge
        remote changes.
    :param `remote_issue`: `dict` instance from which to merge into
        local task.
    :param `hamming`: (default `False`) If `True`, compare entries by
        truncating to maximum length, and comparing hamming distances.
        Useful generally only for annotations.

    """"""
    local_field = local_task.get(field, [])
    remote_field = remote_issue.get(field, [])
    if '<MASK>':
        local_task[field] = []
    new_count = 0
    for remote in remote_field:
        for local in local_field:
            if '<MASK>':
                break
        else:
            log.debug('%s not found in %r' % (remote, local_field))
            local_task[field].append(remote)
            new_count += 1
    if '<MASK>':
        log.debug('Added %s new values to %s (total: %s)' % (new_count, field, len(local_task[field])))",False,"['field not in local_task', 'new_count > 0', 'hamming and get_annotation_hamming_distance(remote, local) == 0 or remote == local']",___________________,0.0
"def _translate_shortcut(self, name):
    """"""Maps a given shortcut to corresponding name

        * 'run_X' or 'r_X' to 'run_XXXXXXXXX'

        * 'crun' to the current run name in case of a
          single run instance if trajectory is used via `v_crun`

        * 'par' 'parameters'

        * 'dpar' to 'derived_parameters'

        * 'res' to 'results'

        * 'conf' to 'config'

        :return: True or False and the mapped name.

        """"""
    if '<MASK>':
        return (True, self._root_instance.f_wildcard('$', name))
    if '<MASK>':
        split_name = name.split('_')
        if '<MASK>':
            index = split_name[1]
            if '<MASK>':
                return (True, self._root_instance.f_wildcard('$', int(index)))
            elif '<MASK>':
                return (True, self._root_instance.f_wildcard('$', -1))
    if '<MASK>':
        split_name = name.split('_')
        if '<MASK>':
            index = split_name[1]
            if '<MASK>':
                return (True, self._root_instance.f_wildcard('$set', int(index)))
            elif '<MASK>':
                return (True, self._root_instance.f_wildcard('$set', -1))
    if '<MASK>':
        if '<MASK>':
            return (True, 'parameters')
        elif '<MASK>':
            return (True, 'derived_parameters')
        elif '<MASK>':
            return (True, 'results')
        elif '<MASK>':
            return (True, 'config')
        else:
            raise RuntimeError('You shall not pass!')
    return (False, name)",False,"['isinstance(name, int)', ""name.startswith('run_') or name.startswith('r_')"", ""name.startswith('runtoset_') or name.startswith('rts_')"", 'name in SHORTCUT_SET', 'len(split_name) == 2', 'len(split_name) == 2', ""name == 'par'"", 'index.isdigit()', 'index.isdigit()', ""name == 'dpar'"", ""index == 'A'"", ""index == 'A'"", ""name == 'res'"", ""name == 'conf'""]",___________________,0.0
"def select_next(iterval):
    """""" select the next best data sample using robust map
            or simply the max iterval ... """"""
    if '<MASK>':
        k = np.argsort(iterval)[::-1]
        d_sub = self.data[:, k[:self._robust_nselect]]
        self.sub.extend(k[:self._robust_nselect])
        kmeans_mdl = Kmeans(d_sub, num_bases=self._robust_cluster)
        kmeans_mdl.factorize(niter=10)
        h = np.histogram(kmeans_mdl.assigned, range(self._robust_cluster + 1))[0]
        largest_cluster = np.argmax(h)
        sel = pdist(kmeans_mdl.W[:, largest_cluster:largest_cluster + 1], d_sub)
        sel = k[np.argmin(sel)]
    else:
        sel = np.argmax(iterval)
    return sel",False,['self._robust_map'],___________________,0.0
"def sign(self, data, unique_identifier=None, cryptographic_parameters=None, credential=None):
    """"""
        Sign specified data using a specified signing key.

        Args:
            data (bytes): Data to be signed. Required.
            unique_identifier (string): The unique ID of the signing
                key to be used. Optional, defaults to None.
            cryptographic_parameters (CryptographicParameters): A structure
                containing various cryptographic settings to be used for
                creating the signature. Optional, defaults to None.
            credential (Credential): A credential object containing a set of
                authorization parameters for the operation. Optional, defaults
                to None.
        Returns:
            dict: The results of the sign operation, containing the
                following key/value pairs:

            Key                  | Value
            ---------------------|-----------------------------------------
            'unique_identifier'  | (string) The unique ID of the signing
                                 | key used to create the signature
            'signature'          | (bytes) The bytes of the signature
            'result_status'      | (ResultStatus) An enumeration indicating
                                 | the status of the operation result
            'result_reason'      | (ResultReason) An enumeration providing
                                 | context for the result status.
            'result_message'     | (string) A message providing additional
                                 | context for the operation result.
        """"""
    operation = Operation(OperationEnum.SIGN)
    request_payload = payloads.SignRequestPayload(unique_identifier=unique_identifier, cryptographic_parameters=cryptographic_parameters, data=data)
    batch_item = messages.RequestBatchItem(operation=operation, request_payload=request_payload)
    request = self._build_request_message(credential, [batch_item])
    response = self._send_and_receive_message(request)
    batch_item = response.batch_items[0]
    payload = batch_item.response_payload
    result = {}
    if '<MASK>':
        result['unique_identifier'] = payload.unique_identifier
        result['signature'] = payload.signature_data
    result['result_status'] = batch_item.result_status.value
    try:
        result['result_reason'] = batch_item.result_reason.value
    except Exception:
        result['result_reason'] = batch_item.result_reason
    try:
        result['result_message'] = batch_item.result_message.value
    except Exception:
        result['result_message'] = batch_item.result_message
    return result",False,['payload'],___________________,0.0
"def get_call_function_name(frame):
    """"""If f_back is looking at a call function, return
    the name for it. Otherwise return None""""""
    f_back = frame.f_back
    if '<MASK>':
        return None
    if '<MASK>':
        return None
    co = f_back.f_code
    code = co.co_code
    linestarts = dict(dis.findlinestarts(co))
    offset = f_back.f_lasti
    while offset >= 0:
        if '<MASK>':
            op = code[offset]
            offset += 1
            arg = code[offset]
            extended_arg = 0
            while True:
                if '<MASK>':
                    if '<MASK>':
                        extended_arg += arg << 8
                        continue
                    arg = code[offset] + extended_arg
                else:
                    if '<MASK>':
                        extended_arg += arg << 256
                        continue
                    arg = code[offset] + code[offset + 1] * 256 + extended_arg
                break
            return co.co_names[arg]
        offset -= 1
        pass
    return None",False,"['not f_back', ""'CALL_FUNCTION' != Mbytecode.op_at_frame(f_back)"", 'offset in linestarts', 'PYTHON_VERSION >= 3.6', 'op == opc.EXTENDED_ARG', 'op == opc.EXTENDED_ARG']",___________________,0.0
"def invert(self, copy=False):
    """"""
        Inverts the striplog, changing its order and the order of its contents.

        Operates in place by default.

        Args:
            copy (bool): Whether to operate in place or make a copy.

        Returns:
            None if operating in-place, or an inverted copy of the striplog
                if not.
        """"""
    if '<MASK>':
        return Striplog([i.invert(copy=True) for i in self])
    else:
        for i in self:
            i.invert()
        self.__sort()
        o = self.order
        self.order = {'depth': 'elevation', 'elevation': 'depth'}[o]
        return",False,['copy'],___________________,0.0
"def load_tmp_dh(self, dhfile):
    """"""
        Load parameters for Ephemeral Diffie-Hellman

        :param dhfile: The file to load EDH parameters from (``bytes`` or
            ``unicode``).

        :return: None
        """"""
    dhfile = _path_string(dhfile)
    bio = _lib.BIO_new_file(dhfile, b'r')
    if '<MASK>':
        _raise_current_error()
    bio = _ffi.gc(bio, _lib.BIO_free)
    dh = _lib.PEM_read_bio_DHparams(bio, _ffi.NULL, _ffi.NULL, _ffi.NULL)
    dh = _ffi.gc(dh, _lib.DH_free)
    _lib.SSL_CTX_set_tmp_dh(self._context, dh)",False,['bio == _ffi.NULL'],___________________,0.0
"def search(self, query=None, args=None):
    """"""query a GitLab artifacts folder for a list of images. 
     If query is None, collections are listed. 
    """"""
    if '<MASK>':
        bot.exit('You must include a collection query, <collection>/<repo>')
    return self._search_all(query)",False,['query is None'],"(self,,,,,,,,,,,,,,,,,",0.0
"def __resolve_namespaced_symbol(ctx: ParserContext, form: sym.Symbol) -> Union[MaybeClass, MaybeHostForm, VarRef]:
    """"""Resolve a namespaced symbol into a Python name or Basilisp Var.""""""
    assert form.ns is not None
    if '<MASK>':
        v = ctx.current_ns.find(sym.symbol(form.name))
        if '<MASK>':
            return VarRef(form=form, var=v, env=ctx.get_node_env())
    elif '<MASK>':
        class_ = munge(form.name, allow_builtins=True)
        target = getattr(builtins, class_, None)
        if '<MASK>':
            raise ParserException(f""cannot resolve builtin function '{class_}'"", form=form)
        return MaybeClass(form=form, class_=class_, target=target, env=ctx.get_node_env())
    if '<MASK>':
        raise ParserException(""symbol names may not contain the '.' operator"", form=form)
    ns_sym = sym.symbol(form.ns)
    if '<MASK>':
        v = Var.find(form)
        if '<MASK>':
            return VarRef(form=form, var=v, env=ctx.get_node_env())
        if '<MASK>':
            ns = ctx.current_ns.import_aliases[ns_sym]
            assert ns is not None
            ns_name = ns.name
        else:
            ns_name = ns_sym.name
        safe_module_name = munge(ns_name)
        assert safe_module_name in sys.modules, f""Module '{safe_module_name}' is not imported""
        ns_module = sys.modules[safe_module_name]
        safe_name = munge(form.name)
        if '<MASK>':
            return MaybeHostForm(form=form, class_=munge(ns_sym.name), field=safe_name, target=vars(ns_module)[safe_name], env=ctx.get_node_env())
        safe_name = munge(form.name, allow_builtins=True)
        if '<MASK>':
            raise ParserException(""can't identify aliased form"", form=form)
        return MaybeHostForm(form=form, class_=munge(ns_sym.name), field=safe_name, target=vars(ns_module)[safe_name], env=ctx.get_node_env())
    elif '<MASK>':
        aliased_ns: runtime.Namespace = ctx.current_ns.aliases[ns_sym]
        v = Var.find(sym.symbol(form.name, ns=aliased_ns.name))
        if '<MASK>':
            raise ParserException(f""unable to resolve symbol '{sym.symbol(form.name, ns_sym.name)}' in this context"", form=form)
        return VarRef(form=form, var=v, env=ctx.get_node_env())
    else:
        raise ParserException(f""unable to resolve symbol '{form}' in this context"", form=form)",False,"['form.ns == ctx.current_ns.name', ""'.' in form.name"", 'ns_sym in ctx.current_ns.imports or ns_sym in ctx.current_ns.import_aliases', 'v is not None', 'form.ns == _BUILTINS_NS', 'v is not None', 'ns_sym in ctx.current_ns.import_aliases', 'safe_name in vars(ns_module)', 'safe_name not in vars(ns_module)', 'ns_sym in ctx.current_ns.aliases', 'target is None', 'v is None']","___________)
        if if if if if",0.0
"def Value(cls, val, ctx: SerializerCtx):
    """"""
        :param dst: is signal connected with value
        :param val: value object, can be instance of Signal or Value
        """"""
    t = val._dtype
    if '<MASK>':
        return cls.SignalItem(val, ctx)
    c = cls.Value_try_extract_as_const(val, ctx)
    if '<MASK>':
        return c
    if '<MASK>':
        return cls.Slice_valAsHdl(t, val, ctx)
    elif '<MASK>':
        return cls.HArrayValAsHdl(t, val, ctx)
    elif '<MASK>':
        return cls.Bits_valAsHdl(t, val, ctx)
    elif '<MASK>':
        return cls.Bool_valAsHdl(t, val, ctx)
    elif '<MASK>':
        return cls.HEnumValAsHdl(t, val, ctx)
    elif '<MASK>':
        return cls.Integer_valAsHdl(t, val, ctx)
    elif '<MASK>':
        return cls.String_valAsHdl(t, val, ctx)
    else:
        raise SerializerException('can not resolve value serialization for %r' % val)",False,"['isinstance(val, RtlSignalBase)', 'c', 'isinstance(t, Slice)', 'isinstance(t, HArray)', 'isinstance(t, Bits)', 'isinstance(t, HBool)', 'isinstance(t, HEnum)', 'isinstance(t, Integer)', 'isinstance(t, String)']",___________________,0.0
"def reproduce(past_analysis, plotting=False, data_folder=None, srm_table=None, custom_stat_functions=None):
    """"""
    Reproduce a previous analysis exported with :func:`latools.analyse.minimal_export`

    For normal use, supplying `log_file` and specifying a plotting option should be
    enough to reproduce an analysis. All requisites (raw data, SRM table and any
    custom stat functions) will then be imported from the minimal_export folder.

    You may also specify your own raw_data, srm_table and custom_stat_functions,
    if you wish.

    Parameters
    ----------
    log_file : str
        The path to the log file produced by :func:`~latools.analyse.minimal_export`.
    plotting : bool
        Whether or not to output plots.
    data_folder : str
        Optional. Specify a different data folder. Data folder
        should normally be in the same folder as the log file.
    srm_table : str
        Optional. Specify a different SRM table. SRM table
        should normally be in the same folder as the log file.
    custom_stat_functions : str
        Optional. Specify a python file containing custom
        stat functions for use by reproduce. Any custom
        stat functions should normally be included in the
        same folder as the log file.
    """"""
    if '<MASK>':
        dirpath = utils.extract_zipdir(past_analysis)
        logpath = os.path.join(dirpath, 'analysis.lalog')
    elif '<MASK>':
        if '<MASK>':
            logpath = os.path.join(past_analysis, 'analysis.lalog')
    elif '<MASK>':
        logpath = past_analysis
    else:
        raise ValueError('\n\n{} is not a valid input.\n\n' + 'Must be one of:\n' + '  - A .zip file exported by latools\n' + '  - An analysis.lalog file\n' + '  - A directory containing an analysis.lalog files\n')
    runargs, paths = logging.read_logfile(logpath)
    csfs = Bunch()
    if '<MASK>':
        with open(paths['custom_stat_functions'], 'r') as f:
            csf = f.read()
        fname = re.compile('def (.*)\\(.*')
        for c in csf.split('\n\n\n\n'):
            if '<MASK>':
                csfs[fname.match(c).groups()[0]] = c
    rep = analyse(*runargs[0][-1]['args'], **runargs[0][-1]['kwargs'])
    for fname, arg in runargs:
        if '<MASK>':
            if '<MASK>':
                getattr(rep, fname)(*arg['args'], **arg['kwargs'])
            elif '<MASK>':
                rep.sample_stats(*arg['args'], csf_dict=csfs, **arg['kwargs'])
            else:
                getattr(rep, fname)(*arg['args'], **arg['kwargs'])
    return rep",False,"[""'.zip' in past_analysis"", ""custom_stat_functions is None and 'custom_stat_functions' in paths.keys()"", 'os.path.isdir(past_analysis)', ""fname != '__init__'"", ""os.path.exists(os.path.join(past_analysis, 'analysis.lalog'))"", ""'analysis.lalog' in past_analysis"", 'fname.match(c)', ""'plot' in fname.lower() and plotting"", ""'sample_stats' in fname.lower()""]",___________________,0.0
"def validate_full_path(cls, full_path, **kwargs):
    """"""Helper method to parse a full or partial path and
        return a full path as well as a dict containing path parts.

        Uses the following rules when processing the path:

            * If no domain, uses the current user's account domain
            * If no vault, uses the current user's personal vault.
            * If no path, uses '/' (vault root)

        Returns a tuple containing:

            * The validated full_path
            * A dictionary with the components:
                * domain: the domain of the vault
                * vault: the name of the vault, without domain
                * vault_full_path: domain:vault
                * path: the object path within the vault
                * parent_path: the parent path to the object
                * filename: the object's filename (if any)
                * full_path: the validated full path

        The following components may be overridden using kwargs:

            * vault
            * path

        Object paths (also known as ""paths"") must begin with a forward slash.

        The following path formats are supported:

            domain:vault:/path -> object ""path"" in the root of ""domain:vault""
            domain:vault/path  -> object ""path"" in the root of ""domain:vault""
            vault:/path        -> object ""path"" in the root of ""vault""
            vault/path         -> object ""path"" in the root of ""vault""
            ~/path             -> object ""path"" in the root of personal vault
            vault/             -> root of ""vault""
            ~/                 -> root of your personal vault

        The following two formats are not supported:

            path               -> invalid/ambiguous path (exception)
            vault:path         -> invalid/ambiguous path (exception)
            vault:path/path    -> unsupported, interpreted as domain:vault/path

        """"""
    from solvebio.resource.vault import Vault
    _client = kwargs.pop('client', None) or cls._client or client
    if '<MASK>':
        raise Exception('Invalid path: ', 'Full path must be in one of the following formats: ""vault:/path"", ""domain:vault:/path"", or ""~/path""')
    input_vault = kwargs.get('vault') or full_path
    try:
        vault_full_path, path_dict = Vault.validate_full_path(input_vault, client=_client)
    except Exception as err:
        raise Exception('Could not determine vault from ""{0}"": {1}'.format(input_vault, err))
    if '<MASK>':
        full_path = '{0}:/{1}'.format(vault_full_path, kwargs['path'])
    match = cls.PATH_RE.match(full_path)
    if '<MASK>':
        object_path = match.groupdict()['path']
    else:
        raise Exception('Cannot find a valid object path in ""{0}"". Full path must be in one of the following formats: ""vault:/path"", ""domain:vault:/path"", or ""~/path""'.format(full_path))
    object_path = re.sub('//+', '/', object_path)
    if '<MASK>':
        object_path = object_path.rstrip('/')
    path_dict['path'] = object_path
    full_path = '{domain}:{vault}:{path}'.format(**path_dict)
    path_dict['full_path'] = full_path
    return (full_path, path_dict)",False,"['not full_path', ""kwargs.get('path')"", 'match', ""object_path != '/'""]",___________________,0.0
"def cancel(self, msg='', exc_type=CancelledError):
    """"""Cancels the TransferFuture

        :param msg: The message to attach to the cancellation
        :param exc_type: The type of exception to set for the cancellation
        """"""
    with self._lock:
        if '<MASK>':
            should_announce_done = False
            logger.debug('%s cancel(%s) called', self, msg)
            self._exception = exc_type(msg)
            if '<MASK>':
                should_announce_done = True
            self._status = 'cancelled'
            if '<MASK>':
                self.announce_done()",False,"['not self.done()', ""self._status == 'not-started'"", 'should_announce_done']","__,_,_,_,_,_,_,_,_,",0.0
"def get_int_noerr(self, arg):
    """"""Eval arg and it is an integer return the value. Otherwise
        return None""""""
    if '<MASK>':
        g = self.curframe.f_globals
        l = self.curframe.f_locals
    else:
        g = globals()
        l = locals()
        pass
    try:
        val = int(eval(arg, g, l))
    except (SyntaxError, NameError, ValueError, TypeError):
        return None
    return val",False,['self.curframe'],___________________,0.0
"def spl(self):
    """"""
        Sound Pressure Level - defined as 20 * log10(p/p0),
        where p is the RMS of the sound wave in Pascals and p0 is
        20 micro Pascals.

        Since we would need to know calibration information about the
        microphone used to record the sound in order to transform
        the PCM values of this audiosegment into Pascals, we can't really
        give an accurate SPL measurement.

        However, we can give a reasonable guess that can certainly be used
        to compare two sounds taken from the same microphone set up.

        Be wary about using this to compare sounds taken under different recording
        conditions however, except as a simple approximation.

        Returns a scalar float representing the dB SPL of this audiosegment.
        """"""
    arr = self.to_numpy_array()
    if '<MASK>':
        return 0.0
    else:
        rms = self.rms
        ratio = rms / P_REF_PCM
        return 20.0 * np.log10(ratio + 1e-09)",False,['len(arr) == 0'],___________________,0.0
"def init_log(logger, filename=None, loglevel=None):
    """"""
    Initializes the log file in the proper format.

    Arguments:

        filename (str): Path to a file. Or None if logging is to
                         be disabled.
        loglevel (str): Determines the level of the log output.
    """"""
    template = '[%(asctime)s] %(levelname)-8s: %(name)-25s: %(message)s'
    formatter = logging.Formatter(template)
    if '<MASK>':
        logger.setLevel(getattr(logging, loglevel))
    console = logging.StreamHandler()
    console.setLevel('WARNING')
    console.setFormatter(formatter)
    if '<MASK>':
        file_handler = logging.FileHandler(filename, encoding='utf-8')
        if '<MASK>':
            file_handler.setLevel(getattr(logging, loglevel))
        file_handler.setFormatter(formatter)
        logger.addHandler(file_handler)
    elif '<MASK>':
        console.setLevel(getattr(logging, loglevel))
    logger.addHandler(console)",False,"['loglevel', 'filename', 'loglevel', 'loglevel']","_,,,,,,,,,,,,,,,,,,",0.0
"def get_organism(self):
    """"""Select Enrichr organism from below:

           Human & Mouse: H. sapiens & M. musculus
           Fly: D. melanogaster
           Yeast: S. cerevisiae
           Worm: C. elegans
           Fish: D. rerio

        """"""
    organism = {'default': ['', 'hs', 'mm', 'human', 'mouse', 'homo sapiens', 'mus musculus', 'h. sapiens', 'm. musculus'], 'Fly': ['fly', 'd. melanogaster', 'drosophila melanogaster'], 'Yeast': ['yeast', 's. cerevisiae', 'saccharomyces cerevisiae'], 'Worm': ['worm', 'c. elegans', 'caenorhabditis elegans', 'nematode'], 'Fish': ['fish', 'd. rerio', 'danio rerio', 'zebrafish']}
    for k, v in organism.items():
        if '<MASK>':
            self._organism = k
    if '<MASK>':
        raise Exception('No supported organism found !!!')
    if '<MASK>':
        self._organism = ''
    return",False,"['self._organism is None', ""self._organism == 'default'"", 'self.organism.lower() in v']",___________________,0.0
"def bytes_as_dict(msg):
    """"""Parse CAM message to OrderedDict based on format /key:val.

    Parameters
    ----------
    msg : bytes
        Sequence of /key:val.

    Returns
    -------
    collections.OrderedDict
        With /key:val => dict[key] = val.

    """"""
    cmd_strings = msg.decode()[1:].split(' /')
    cmds = OrderedDict()
    for cmd in cmd_strings:
        unpacked = cmd.split(':')
        if '<MASK>':
            key = unpacked[0]
            val = ':'.join(unpacked[1:])
        elif '<MASK>':
            continue
        else:
            key, val = unpacked
        cmds[key] = val
    return cmds",False,"['len(unpacked) > 2', 'len(unpacked) < 2']",___________________,0.0
"def _remove_extra_delims(expr, ldelim='(', rdelim=')', fcount=None):
    """"""
    Remove unnecessary delimiters (parenthesis, brackets, etc.).

    Internal function that can be recursed
    """"""
    if '<MASK>':
        return ''
    fcount = [0] if fcount is None else fcount
    tfuncs = _get_functions(expr, ldelim=ldelim, rdelim=rdelim)
    for fdict in reversed(tfuncs):
        fcount[0] += 1
        fdict['token'] = '__' + str(fcount[0])
        expr = expr[:fdict['start']] + fdict['token'] + expr[fdict['stop'] + 1:]
        fdict['expr'] = _remove_extra_delims(fdict['expr'], ldelim=ldelim, rdelim=rdelim, fcount=fcount)
    expr = _build_expr(_parse_expr(expr, ldelim=ldelim, rdelim=rdelim), ldelim=ldelim, rdelim=rdelim)
    for fdict in tfuncs:
        expr = expr.replace(fdict['token'], fdict['fname'] + ldelim + fdict['expr'] + rdelim)
    return expr",False,['not expr.strip()'],___________________,0.0
"def transform(self, features):
    """"""Uses the Continuous MDR feature map to construct a new feature from the provided features.

        Parameters
        ----------
        features: array-like {n_samples, n_features}
            Feature matrix to transform

        Returns
        ----------
        array-like: {n_samples}
            Constructed feature from the provided feature matrix
            The constructed feature will be a binary variable, taking the values 0 and 1

        """"""
    new_feature = np.zeros(features.shape[0], dtype=np.int)
    for row_i in range(features.shape[0]):
        feature_instance = tuple(features[row_i])
        if '<MASK>':
            new_feature[row_i] = self.feature_map[feature_instance]
        else:
            new_feature[row_i] = self.default_label
    return new_feature.reshape(features.shape[0], 1)",False,['feature_instance in self.feature_map'],"_____)
    """"""
    """"""
    """"""
    """"""
",0.0
"def verify(cert, signature, data, digest):
    """"""
    Verify the signature for a data string.

    :param cert: signing certificate (X509 object) corresponding to the
        private key which generated the signature.
    :param signature: signature returned by sign function
    :param data: data to be verified
    :param digest: message digest to use
    :return: ``None`` if the signature is correct, raise exception otherwise.

    .. versionadded:: 0.11
    """"""
    data = _text_to_bytes_and_warn('data', data)
    digest_obj = _lib.EVP_get_digestbyname(_byte_string(digest))
    if '<MASK>':
        raise ValueError('No such digest method')
    pkey = _lib.X509_get_pubkey(cert._x509)
    _openssl_assert(pkey != _ffi.NULL)
    pkey = _ffi.gc(pkey, _lib.EVP_PKEY_free)
    md_ctx = _lib.Cryptography_EVP_MD_CTX_new()
    md_ctx = _ffi.gc(md_ctx, _lib.Cryptography_EVP_MD_CTX_free)
    _lib.EVP_VerifyInit(md_ctx, digest_obj)
    _lib.EVP_VerifyUpdate(md_ctx, data, len(data))
    verify_result = _lib.EVP_VerifyFinal(md_ctx, signature, len(signature), pkey)
    if '<MASK>':
        _raise_current_error()",False,"['digest_obj == _ffi.NULL', 'verify_result != 1']",___________________,0.0
"def set_access_credentials(self, _retry=0):
    """"""
		Set the token on the Reddit Object again
		""""""
    if '<MASK>':
        raise ConnectionAbortedError('Reddit is not accessible right now, cannot refresh OAuth2 tokens.')
    self._check_token_present()
    try:
        self.r.set_access_credentials(self._get_value(CONFIGKEY_SCOPE, set, split_val=','), self._get_value(CONFIGKEY_TOKEN), self._get_value(CONFIGKEY_REFRESH_TOKEN))
    except (praw.errors.OAuthInvalidToken, praw.errors.HTTPException) as e:
        self._log('Request new Token (SAC)')
        self._get_new_access_information()",False,['_retry >= 5'],___________________,0.0
"def process_received_ack(self, pkt):
    """"""Process a received ACK packet.

        Not specifiyed in [:rfc:`7844`].
        Probe the offered IP in [:rfc:`2131#section-2.2.`]::

            the allocating
            server SHOULD probe the reused address before allocating the
            address, e.g., with an ICMP echo request, and the client SHOULD
            probe the newly received address, e.g., with ARP.

            The client SHOULD broadcast an ARP
            reply to announce the client's new IP address and clear any
            outdated ARP cache entries in hosts on the client's subnet.

        It is also not specifiyed in [:rfc:`7844`] nor [:rfc:`2131`] how to
        check that the offered IP is valid.

        .. todo::
           - Check that nor ``dhclient`` nor ``systemd-networkd`` send an ARP.
           - Check how other implementations check that the ACK paremeters
             are valid, ie, if the ACK fields match the fields in the OFFER.
           - Check to which state the client should go back to when the
             offered parameters are not valid.

        """"""
    if '<MASK>':
        try:
            self.event = self.client.handle_ack(pkt, self.time_sent_request)
        except AddrFormatError as err:
            logger.error(err)
            raise self.SELECTING()
        logger.info('DHCPACK of %s from %s' % (self.client.client_ip, self.client.server_ip))
        return True
    return False",False,['isack(pkt)'],"_,,,,,,,,,,,,,,,,,,",0.0
"def _bound_waveform(wave, indep_min, indep_max):
    """"""Add independent variable vector bounds if they are not in vector.""""""
    indep_min, indep_max = _validate_min_max(wave, indep_min, indep_max)
    indep_vector = copy.copy(wave._indep_vector)
    if '<MASK>':
        indep_vector = indep_vector.astype(float)
    min_pos = np.searchsorted(indep_vector, indep_min)
    if '<MASK>':
        indep_vector = np.insert(indep_vector, min_pos, indep_min)
    max_pos = np.searchsorted(indep_vector, indep_max)
    if '<MASK>':
        indep_vector = np.insert(indep_vector, max_pos, indep_max)
    dep_vector = _interp_dep_vector(wave, indep_vector)
    wave._indep_vector = indep_vector[min_pos:max_pos + 1]
    wave._dep_vector = dep_vector[min_pos:max_pos + 1]",False,"[""(isinstance(indep_min, float) or isinstance(indep_max, float)) and indep_vector.dtype.name.startswith('int')"", 'not np.isclose(indep_min, indep_vector[min_pos], FP_RTOL, FP_ATOL)', 'not np.isclose(indep_max, indep_vector[max_pos], FP_RTOL, FP_ATOL)']",___________________,0.0
"def main(sample_id, fastq_pair, trim_range, trim_opts, phred, adapters_file, clear):
    """""" Main executor of the trimmomatic template.

    Parameters
    ----------
    sample_id : str
        Sample Identification string.
    fastq_pair : list
        Two element list containing the paired FastQ files.
    trim_range : list
        Two element list containing the trimming range.
    trim_opts : list
        Four element list containing several trimmomatic options:
        [*SLIDINGWINDOW*; *LEADING*; *TRAILING*; *MINLEN*]
    phred : int
        Guessed phred score for the sample. The phred score is a generated
        output from :py:class:`templates.integrity_coverage`.
    adapters_file : str
        Path to adapters file. If not provided, or the path is not available,
        it will use the default adapters from Trimmomatic will be used
    clear : str
        Can be either 'true' or 'false'. If 'true', the input fastq files will
        be removed at the end of the run, IF they are in the working directory
    """"""
    logger.info('Starting trimmomatic')
    cli = ['java', '-Xmx{}'.format('$task.memory'[:-1].lower().replace(' ', '')), '-jar', TRIM_PATH.strip(), 'PE', '-threads', '$task.cpus']
    try:
        phred = int(phred)
        phred_flag = '-phred{}'.format(str(phred))
        cli += [phred_flag]
    except ValueError:
        pass
    cli += fastq_pair
    output_names = []
    for i in range(len(fastq_pair)):
        output_names.append('{}_{}_trim.fastq.gz'.format(SAMPLE_ID, str(i + 1)))
        output_names.append('{}_{}_U.fastq.gz'.format(SAMPLE_ID, str(i + 1)))
    cli += output_names
    if '<MASK>':
        cli += ['CROP:{}'.format(trim_range[1]), 'HEADCROP:{}'.format(trim_range[0])]
    if '<MASK>':
        logger.debug(""Using the provided adapters file '{}'"".format(adapters_file))
    else:
        logger.debug(""Adapters file '{}' not provided or does not exist. Using default adapters"".format(adapters_file))
        adapters_file = merge_default_adapters()
    cli += ['ILLUMINACLIP:{}:3:30:10:6:true'.format(adapters_file)]
    logfile = os.path.join(tempfile.mkdtemp(prefix='tmp'), '{}_trimlog.txt'.format(sample_id))
    cli += ['SLIDINGWINDOW:{}'.format(trim_opts[0]), 'LEADING:{}'.format(trim_opts[1]), 'TRAILING:{}'.format(trim_opts[2]), 'MINLEN:{}'.format(trim_opts[3]), 'TOPHRED33', '-trimlog', logfile]
    logger.debug('Running trimmomatic subprocess with command: {}'.format(cli))
    p = subprocess.Popen(cli, stdout=PIPE, stderr=PIPE)
    stdout, stderr = p.communicate()
    try:
        stderr = stderr.decode('utf8')
    except (UnicodeDecodeError, AttributeError):
        stderr = str(stderr)
    logger.info('Finished trimmomatic subprocess with STDOUT:\\n======================================\\n{}'.format(stdout))
    logger.info('Finished trimmomatic subprocesswith STDERR:\\n======================================\\n{}'.format(stderr))
    logger.info('Finished trimmomatic with return code: {}'.format(p.returncode))
    trimmomatic_log(logfile, sample_id)
    if '<MASK>':
        clean_up(fastq_pair, clear)
    with open('.status', 'w') as status_fh:
        if '<MASK>':
            status_fh.write('fail')
            return
        else:
            status_fh.write('pass')",False,"[""trim_range != ['None']"", 'os.path.exists(adapters_file)', ""p.returncode == 0 and os.path.exists('{}_1_trim.fastq.gz'.format(SAMPLE_ID))"", 'p.returncode != 0']",___________________,0.0
"def _trj_store_meta_data(self, traj):
    """""" Stores general information about the trajectory in the hdf5file.

        The `info` table will contain the name of the trajectory, it's timestamp, a comment,
        the length (aka the number of single runs), and the current version number of pypet.

        Also prepares the desired overview tables and fills the `run` table with dummies.

        """"""
    descriptiondict = {'name': pt.StringCol(pypetconstants.HDF5_STRCOL_MAX_LOCATION_LENGTH, pos=0), 'time': pt.StringCol(len(traj.v_time), pos=1), 'timestamp': pt.FloatCol(pos=3), 'comment': pt.StringCol(pypetconstants.HDF5_STRCOL_MAX_COMMENT_LENGTH, pos=4), 'length': pt.IntCol(pos=2), 'version': pt.StringCol(pypetconstants.HDF5_STRCOL_MAX_NAME_LENGTH, pos=5), 'python': pt.StringCol(pypetconstants.HDF5_STRCOL_MAX_NAME_LENGTH, pos=5)}
    infotable = self._all_get_or_create_table(where=self._overview_group, tablename='info', description=descriptiondict, expectedrows=len(traj))
    insert_dict = self._all_extract_insert_dict(traj, infotable.colnames)
    self._all_add_or_modify_row(traj.v_name, insert_dict, infotable, index=0, flags=(HDF5StorageService.ADD_ROW, HDF5StorageService.MODIFY_ROW))
    rundescription_dict = {'name': pt.StringCol(pypetconstants.HDF5_STRCOL_MAX_NAME_LENGTH, pos=1), 'time': pt.StringCol(len(traj.v_time), pos=2), 'timestamp': pt.FloatCol(pos=3), 'idx': pt.IntCol(pos=0), 'completed': pt.IntCol(pos=8), 'parameter_summary': pt.StringCol(pypetconstants.HDF5_STRCOL_MAX_COMMENT_LENGTH, pos=6), 'short_environment_hexsha': pt.StringCol(7, pos=7), 'finish_timestamp': pt.FloatCol(pos=4), 'runtime': pt.StringCol(pypetconstants.HDF5_STRCOL_MAX_RUNTIME_LENGTH, pos=5)}
    runtable = self._all_get_or_create_table(where=self._overview_group, tablename='runs', description=rundescription_dict)
    hdf5_description_dict = {'complib': pt.StringCol(7, pos=0), 'complevel': pt.IntCol(pos=1), 'shuffle': pt.BoolCol(pos=2), 'fletcher32': pt.BoolCol(pos=3), 'pandas_format': pt.StringCol(7, pos=4), 'encoding': pt.StringCol(11, pos=5)}
    pos = 7
    for name, table_name in HDF5StorageService.NAME_TABLE_MAPPING.items():
        hdf5_description_dict[table_name] = pt.BoolCol(pos=pos)
        pos += 1
    hdf5_description_dict.update({'purge_duplicate_comments': pt.BoolCol(pos=pos + 2), 'results_per_run': pt.IntCol(pos=pos + 3), 'derived_parameters_per_run': pt.IntCol(pos=pos + 4)})
    hdf5table = self._all_get_or_create_table(where=self._overview_group, tablename='hdf5_settings', description=hdf5_description_dict)
    insert_dict = {}
    for attr_name in self.ATTR_LIST:
        insert_dict[attr_name] = getattr(self, attr_name)
    for attr_name, table_name in self.NAME_TABLE_MAPPING.items():
        insert_dict[table_name] = getattr(self, attr_name)
    for attr_name, name in self.PR_ATTR_NAME_MAPPING.items():
        insert_dict[name] = getattr(self, attr_name)
    self._all_add_or_modify_row(traj.v_name, insert_dict, hdf5table, index=0, flags=(HDF5StorageService.ADD_ROW, HDF5StorageService.MODIFY_ROW))
    actual_rows = runtable.nrows
    self._trj_fill_run_table(traj, actual_rows, len(traj._run_information))
    self._grp_store_group(traj, store_data=pypetconstants.STORE_DATA, with_links=False, recursive=False, _hdf5_group=self._trajectory_group)
    self._trj_store_explorations(traj)
    tostore_tables = []
    for name, table_name in HDF5StorageService.NAME_TABLE_MAPPING.items():
        if '<MASK>':
            tostore_tables.append(table_name)
    self._srvc_make_overview_tables(tostore_tables, traj)",False,"['getattr(self, name)']",___________________,0.0
"def authenticate_credentials(self, key):
    """"""Custom authentication to check if auth token has expired.""""""
    user, token = super(TokenAuthentication, self).authenticate_credentials(key)
    if '<MASK>':
        msg = _('Token has expired.')
        raise exceptions.AuthenticationFailed(msg)
    token.update_expiry()
    return (user, token)",False,['token.expires < timezone.now()'],___________________,0.0
"def set_serial(self, hex_str):
    """"""
        Set the serial number.

        The serial number is formatted as a hexadecimal number encoded in
        ASCII.

        :param bytes hex_str: The new serial number.

        :return: ``None``
        """"""
    bignum_serial = _ffi.gc(_lib.BN_new(), _lib.BN_free)
    bignum_ptr = _ffi.new('BIGNUM**')
    bignum_ptr[0] = bignum_serial
    bn_result = _lib.BN_hex2bn(bignum_ptr, hex_str)
    if '<MASK>':
        raise ValueError('bad hex string')
    asn1_serial = _ffi.gc(_lib.BN_to_ASN1_INTEGER(bignum_serial, _ffi.NULL), _lib.ASN1_INTEGER_free)
    _lib.X509_REVOKED_set_serialNumber(self._revoked, asn1_serial)",False,['not bn_result'],"_,_,_,_,_,_,_,_,_,_",0.0
"def hook(request, data):
    data['framework'] = 'pyramid'
    if '<MASK>':
        request.environ['rollbar.uuid'] = data['uuid']
        if '<MASK>':
            data['context'] = request.matched_route.name",False,"['request', 'request.matched_route']",___________________,0.0
"def get_model_class(settings_entry_name):
    """"""Returns a certain sitetree model as defined in the project settings.

    :param str|unicode settings_entry_name:
    :rtype: TreeItemBase|TreeBase
    """"""
    app_name, model_name = get_app_n_model(settings_entry_name)
    try:
        model = apps_get_model(app_name, model_name)
    except (LookupError, ValueError):
        model = None
    if '<MASK>':
        raise ImproperlyConfigured('`SITETREE_%s` refers to model `%s` that has not been installed.' % (settings_entry_name, model_name))
    return model",False,['model is None'],"__name,_name,_name,_name,_name,_name,",0.0
"def authenticate_search_bind(self, username, password):
    """"""
        Performs a search bind to authenticate a user. This is
        required when a the login attribute is not the same
        as the RDN, since we cannot string together their DN on
        the fly, instead we have to find it in the LDAP, then attempt
        to bind with their credentials.

        Args:
            username (str): Username of the user to bind (the field specified
                            as LDAP_BIND_LOGIN_ATTR)
            password (str): User's password to bind with when we find their dn.

        Returns:
            AuthenticationResponse

        """"""
    connection = self._make_connection(bind_user=self.config.get('LDAP_BIND_USER_DN'), bind_password=self.config.get('LDAP_BIND_USER_PASSWORD'))
    try:
        connection.bind()
        log.debug(""Successfully bound to LDAP as '{0}' for search_bind method"".format(self.config.get('LDAP_BIND_USER_DN') or 'Anonymous'))
    except Exception as e:
        self.destroy_connection(connection)
        log.error(e)
        return AuthenticationResponse()
    user_filter = '({search_attr}={username})'.format(search_attr=self.config.get('LDAP_USER_LOGIN_ATTR'), username=username)
    search_filter = '(&{0}{1})'.format(self.config.get('LDAP_USER_OBJECT_FILTER'), user_filter)
    log.debug(""Performing an LDAP Search using filter '{0}', base '{1}', and scope '{2}'"".format(search_filter, self.full_user_search_dn, self.config.get('LDAP_USER_SEARCH_SCOPE')))
    connection.search(search_base=self.full_user_search_dn, search_filter=search_filter, search_scope=getattr(ldap3, self.config.get('LDAP_USER_SEARCH_SCOPE')), attributes=self.config.get('LDAP_GET_USER_ATTRIBUTES'))
    response = AuthenticationResponse()
    if '<MASK>':
        log.debug(""Authentication was not successful for user '{0}'"".format(username))
    else:
        for user in connection.response:
            if '<MASK>':
                continue
            user_connection = self._make_connection(bind_user=user['dn'], bind_password=password)
            log.debug(""Directly binding a connection to a server with user:'{0}'"".format(user['dn']))
            try:
                user_connection.bind()
                log.debug(""Authentication was successful for user '{0}'"".format(username))
                response.status = AuthenticationResponseStatus.success
                user['attributes']['dn'] = user['dn']
                response.user_info = user['attributes']
                response.user_id = username
                response.user_dn = user['dn']
                if '<MASK>':
                    response.user_groups = self.get_user_groups(dn=user['dn'], _connection=connection)
                self.destroy_connection(user_connection)
                break
            except ldap3.core.exceptions.LDAPInvalidCredentialsResult:
                log.debug(""Authentication was not successful for user '{0}'"".format(username))
                response.status = AuthenticationResponseStatus.fail
            except Exception as e:
                log.error(e)
                response.status = AuthenticationResponseStatus.fail
            self.destroy_connection(user_connection)
    self.destroy_connection(connection)
    return response",False,"[""len(connection.response) == 0 or (self.config.get('LDAP_FAIL_AUTH_ON_MULTIPLE_FOUND') and len(connection.response) > 1)"", ""'type' not in user or user.get('type') != 'searchResEntry'"", ""self.config.get('LDAP_SEARCH_FOR_GROUPS')""]",___________________,0.0
"def extract_xml(input_):
    """"""
    Extracts xml from a zip or gzip file at the given path, file-like object,
    or bytes.

    Args:
        input_: A path to a file, a file like object, or bytes

    Returns:
        str: The extracted XML

    """"""
    if '<MASK>':
        file_object = open(input_, 'rb')
    elif '<MASK>':
        file_object = BytesIO(input_)
    else:
        file_object = input_
    try:
        header = file_object.read(6)
        file_object.seek(0)
        if '<MASK>':
            _zip = zipfile.ZipFile(file_object)
            xml = _zip.open(_zip.namelist()[0]).read().decode()
        elif '<MASK>':
            xml = GzipFile(fileobj=file_object).read().decode()
        elif '<MASK>':
            xml = file_object.read().decode()
        else:
            file_object.close()
            raise InvalidAggregateReport('Not a valid zip, gzip, or xml file')
        file_object.close()
    except UnicodeDecodeError:
        raise InvalidAggregateReport('File objects must be opened in binary (rb) mode')
    except Exception as error:
        raise InvalidAggregateReport('Invalid archive file: {0}'.format(error.__str__()))
    return xml",False,"['type(input_) == str', 'type(input_) == bytes', 'header.startswith(MAGIC_ZIP)', 'header.startswith(MAGIC_GZIP)', 'header.startswith(MAGIC_XML)']",___________________,0.0
"def patch_debugtoolbar(settings):
    """"""
    Patches the pyramid_debugtoolbar (if installed) to display a link to the related rollbar item.
    """"""
    try:
        from pyramid_debugtoolbar import tbtools
    except ImportError:
        return
    rollbar_web_base = settings.get('rollbar.web_base', DEFAULT_WEB_BASE)
    if '<MASK>':
        rollbar_web_base = rollbar_web_base[:-1]

    def insert_rollbar_console(request, html):
        item_uuid = request.environ.get('rollbar.uuid')
        if '<MASK>':
            return html
        url = '%s/item/uuid/?uuid=%s' % (rollbar_web_base, item_uuid)
        link = '<a style=""color:white;"" href=""%s"">View in Rollbar</a>' % url
        new_data = '<h2>Rollbar: %s</h2>' % link
        insertion_marker = '</h1>'
        replacement = insertion_marker + new_data
        return html.replace(insertion_marker, replacement, 1)
    old_render_full = tbtools.Traceback.render_full

    def new_render_full(self, request, *args, **kw):
        html = old_render_full(self, request, *args, **kw)
        return insert_rollbar_console(request, html)
    tbtools.Traceback.render_full = new_render_full",False,"[""rollbar_web_base.endswith('/')"", 'not item_uuid']",___________________,0.0
"def parse_param(param, include_desc=False):
    """"""Parse a single typed parameter statement.""""""
    param_def, _colon, desc = param.partition(':')
    if '<MASK>':
        desc = None
    else:
        desc = desc.lstrip()
    if '<MASK>':
        raise ValidationError('Invalid parameter declaration in docstring, missing colon', declaration=param)
    param_name, _space, param_type = param_def.partition(' ')
    if '<MASK>':
        raise ValidationError('Invalid parameter type string not enclosed in ( ) characters', param_string=param_def, type_string=param_type)
    param_type = param_type[1:-1]
    return (param_name, ParameterInfo(param_type, [], desc))",False,"['not include_desc', ""_colon == ''"", ""len(param_type) < 2 or param_type[0] != '(' or param_type[-1] != ')'""]",___________________,0.0
"def name(obj) -> str:
    """"""This helper function attempts to resolve the dot-colon import path for a given object.
	
	Specifically searches for classes and methods, it should be able to find nearly anything at either the module
	level or nested one level deep.  Uses ``__qualname__`` if available.
	""""""
    if '<MASK>':
        obj = obj.__class__
    module = getmodule(obj)
    return module.__name__ + ':' + obj.__qualname__",False,"[""not isroutine(obj) and (not hasattr(obj, '__name__')) and hasattr(obj, '__class__')""]","(,,,,,,,,,,,,,,,,,,",0.0
"def mac(self, algorithm, key, data):
    """"""
        Generate message authentication code.

        Args:
            algorithm(CryptographicAlgorithm): An enumeration specifying the
                algorithm for which the MAC operation will use.
            key(bytes): secret key used in the MAC operation
            data(bytes): The data to be MACed.

        Returns:
            bytes: The MACed data

        Raises:
            InvalidField: Raised when the algorithm is unsupported or the
                length is incompatible with the algorithm.
            CryptographicFailure: Raised when the key generation process
                fails.

        Example:
            >>> engine = CryptographyEngine()
            >>> mac_data = engine.mac(
            ...     CryptographicAlgorithm.HMAC-SHA256, b'\x01\x02\x03\x04',
            ...     b'\x05\x06\x07\x08')
        """"""
    mac_data = None
    if '<MASK>':
        self.logger.info('Generating a hash-based message authentication code using {0}'.format(algorithm.name))
        hash_algorithm = self._hash_algorithms.get(algorithm)
        try:
            h = hmac.HMAC(key, hash_algorithm(), backend=default_backend())
            h.update(data)
            mac_data = h.finalize()
        except Exception as e:
            self.logger.exception(e)
            raise exceptions.CryptographicFailure('An error occurred while computing an HMAC. See the server log for more information.')
    elif '<MASK>':
        self.logger.info('Generating a cipher-based message authentication code using {0}'.format(algorithm.name))
        cipher_algorithm = self._symmetric_key_algorithms.get(algorithm)
        try:
            c = cmac.CMAC(cipher_algorithm(key), backend=default_backend())
            c.update(data)
            mac_data = c.finalize()
        except Exception as e:
            raise exceptions.CryptographicFailure('An error occurred while computing a CMAC. See the server log for more information.')
    else:
        raise exceptions.InvalidField('The cryptographic algorithm ({0}) is not a supported for a MAC operation.'.format(algorithm))
    return mac_data",False,"['algorithm in self._hash_algorithms.keys()', 'algorithm in self._symmetric_key_algorithms.keys()']","_,,,,,,,,,,,,,,,,,,",0.0
"def simEvalCond(simulator, *conds):
    """"""
    Evaluate list of values as condition
    """"""
    _cond = True
    _vld = True
    for v in conds:
        val = bool(v.val)
        fullVld = v.vldMask == 1
        if '<MASK>':
            if '<MASK>':
                return (False, True)
        else:
            return (False, False)
        _cond = _cond and val
        _vld = _vld and fullVld
    return (_cond, _vld)",False,"['fullVld', 'not val']",___________________,0.0
"def merge_tiers(self, tiers, tiernew=None, gapt=0, sep='_', safe=False):
    """"""Merge tiers into a new tier and when the gap is lower then the
        threshhold glue the annotations together.

        :param list tiers: List of tier names.
        :param str tiernew: Name for the new tier, if ``None`` the name will be
                            generated.
        :param int gapt: Threshhold for the gaps, if the this is set to 10 it
                         means that all gaps below 10 are ignored.
        :param str sep: Separator for the merged annotations.
        :param bool safe: Ignore zero length annotations(when working with
            possible malformed data).
        :returns: Name of the created tier.
        :raises KeyError: If a tier is non existent.
        """"""
    if '<MASK>':
        tiernew = u'{}_merged'.format('_'.join(tiers))
    self.add_tier(tiernew)
    aa = [(sys.maxsize, sys.maxsize, None)] + sorted((a for t in tiers for a in self.get_annotation_data_for_tier(t)), reverse=True)
    l = None
    while aa:
        begin, end, value = aa.pop()
        if '<MASK>':
            l = [begin, end, [value]]
        elif '<MASK>':
            if '<MASK>':
                self.add_annotation(tiernew, l[0], l[1], sep.join(l[2]))
            l = [begin, end, [value]]
        else:
            if '<MASK>':
                l[1] = end
            l[2].append(value)
    return tiernew",False,"['tiernew is None', 'l is None', 'begin - l[1] >= gapt', 'not safe or l[1] > l[0]', 'end > l[1]']",___________________,0.0
"def getargspec_permissive(func):
    """"""
    An `inspect.getargspec` with a relaxed sanity check to support Cython.

    Motivation:

        A Cython-compiled function is *not* an instance of Python's
        types.FunctionType.  That is the sanity check the standard Py2
        library uses in `inspect.getargspec()`.  So, an exception is raised
        when calling `argh.dispatch_command(cythonCompiledFunc)`.  However,
        the CyFunctions do have perfectly usable `.func_code` and
        `.func_defaults` which is all `inspect.getargspec` needs.

        This function just copies `inspect.getargspec()` from the standard
        library but relaxes the test to a more duck-typing one of having
        both `.func_code` and `.func_defaults` attributes.
    """"""
    if '<MASK>':
        func = func.im_func
    if '<MASK>':
        raise TypeError('{!r} missing func_code or func_defaults'.format(func))
    args, varargs, varkw = inspect.getargs(func.func_code)
    return inspect.ArgSpec(args, varargs, varkw, func.func_defaults)",False,"['inspect.ismethod(func)', ""not (hasattr(func, 'func_code') and hasattr(func, 'func_defaults'))""]",___________________,0.0
"def visit_subscript(self, node):
    """""" Look for indexing exceptions. """"""
    try:
        for inferred in node.value.infer():
            if '<MASK>':
                continue
            if '<MASK>':
                self.add_message('indexing-exception', node=node)
    except astroid.InferenceError:
        return",False,"['not isinstance(inferred, astroid.Instance)', 'utils.inherit_from_std_ex(inferred)']",",,,,,,,,,,,,,,,,,,,",0.0
"def do_help(self, command):
    """"""Display this help message.""""""
    if '<MASK>':
        doc = getattr(self, 'do_' + command).__doc__
        print(cyan(doc.replace(' ' * 8, '')))
    else:
        print(magenta('Available commands:'))
        print(magenta('Type ""HELP <command>"" to get more info.'))
        names = self.get_names()
        names.sort()
        for name in names:
            if '<MASK>':
                continue
            doc = getattr(self, name).__doc__
            doc = doc.split('\n')[0]
            print('{} {}'.format(yellow(name[3:]), cyan(doc.replace(' ' * 8, ' ').replace('\n', ''))))",False,"['command', ""name[:3] != 'do_'""]","_,,,,,,,,,,,,,,,,,,",0.0
"def unreferenced_vert(script):
    """""" Check for every vertex on the mesh: if it is NOT referenced by a face,
        removes it.

    Args:
        script: the FilterScript object or script filename to write
            the filter to.

    Layer stack:
        No impacts

    MeshLab versions:
        2016.12
        1.3.4BETA
    """"""
    if '<MASK>':
        filter_xml = '  <filter name=""Remove Unreferenced Vertex""/>\n'
    else:
        filter_xml = '  <filter name=""Remove Unreferenced Vertices""/>\n'
    util.write_filter(script, filter_xml)
    return None",False,"[""script.ml_version == '1.3.4BETA'""]",___________________,0.0
"def _neg_prob(t, seq_pair, multiplicity):
    """"""
            Probability to observe a child given the the parent state, transition
            matrix, and the time of evolution (branch length).

            Parameters
            ----------

             t : double
                Branch length (time between sequences)

             parent :  numpy.array
                Parent sequence

             child : numpy.array
                Child sequence

             tm :  GTR
                Model of evolution

            Returns
            -------

             prob : double
                Negative probability of the two given sequences
                to be separated by the time t.
            """"""
    if '<MASK>':
        res = -1.0 * self.prob_t_profiles(seq_pair, multiplicity, t ** 2, return_log=True)
        return res
    else:
        return -1.0 * self.prob_t_compressed(seq_pair, multiplicity, t ** 2, return_log=True)",False,['profiles'],___________________,0.0
"def source_expand(self, source):
    """"""Expand the wildcards for an S3 path. This emulates the shall expansion
       for wildcards if the input is local path.
    """"""
    result = []
    if '<MASK>':
        source = [source]
    for src in source:
        tmp = self.opt.recursive
        self.opt.recursive = False
        result += [f['name'] for f in self.s3walk(src, True)]
        self.opt.recursive = tmp
    if '<MASK>':
        fail(""[Runtime Failure] Source doesn't exist."")
    return result",False,"['not isinstance(source, list)', 'len(result) == 0 and (not self.opt.ignore_empty_source)']",___________________,0.0
"def initialize_logger(args):
    """"""Sets command name and formatting for subsequent calls to logger""""""
    global log_filename
    log_filename = os.path.join(os.getcwd(), 'jacquard.log')
    if '<MASK>':
        _validate_log_file(args.log_file)
        log_filename = args.log_file
    logging.basicConfig(format=_FILE_LOG_FORMAT, level='DEBUG', datefmt=_DATE_FORMAT, filename=log_filename)
    global _verbose
    if '<MASK>':
        _verbose = args.verbose
    start_time = datetime.now().strftime(_DATE_FORMAT)
    global _logging_dict
    _logging_dict = {'user': getpass.getuser(), 'host': socket.gethostname(), 'start_time': start_time, 'tool': args.subparser_name}",False,"['args.log_file', 'args.verbose']",___________________,0.0
"def get_table(self, arch, pattern, colored=False, verbose=False):
    """"""
        This function is used in sys command (when user want to find a specific syscall)

        :param Architecture for syscall table;
        :param Searching pattern;
        :param Flag for verbose output
        :return Return a printable table of matched syscalls
        """"""
    rawtable = self.search(arch, pattern)
    if '<MASK>':
        return None
    used_hd = self.__fetch_used_headers(rawtable, verbose)
    table = [self.__make_colored_row(used_hd, 'yellow,bold', upper=True) if colored else used_hd]
    for command in rawtable:
        cur_tb_field = []
        for hd in used_hd:
            value = command[hd]
            cur_tb_field.append(self.__make_colored_field(value, hd, verbose=verbose))
        table.append(cur_tb_field)
    return DoubleTable(table)",False,['len(rawtable) == 0'],___________________,0.0
"def summary_err_table(df, qvalues=[0, 0.01, 0.02, 0.05, 0.1, 0.2, 0.3, 0.4, 0.5]):
    """""" Summary error table for some typical q-values """"""
    qvalues = to_one_dim_array(qvalues)
    ix = find_nearest_matches(np.float32(df.qvalue.values), qvalues)
    df_sub = df.iloc[ix].copy()
    for i_sub, (i0, i1) in enumerate(zip(ix, ix[1:])):
        if '<MASK>':
            df_sub.iloc[i_sub + 1, :] = None
    df_sub.qvalue = qvalues
    df_sub.reset_index(inplace=True, drop=True)
    return df_sub[['qvalue', 'pvalue', 'svalue', 'pep', 'fdr', 'fnr', 'fpr', 'tp', 'tn', 'fp', 'fn', 'cutoff']]",False,['i1 == i0'],___________________,0.0
"def use_privatekey_file(self, keyfile, filetype=_UNSPECIFIED):
    """"""
        Load a private key from a file

        :param keyfile: The name of the key file (``bytes`` or ``unicode``)
        :param filetype: (optional) The encoding of the file, which is either
            :const:`FILETYPE_PEM` or :const:`FILETYPE_ASN1`.  The default is
            :const:`FILETYPE_PEM`.

        :return: None
        """"""
    keyfile = _path_string(keyfile)
    if '<MASK>':
        filetype = FILETYPE_PEM
    elif '<MASK>':
        raise TypeError('filetype must be an integer')
    use_result = _lib.SSL_CTX_use_PrivateKey_file(self._context, keyfile, filetype)
    if '<MASK>':
        self._raise_passphrase_exception()",False,"['filetype is _UNSPECIFIED', 'not use_result', 'not isinstance(filetype, integer_types)']",___________________,0.0
"def print_context(self, context):
    """"""
        Print the entire template context
        """"""
    text = [CONTEXT_TITLE]
    for i, context_scope in enumerate(context):
        dump1 = linebreaksbr(pformat_django_context_html(context_scope))
        dump2 = pformat_dict_summary_html(context_scope)
        if '<MASK>':
            dump1, dump2 = (dump2, dump1)
        text.append(CONTEXT_BLOCK.format(style=PRE_STYLE, num=i, dump1=dump1, dump2=dump2))
    return u''.join(text)",False,"[""len(context_scope) <= 3 and dump1.count('<br />') > 20""]",___________________,0.0
"def setup(self):
    """"""Initialization done before entering the debugger-command
        loop. In particular we set up the call stack used for local
        variable lookup and frame/up/down commands.

        We return True if we should NOT enter the debugger-command
        loop.""""""
    self.forget()
    if '<MASK>':
        self.frame = inspect.currentframe()
        pass
    if '<MASK>':
        exc_type, exc_value, exc_traceback = self.event_arg
    else:
        _, _, exc_traceback = (None, None, None)
        pass
    if '<MASK>':
        self.stack, self.curindex = get_stack(self.frame, exc_traceback, None, self)
        self.curframe = self.stack[self.curindex][0]
        self.thread_name = Mthread.current_thread_name()
        if '<MASK>':
            self.list_lineno = traceback.extract_tb(exc_traceback, 1)[0][1]
            self.list_offset = self.curframe.f_lasti
            self.list_object = self.curframe
    else:
        self.stack = self.curframe = self.botframe = None
        pass
    if '<MASK>':
        self.list_lineno = max(1, inspect.getlineno(self.curframe) - int(self.settings('listsize') / 2)) - 1
        self.list_offset = self.curframe.f_lasti
        self.list_filename = self.curframe.f_code.co_filename
        self.list_object = self.curframe
    else:
        if '<MASK>':
            self.list_lineno = None
        pass
    return False",False,"[""self.settings('dbg_trepan')"", ""self.event in ['exception', 'c_exception']"", 'self.frame or exc_traceback', 'self.curframe', 'exc_traceback', 'not exc_traceback']",___________________,0.0
"def get_timeout(self, state, function):
    """"""Workaround to get timeout in the ATMT.timeout class method.""""""
    state = STATES2NAMES[state]
    for timeout_fn_t in self.timeout[state]:
        if '<MASK>':
            logger.debug('Timeout for state %s, function %s, is %s', state, function.atmt_condname, timeout_fn_t[0])
            return timeout_fn_t[0]
    return None",False,['timeout_fn_t[1] is not None and timeout_fn_t[1].atmt_condname == function.atmt_condname'],___________________,0.0
"def from_numpy_array(nparr, framerate):
    """"""
    Returns an AudioSegment created from the given numpy array.

    The numpy array must have shape = (num_samples, num_channels).

    :param nparr: The numpy array to create an AudioSegment from.
    :returns: An AudioSegment created from the given array.
    """"""
    if '<MASK>':
        raise ValueError('Numpy Array must contain 8, 16, or 32 bit values.')
    if '<MASK>':
        arrays = [nparr]
    elif '<MASK>':
        arrays = [nparr[i, :] for i in range(nparr.shape[0])]
    else:
        raise ValueError('Numpy Array must be one or two dimensional. Shape must be: (num_samples, num_channels).')
    interleaved = np.vstack(arrays).reshape((-1,), order='F')
    dubseg = pydub.AudioSegment(interleaved.tobytes(), frame_rate=framerate, sample_width=interleaved.dtype.itemsize, channels=len(interleaved.shape))
    return AudioSegment(dubseg, '')",False,"['nparr.dtype.itemsize not in (1, 2, 4)', 'len(nparr.shape) == 1', 'len(nparr.shape) == 2']","_,,,,,,,,,,,,,,,,,,",0.0
"def run(self):
    """"""Starts listening to the queue.""""""
    try:
        while True:
            msg, args, kwargs = self._receive_data()
            stop = self._handle_data(msg, args, kwargs)
            if '<MASK>':
                break
    finally:
        if '<MASK>':
            self._close_file()
        self._trajectory_name = ''",False,"['self._storage_service.is_open', 'stop']",(((((((((((((((((((,0.0
"def nameAvailabilityCheck(obj, propName, prop):
    """"""
    Check if not redefining property on obj
    """"""
    if '<MASK>':
        raise IntfLvlConfErr('%r already has property %s old:%s new:%s' % (obj, propName, repr(getattr(obj, propName)), prop))",False,"['getattr(obj, propName, None) is not None']",(((((((((((((((((((,0.0
"def boot(self):
    """"""
        Boots a server for the app, if it isn't already booted.

        Returns:
            Server: This server.
        """"""
    if '<MASK>':
        type(self)._ports[self.port_key] = self.port
        init_func = capybara.servers[capybara.server_name]
        init_args = (self.middleware, self.port, self.host)
        self.server_thread = Thread(target=init_func, args=init_args)
        self.server_thread.daemon = True
        self.server_thread.start()
        timer = Timer(60)
        while not self.responsive:
            if '<MASK>':
                raise RuntimeError('WSGI application timed out during boot')
            self.server_thread.join(0.1)
    return self",False,"['not self.responsive', 'timer.expired']",___________________,0.0
"def cli_put_directory_structure(context, path):
    """"""
    Performs PUTs rooted at the path using a directory structure
    pointed to by context.input\\_.

    See :py:mod:`swiftly.cli.put` for context usage information.

    See :py:class:`CLIPut` for more information.
    """"""
    if '<MASK>':
        raise ReturnCode('called cli_put_directory_structure without context.input_ set')
    if '<MASK>':
        raise ReturnCode('%r is not a directory' % context.input_)
    if '<MASK>':
        raise ReturnCode('uploading a directory structure requires at least a container name')
    new_context = context.copy()
    new_context.input_ = None
    container = path.split('/', 1)[0]
    cli_put_container(new_context, container)
    ilen = len(context.input_)
    if '<MASK>':
        ilen += 1
    conc = Concurrency(context.concurrency)
    for dirpath, dirnames, filenames in os.walk(context.input_):
        if '<MASK>':
            new_context = context.copy()
            new_context.headers = dict(context.headers)
            new_context.headers['content-type'] = 'text/directory'
            new_context.headers['x-object-meta-mtime'] = '%f' % os.path.getmtime(context.input_)
            new_context.input_ = None
            new_context.empty = True
            new_path = path
            if '<MASK>':
                new_path += '/'
            new_path += dirpath[ilen:]
            for exc_type, exc_value, exc_tb, result in six.itervalues(conc.get_results()):
                if '<MASK>':
                    conc.join()
                    raise exc_value
            conc.spawn(new_path, cli_put_object, new_context, new_path)
        else:
            for fname in filenames:
                new_context = context.copy()
                new_context.input_ = os.path.join(dirpath, fname)
                new_path = path
                if '<MASK>':
                    new_path += '/'
                if '<MASK>':
                    new_path += dirpath[ilen:] + '/'
                new_path += fname
                for exc_type, exc_value, exc_tb, result in six.itervalues(conc.get_results()):
                    if '<MASK>':
                        conc.join()
                        raise exc_value
                conc.spawn(new_path, cli_put_object, new_context, new_path)
    conc.join()
    for exc_type, exc_value, exc_tb, result in six.itervalues(conc.get_results()):
        if '<MASK>':
            raise exc_value",False,"['not context.input_', 'not os.path.isdir(context.input_)', 'not path', 'not context.input_.endswith(os.sep)', 'not dirnames and (not filenames)', 'exc_value', ""path[-1] != '/'"", 'exc_value', ""path[-1] != '/'"", 'dirpath[ilen:]', 'exc_value']",___________________,0.0
"def _is_big_enough(image, size):
    """"""Check that the image's size superior to `size`""""""
    if '<MASK>':
        raise ImageSizeError(image.size, size)",False,['size[0] > image.size[0] and size[1] > image.size[1]'],"_en,, size, size, size, size, size, size, size, size",0.0
"def refresh(self, force=False, soon=86400):
    """"""
        Refreshes the credentials only if the **provider** supports it and if
        it will expire in less than one day. It does nothing in other cases.

        .. note::

            The credentials will be refreshed only if it gives sense
            i.e. only |oauth2|_ has the notion of credentials
            *refreshment/extension*.
            And there are also differences across providers e.g. Google
            supports refreshment only if there is a ``refresh_token`` in
            the credentials and that in turn is present only if the
            ``access_type`` parameter was set to ``offline`` in the
            **user authorization request**.

        :param bool force:
            If ``True`` the credentials will be refreshed even if they
            won't expire soon.

        :param int soon:
            Number of seconds specifying what means *soon*.

        """"""
    if '<MASK>':
        if '<MASK>':
            logging.info('PROVIDER NAME: {0}'.format(self.provider_name))
            return self.provider_class(self, None, self.provider_name).refresh_credentials(self)",False,"[""hasattr(self.provider_class, 'refresh_credentials')"", 'force or self.expire_soon(soon)']",___________________,0.0
"def _link_field_to_dict(field):
    """""" Utility for ripping apart github's Link header field.
        It's kind of ugly.
        """"""
    if '<MASK>':
        return dict()
    return dict([(part.split('; ')[1][5:-1], part.split('; ')[0][1:-1]) for part in field.split(', ')])",False,['not field'],___________________,0.0
"def f_remove_child(self, name, recursive=False, predicate=None):
    """"""Removes a child of the group.

        Note that groups and leaves are only removed from the current trajectory in RAM.
        If the trajectory is stored to disk, this data is not affected. Thus, removing children
        can be only be used to free RAM memory!

        If you want to free memory on disk via your storage service,
        use :func:`~pypet.trajectory.Trajectory.f_delete_items` of your trajectory.

        :param name:

            Name of child, naming by grouping is NOT allowed ('groupA.groupB.childC'),
            child must be direct successor of current node.

        :param recursive:

            Must be true if child is a group that has children. Will remove
            the whole subtree in this case. Otherwise a Type Error is thrown.

        :param predicate:

            Predicate which can evaluate for each node to ``True`` in order to remove the node or
            ``False`` if the node should be kept. Leave ``None`` if you want to remove all nodes.

        :raises:

            TypeError if recursive is false but there are children below the node.

            ValueError if child does not exist.

        """"""
    if '<MASK>':
        raise ValueError('Your group `%s` does not contain the child `%s`.' % (self.v_full_name, name))
    else:
        child = self._children[name]
        if '<MASK>':
            raise TypeError('Cannot remove child. It is a group with children. Use f_remove with ``recursive = True``')
        else:
            self._nn_interface._remove_subtree(self, name, predicate)",False,"['name not in self._children', 'name not in self._links and (not child.v_is_leaf) and child.f_has_children() and (not recursive)']",___________________,0.0
"def has_arg(self, name: str, value: str=None) -> bool:
    """"""Return true if the is an arg named `name`.

        Also check equality of values if `value` is provided.

        Note: If you just need to get an argument and you want to LBYL, it's
            better to get_arg directly and then check if the returned value
            is None.
        """"""
    for arg in reversed(self.arguments):
        if '<MASK>':
            if '<MASK>':
                if '<MASK>':
                    if '<MASK>':
                        return True
                    return False
                if '<MASK>':
                    return True
                return False
            return True
    return False",False,"['arg.name.strip(WS) == name.strip(WS)', 'value', 'arg.positional', 'arg.value.strip(WS) == value.strip(WS)', 'arg.value == value']",___________________,0.0
"def waitForSlotEvent(self, flags=0):
    """"""
        C_WaitForSlotEvent

        :param flags: 0 (default) or `CKF_DONT_BLOCK`
        :type flags: integer
        :return: slot
        :rtype: integer
        """"""
    tmp = 0
    rv, slot = self.lib.C_WaitForSlotEvent(flags, tmp)
    if '<MASK>':
        raise PyKCS11Error(rv)
    return slot",False,['rv != CKR_OK'],___________________,0.0
"def patch(self, endpoint, data, headers=None, inception=False):
    """"""
        Method to update an item

        The headers must include an If-Match containing the object _etag.
            headers = {'If-Match': contact_etag}

        The data dictionary contain the fields that must be modified.

        If the patching fails because the _etag object do not match with the provided one, a
        BackendException is raised with code = 412.

        If inception is True, this method makes e new get request on the endpoint to refresh the
        _etag and then a new patch is called.

        If an HTTP 412 error occurs, a BackendException is raised. This exception is:
        - code: 412
        - message: response content
        - response: backend response

        All other HTTP error raises a BackendException.
        If some _issues are provided by the backend, this exception is:
        - code: HTTP error code
        - message: response content
        - response: JSON encoded backend response (including '_issues' dictionary ...)

        If no _issues are provided and an _error is signaled by the backend, this exception is:
        - code: backend error code
        - message: backend error message
        - response: JSON encoded backend response

        :param endpoint: endpoint (API URL)
        :type endpoint: str
        :param data: properties of item to update
        :type data: dict
        :param headers: headers (example: Content-Type). 'If-Match' required
        :type headers: dict
        :param inception: if True tries to get the last _etag
        :type inception: bool
        :return: dictionary containing patch response from the backend
        :rtype: dict
        """"""
    if '<MASK>':
        raise BackendException(BACKEND_ERROR, 'Header If-Match required for patching an object')
    response = self.get_response(method='PATCH', endpoint=endpoint, json=data, headers=headers)
    if '<MASK>':
        return self.decode(response=response)
    if '<MASK>':
        if '<MASK>':
            resp = self.get(endpoint)
            headers = {'If-Match': resp['_etag']}
            return self.patch(endpoint, data=data, headers=headers, inception=False)
        raise BackendException(response.status_code, response.content)
    else:
        raise BackendException(response.status_code, response.content)",False,"['not headers', 'response.status_code == 200', 'response.status_code == 412', 'inception']",___________________,0.0
"def reinterpretBits(self, sigOrVal, toType):
    """"""
    Cast object of same bit size between to other type
    (f.e. bits to struct, union or array)
    """"""
    if '<MASK>':
        return reinterpretBits__val(self, sigOrVal, toType)
    elif '<MASK>':
        return fitTo_t(sigOrVal, toType)
    elif '<MASK>':
        if '<MASK>':
            raise reinterpret_bits_to_hstruct(sigOrVal, toType)
        elif '<MASK>':
            raise NotImplementedError()
        elif '<MASK>':
            reinterpret_bits_to_harray(sigOrVal, toType)
    return default_auto_cast_fn(self, sigOrVal, toType)",False,"['isinstance(sigOrVal, Value)', 'isinstance(toType, Bits)', 'sigOrVal._dtype.bit_length() == toType.bit_length()', 'isinstance(toType, HStruct)', 'isinstance(toType, HUnion)', 'isinstance(toType, HArray)']",___________________,0.0
"def _main(argv):
    """"""
    Handle arguments for the 'lumi-upload' command.
    """"""
    parser = argparse.ArgumentParser(description=DESCRIPTION, formatter_class=argparse.RawDescriptionHelpFormatter)
    parser.add_argument('-b', '--base-url', default=URL_BASE, help='API root url, default: %s' % URL_BASE)
    parser.add_argument('-a', '--account-id', default=None, help='Account ID that should own the project, if not the default')
    parser.add_argument('-l', '--language', default='en', help='The language code for the language the text is in. Default: en')
    parser.add_argument('-t', '--token', help='API authentication token')
    parser.add_argument('-s', '--save-token', action='store_true', help='save --token for --base-url to ~/.luminoso/tokens.json')
    parser.add_argument('input_filename', help='The JSON-lines (.jsons) file of documents to upload')
    parser.add_argument('project_name', nargs='?', default=None, help='What the project should be called')
    args = parser.parse_args(argv)
    if '<MASK>':
        if '<MASK>':
            raise ValueError('error: no token provided')
        LuminosoClient.save_token(args.token, domain=urlparse(args.base_url).netloc)
    client = LuminosoClient.connect(url=args.base_url, token=args.token)
    name = args.project_name
    if '<MASK>':
        name = input('Enter a name for the project: ')
        if '<MASK>':
            print('Aborting because no name was provided.')
            return
    result = upload_docs(client, args.input_filename, args.language, name, account=args.account_id, progress=True)
    print('Project {!r} created with {} documents'.format(result['project_id'], result['document_count']))",False,"['args.save_token', 'name is None', 'not args.token', 'not name']",___________________,0.0
"def _install_directory_structure_file(cls):
    """"""
        Download the latest version of `dir_structure_production.json`.
        """"""
    dir_structure_link = PyFunceble.CONFIGURATION['links']['dir_structure']
    dir_structure_link = Version(True).right_url_from_version(dir_structure_link)
    destination = PyFunceble.CURRENT_DIRECTORY + PyFunceble.CONFIGURATION['outputs']['default_files']['dir_structure']
    if '<MASK>':
        data = Download(dir_structure_link, destination, return_data=True).text()
        File(destination).write(data, overwrite=True)
        return True
    return None",False,['not Version(True).is_cloned() or not PyFunceble.path.isfile(destination)'],___________________,0.0
"def resolve_provider_class(class_):
    """"""
    Returns a provider class.

    :param class_name: :class:`string` or
    :class:`authomatic.providers.BaseProvider` subclass.

    """"""
    if '<MASK>':
        path = '.'.join([__package__, 'providers', class_])
        return import_string(class_, True) or import_string(path)
    else:
        return class_",False,"['isinstance(class_, str)']",___________________,0.0
"def set_exception(self, exception):
    """"""Sets the exception on the future.""""""
    if '<MASK>':
        raise TransferNotDoneError('set_exception can only be called once the transfer is complete.')
    self._coordinator.set_exception(exception, override=True)",False,['not self.done()'],_exception(exception(exception(exception(exception(exception(exception(exception(exception(,0.0
"def verified(self, institute_id):
    """"""Return all verified variants for a given institute

        Args:
            institute_id(str): institute id

        Returns:
            res(list): a list with validated variants
        """"""
    query = {'verb': 'validate', 'institute': institute_id}
    res = []
    validate_events = self.event_collection.find(query)
    for validated in list(validate_events):
        case_id = validated['case']
        var_obj = self.variant(case_id=case_id, document_id=validated['variant_id'])
        case_obj = self.case(case_id=case_id)
        if '<MASK>':
            continue
        var_obj['case_obj'] = {'display_name': case_obj['display_name'], 'individuals': case_obj['individuals']}
        res.append(var_obj)
    return res",False,['not case_obj or not var_obj'],___________________,0.0
"def predict(self, vector):
    if '<MASK>':
        return 1
    else:
        return 0",False,"['self.v.is_speech(vector.raw_data, vector.frame_rate)']","(self,
    """"""
    """"""
    """"""
    """"""
    """"""
",0.0
"def find_bai_file(bam_file):
    """"""Find out BAI file by extension given the BAM file.""""""
    bai_file = bam_file.replace('.bam', '.bai')
    if '<MASK>':
        bai_file = '{}.bai'.format(bam_file)
    return bai_file",False,['not os.path.exists(bai_file)'],_file_file_file_file_file_file_file_file_file_,0.0
"def format_metric_string(self, name, value, m_type):
    """"""Compose a statsd compatible string for a metric's measurement.""""""
    template = '{name}:{value}|{m_type}\n'
    if '<MASK>':
        name = '{prefix}.{m_name}'.format(prefix=self.prefix, m_name=name)
    return template.format(name=name, value=value, m_type=m_type)",False,['self.prefix'],"_name_name_name_name_name)
    """""" """""" """""" """""" """""" """"""",0.0
"def parse(self):
    """"""Parse a Supybot IRC stream.

        Returns an iterator of dicts. Each dicts contains information
        about the date, type, nick and body of a single log entry.

        :returns: iterator of parsed lines

        :raises ParseError: when an invalid line is found parsing the given
            stream
        """"""
    for line in self.stream:
        line = line.rstrip('\n')
        self.nline += 1
        if '<MASK>':
            continue
        ts, msg = self._parse_supybot_timestamp(line)
        if '<MASK>':
            continue
        elif '<MASK>':
            continue
        elif '<MASK>':
            continue
        itype, nick, body = self._parse_supybot_msg(msg)
        item = self._build_item(ts, itype, nick, body)
        yield item",False,"['self.SUPYBOT_EMPTY_REGEX.match(line)', 'self.SUPYBOT_EMPTY_COMMENT_REGEX.match(msg)', 'self.SUPYBOT_EMPTY_COMMENT_ACTION_REGEX.match(msg)', 'self.SUPYBOT_EMPTY_BOT_REGEX.match(msg)']",___________________,0.0
"def _merge_adjacent_segments(mask):
    """"""
    Merges all segments in `mask` which are touching.
    """"""
    mask_ids = [id for id in np.unique(mask) if id != 0]
    for id in mask_ids:
        myfidxs, mysidxs = np.where(mask == id)
        for other in mask_ids:
            if '<MASK>':
                continue
            else:
                other_fidxs, other_sidxs = np.where(mask == other)
                if '<MASK>':
                    mask[other_fidxs, other_sidxs] = id",False,"['id == other', '_segments_are_adjacent((myfidxs, mysidxs), (other_fidxs, other_sidxs))']",___________________,0.0
"def _serializeExclude_eval(parentUnit, obj, isDeclaration, priv):
    """"""
    Always decide not to serialize obj

    :param priv: private data for this function first unit of this class
    :return: tuple (do serialize this object, next priv)
    """"""
    if '<MASK>':
        prepareEntity(obj, parentUnit.__class__.__name__, priv)
    if '<MASK>':
        priv = parentUnit
    return (False, priv)",False,"['isDeclaration', 'priv is None']",",,,,,,,,,,,,,,,,,,,",0.0
"def feature_selection(feat_select, X, y):
    """""""" Implements various kinds of feature selection """"""
    if '<MASK>':
        n = int(feat_select.split('-')[0])
        selector = SelectKBest(k=n)
        import warnings
        with warnings.catch_warnings():
            warnings.simplefilter('ignore', category=UserWarning)
            features_selected = np.where(selector.fit(X, y).get_support() is True)[0]
    elif '<MASK>':
        n = int(feat_select.split('-')[0])
        from random import shuffle
        features = range(0, X.shape[1])
        shuffle(features)
        features_selected = features[:n]
    return features_selected",False,"[""re.match('.*-best', feat_select) is not None"", ""re.match('.*-randombest', feat_select) is not None""]",___________________,0.0
"def start_svg(self):
    """"""Base SVG Document Creation""""""
    SVG_NAMESPACE = 'http://www.w3.org/2000/svg'
    SVG = '{%s}' % SVG_NAMESPACE
    NSMAP = {None: SVG_NAMESPACE, 'xlink': 'http://www.w3.org/1999/xlink', 'a3': 'http://ns.adobe.com/AdobeSVGViewerExtensions/3.0/'}
    root_attrs = self._get_root_attributes()
    self.root = etree.Element(SVG + 'svg', attrib=root_attrs, nsmap=NSMAP)
    if '<MASK>':
        pi = etree.ProcessingInstruction('xml-stylesheet', 'href=""%s"" type=""text/css""' % self.style_sheet_href)
        self.root.addprevious(pi)
    comment_strings = (' Created with SVG.Graph ', ' SVG.Graph by Jason R. Coombs ', ' Based on SVG::Graph by Sean E. Russel ', ' Based on Perl SVG:TT:Graph by Leo Lapworth & Stephan Morgan ', ' ' + '/' * 66)
    list(map(self.root.append, map(etree.Comment, comment_strings)))
    defs = etree.SubElement(self.root, 'defs')
    self.add_defs(defs)
    if '<MASK>':
        self.root.append(etree.Comment(' include default stylesheet if none specified '))
        style = etree.SubElement(defs, 'style', type='text/css')
        style.text = self.get_stylesheet().cssText
    self.root.append(etree.Comment('SVG Background'))
    etree.SubElement(self.root, 'rect', {'width': str(self.width), 'height': str(self.height), 'x': '0', 'y': '0', 'class': 'svgBackground'})",False,"[""hasattr(self, 'style_sheet_href')"", ""not hasattr(self, 'style_sheet_href') and (not self.css_inline)""]",___________________,0.0
"def vminiFromSpt(spt):
    """"""
  Obtain (V-I) for the input spectral type.

  Parameters
  ----------
  
  spt - String representing the spectral type of the star.

  Returns
  -------
  
  The value of (V-I).
  """"""
    if '<MASK>':
        return _sptToVminiVabsDictionary[spt][0]
    else:
        message = 'Unknown spectral type. Allowed values are: '
        for key in _sptToVminiVabsDictionary.keys():
            message += key + ' '
        raise Exception(message)",False,['spt in _sptToVminiVabsDictionary'],___________________,0.0
"def list_templates(self, name=None):
    """"""list templates in the builder bundle library. If a name is provided,
       look it up

       Parameters
       ==========
       name: the name of a template to look up
    """"""
    configs = self._get_templates()
    rows = []
    if '<MASK>':
        matches = self._load_templates(name)
        bot.debug('Found %s matches for %s' % (len(matches), name))
        for match in matches:
            print(json.dumps(match, indent=4, sort_keys=True))
    else:
        for config in configs['data']:
            rows.append([config['name']])
        bot.table(rows)",False,['name'],(((((((((((((((((((,0.0
"def default_table(self, layout, table_content, cols_width):
    """"""format a table""""""
    cols_width = [size + 1 for size in cols_width]
    format_strings = ' '.join(['%%-%ss'] * len(cols_width))
    format_strings = format_strings % tuple(cols_width)
    format_strings = format_strings.split(' ')
    table_linesep = '\n+' + '+'.join(['-' * w for w in cols_width]) + '+\n'
    headsep = '\n+' + '+'.join(['=' * w for w in cols_width]) + '+\n'
    self.write(table_linesep)
    for index, line in enumerate(table_content):
        self.write('|')
        for line_index, at_index in enumerate(line):
            self.write(format_strings[line_index] % at_index)
            self.write('|')
        if '<MASK>':
            self.write(headsep)
        else:
            self.write(table_linesep)",False,['index == 0 and layout.rheaders'],___________________,0.0
"def file_url(self):
    """"""
        Manage the case that we have to test a file

        .. note::
            1 URL per line.
        """"""
    list_to_test = self._file_list_to_test_filtering()
    not_filtered = list_to_test
    try:
        list_to_test = List(list(set(list_to_test[PyFunceble.INTERN['counter']['number']['tested']:]) - set(PyFunceble.INTERN['flatten_inactive_db']))).format()
        _ = list_to_test[-1]
    except IndexError:
        list_to_test = not_filtered[PyFunceble.INTERN['counter']['number']['tested']:]
        del not_filtered
    if '<MASK>':
        list_to_test = List(list(list_to_test)).custom_format(Sort.hierarchical)
    try:
        return [self.url(x, list_to_test[-1]) for x in list_to_test if x]
    except IndexError:
        print(PyFunceble.Fore.CYAN + PyFunceble.Style.BRIGHT + 'Nothing to test.')",False,"[""PyFunceble.CONFIGURATION['hierarchical_sorting']""]",___________________,0.0
"def validate_config(cls, service_config, target):
    """""" Validate generic options for a particular target """"""
    if '<MASK>':
        die(""[%s] has an 'only_if_assigned' option.  Should be '%s.only_if_assigned'."" % (target, cls.CONFIG_PREFIX))
    if '<MASK>':
        die(""[%s] has an 'also_unassigned' option.  Should be '%s.also_unassigned'."" % (target, cls.CONFIG_PREFIX))
    if '<MASK>':
        die(""[%s] has a 'default_priority' option.  Should be '%s.default_priority'."" % (target, cls.CONFIG_PREFIX))
    if '<MASK>':
        die(""[%s] has an 'add_tags' option.  Should be '%s.add_tags'."" % (target, cls.CONFIG_PREFIX))",False,"[""service_config.has_option(target, 'only_if_assigned')"", ""service_config.has_option(target, 'also_unassigned')"", ""service_config.has_option(target, 'default_priority')"", ""service_config.has_option(target, 'add_tags')""]",___________________,0.0
"def user(self, id):
    """"""
        Retrieve public information about a user.

        Args:
            id (str): user identifier to lookup

        Returns:
            SkypeUser: resulting user object
        """"""
    json = self.skype.conn('POST', '{0}/batch/profiles'.format(SkypeConnection.API_PROFILE), auth=SkypeConnection.Auth.SkypeToken, json={'usernames': [id]}).json()
    if '<MASK>':
        return self.merge(SkypeUser.fromRaw(self.skype, json[0]))
    else:
        return None",False,"[""json and 'status' not in json[0]""]",(((((((((((((((((((,0.0
"def url(self, sitetree_item, context=None):
    """"""Resolves item's URL.

        :param TreeItemBase sitetree_item: TreeItemBase heir object, 'url' property of which
            is processed as URL pattern or simple URL.

        :param Context context:

        :rtype: str|unicode
        """"""
    context = context or self.current_page_context
    resolve_var = self.resolve_var
    if '<MASK>':
        sitetree_item = resolve_var(sitetree_item, context)
    resolved_url = self._items_urls.get(sitetree_item)
    if '<MASK>':
        return resolved_url
    if '<MASK>':
        url = sitetree_item.url
        view_path = url
        all_arguments = []
        if '<MASK>':
            view_path = url.split(' ')
            for view_argument in view_path[1:]:
                resolved = resolve_var(view_argument)
                all_arguments.append('""%s""' % resolved)
            view_path = view_path[0].strip('""\' ')
        url_pattern = ""'%s' %s"" % (view_path, ' '.join(all_arguments))
    else:
        url_pattern = '%s' % sitetree_item.url
    if '<MASK>':
        url_token = 'url %s as item.url_resolved' % url_pattern
        url_tag(Parser(None), Token(token_type=TOKEN_BLOCK, contents=url_token)).render(context)
        resolved_url = context['item.url_resolved'] or UNRESOLVED_ITEM_MARKER
    else:
        resolved_url = url_pattern
    self._items_urls[sitetree_item] = resolved_url
    return resolved_url",False,"['not isinstance(sitetree_item, MODEL_TREE_ITEM_CLASS)', 'resolved_url is not None', 'sitetree_item.urlaspattern', 'sitetree_item.urlaspattern', ""' ' in url""]",___________________,0.0
"def _parse_validators(valids):
    """"""Parse a list of validator names or n-tuples, checking for errors.

    Returns:
        list((func_name, [args...])): A list of validator function names and a
            potentially empty list of optional parameters for each function.
    """"""
    outvals = []
    for val in valids:
        if '<MASK>':
            args = []
        elif '<MASK>':
            args = val[1:]
            val = val[0]
        else:
            raise ValidationError('You must pass either an n-tuple or a string to define a validator', validator=val)
        name = 'validate_%s' % str(val)
        outvals.append((name, args))
    return outvals",False,"['isinstance(val, str)', 'len(val) > 1']",___________________,0.0
"def ranking_metric_tensor(exprs, method, permutation_num, pos, neg, classes, ascending, rs=np.random.RandomState()):
    """"""Build shuffled ranking matrix when permutation_type eq to phenotype.

       :param exprs:   gene_expression DataFrame, gene_name indexed.
       :param str method:  calculate correlation or ranking. methods including:
                           1. 'signal_to_noise'.
                           2. 't_test'.
                           3. 'ratio_of_classes' (also referred to as fold change).
                           4. 'diff_of_classes'.
                           5. 'log2_ratio_of_classes'.
       :param int permuation_num: how many times of classes is being shuffled
       :param str pos: one of labels of phenotype's names.
       :param str neg: one of labels of phenotype's names.
       :param list classes:  a list of phenotype labels, to specify which column of
                             dataframe belongs to what class of phenotype.
       :param bool ascending:  bool. Sort ascending vs. descending.

       :return:
                returns two 2d ndarray with shape (nperm, gene_num).

                | cor_mat_indices: the indices of sorted and permutated (exclude last row) ranking matrix.
                | cor_mat: sorted and permutated (exclude last row) ranking matrix.

    """"""
    G, S = exprs.shape
    expr_mat = exprs.values.T
    perm_cor_tensor = np.tile(expr_mat, (permutation_num + 1, 1, 1))
    for arr in perm_cor_tensor[:-1]:
        rs.shuffle(arr)
    classes = np.array(classes)
    pos = classes == pos
    neg = classes == neg
    pos_cor_mean = perm_cor_tensor[:, pos, :].mean(axis=1)
    neg_cor_mean = perm_cor_tensor[:, neg, :].mean(axis=1)
    pos_cor_std = perm_cor_tensor[:, pos, :].std(axis=1, ddof=1)
    neg_cor_std = perm_cor_tensor[:, neg, :].std(axis=1, ddof=1)
    if '<MASK>':
        cor_mat = (pos_cor_mean - neg_cor_mean) / (pos_cor_std + neg_cor_std)
    elif '<MASK>':
        denom = 1.0 / G
        cor_mat = (pos_cor_mean - neg_cor_mean) / np.sqrt(denom * pos_cor_std ** 2 + denom * neg_cor_std ** 2)
    elif '<MASK>':
        cor_mat = pos_cor_mean / neg_cor_mean
    elif '<MASK>':
        cor_mat = pos_cor_mean - neg_cor_mean
    elif '<MASK>':
        cor_mat = np.log2(pos_cor_mean / neg_cor_mean)
    else:
        logging.error('Please provide correct method name!!!')
        sys.exit(0)
    cor_mat_ind = cor_mat.argsort()
    cor_mat.sort()
    if '<MASK>':
        return (cor_mat_ind, cor_mat)
    return (cor_mat_ind[:, ::-1], cor_mat[:, ::-1])",False,"[""method == 'signal_to_noise'"", 'ascending', ""method == 't_test'"", ""method == 'ratio_of_classes'"", ""method == 'diff_of_classes'"", ""method == 'log2_ratio_of_classes'""]",___________________,0.0
"def add_registry(self, registry, registry_user, registry_pass, insecure=False, registry_type='docker_v2', validate=True):
    """"""**Description**
            Add image registry

        **Arguments**
            - registry: Full hostname/port of registry. Eg. myrepo.example.com:5000
            - registry_user: Username
            - registry_pass: Password
            - insecure: Allow connection to registry without SSL cert checks (ex: if registry uses a self-signed SSL certificate)
            - registry_type: Specify the registry type. 'docker_v2' and 'awsecr' are supported (default='docker_v2')
            - validate: If set to 'False' will not attempt to validate registry/creds on registry add

        **Success Return Value**
            A JSON object representing the registry.
        """"""
    registry_types = ['docker_v2', 'awsecr']
    if '<MASK>':
        return [False, 'input registry type not supported (supported registry_types: ' + str(registry_types)]
    if '<MASK>':
        return [False, ""input registry name cannot contain '/' characters - valid registry names are of the form <host>:<port> where :<port> is optional""]
    if '<MASK>':
        registry_type = self._get_registry_type(registry)
    payload = {'registry': registry, 'registry_user': registry_user, 'registry_pass': registry_pass, 'registry_type': registry_type, 'registry_verify': not insecure}
    url = '{base_url}/api/scanning/v1/anchore/registries?validate={validate}'.format(base_url=self.url, validate=validate)
    res = requests.post(url, data=json.dumps(payload), headers=self.hdrs, verify=self.ssl_verify)
    if '<MASK>':
        return [False, self.lasterr]
    return [True, res.json()]",False,"['registry_type and registry_type not in registry_types', 'self._registry_string_is_valid(registry)', 'not registry_type', 'not self._checkResponse(res)']",___________________,0.0
"def _pool_single_run(kwargs):
    """"""Starts a pool single run and passes the storage service""""""
    wrap_mode = kwargs['wrap_mode']
    traj = kwargs['traj']
    traj.v_storage_service = _pool_single_run.storage_service
    if '<MASK>':
        traj.v_storage_service.free_references()
    return _sigint_handling_single_run(kwargs)",False,['wrap_mode == pypetconstants.WRAP_MODE_LOCAL'],___________________,0.0
"def complete_token_filtered_with_next(aliases, prefix, expanded, commands):
    """"""Find all starting matches in dictionary *aliases* that start
    with *prefix*, but filter out any matches already in
    *expanded*.""""""
    complete_ary = list(aliases.keys())
    expanded_ary = list(expanded.keys())
    result = []
    for cmd in complete_ary:
        if '<MASK>':
            if '<MASK>':
                result.append([cmd, aliases[cmd]])
            pass
        pass
    pass
    return sorted(result, key=lambda pair: pair[0])",False,"['cmd.startswith(prefix)', 'cmd in aliases and 0 == len(set(expanded_ary) - set([aliases[cmd]]))']",___________________,0.0
"def _get_new_access_information(self):
    """"""
		Request new access information from reddit using the built in webserver
		""""""
    if '<MASK>':
        self._log('Cannot obtain authorize url from PRAW. Please check your configuration.', logging.ERROR)
        raise AttributeError('Reddit Session invalid, please check your designated config file.')
    url = self.r.get_authorize_url('UsingOAuth2Util', self._get_value(CONFIGKEY_SCOPE, set, split_val=','), self._get_value(CONFIGKEY_REFRESHABLE, as_boolean=True))
    self._start_webserver(url)
    if '<MASK>':
        webbrowser.open(url)
    else:
        print('Webserver is waiting for you :D. Please open {0}:{1}/{2} in your browser'.format(SERVER_URL, SERVER_PORT, SERVER_LINK_PATH))
    self._wait_for_response()
    try:
        access_information = self.r.get_access_information(self.server.response_code)
    except praw.errors.OAuthException:
        self._log('Can not authenticate, maybe the app infos (e.g. secret) are wrong.', logging.ERROR)
        raise
    self._change_value(CONFIGKEY_TOKEN, access_information['access_token'])
    self._change_value(CONFIGKEY_REFRESH_TOKEN, access_information['refresh_token'])
    self._change_value(CONFIGKEY_VALID_UNTIL, time.time() + TOKEN_VALID_DURATION)",False,"['not self.r.has_oauth_app_info', 'not self._get_value(CONFIGKEY_SERVER_MODE, as_boolean=True)']",___________________,0.0
"def reinterptet_harray_to_bits(typeFrom, sigOrVal, bitsT):
    """"""
    Cast HArray signal or value to signal or value of type Bits
    """"""
    size = int(typeFrom.size)
    widthOfElm = typeFrom.elmType.bit_length()
    w = bitsT.bit_length()
    if '<MASK>':
        raise TypeConversionErr('Size of types is different', size * widthOfElm, w)
    partT = Bits(widthOfElm)
    parts = [p._reinterpret_cast(partT) for p in sigOrVal]
    return Concat(*reversed(parts))._reinterpret_cast(bitsT)",False,['size * widthOfElm != w'],___________________,0.0
"def remove_key(self, key_to_remove):
    """"""
        Remove a given key from a given dictionary.

        :param key_to_remove: The key(s) to delete.
        :type key_to_remove: list|str

        :return: The dict without the given key(s).
        :rtype: dict|None
        """"""
    if '<MASK>':
        if '<MASK>':
            for key in key_to_remove:
                del self.main_dictionnary[key]
        else:
            try:
                del self.main_dictionnary[key_to_remove]
            except KeyError:
                pass
        return self.main_dictionnary
    return None",False,"['isinstance(self.main_dictionnary, dict)', 'isinstance(key_to_remove, list)']","_,_,_,_,_,_,_,_,_,_",0.0
"def _setup_freqs(self):
    """"""Updating frequency borders from channel values
        """"""
    if '<MASK>':
        self.f_start = self.f_begin + self.chan_start_idx * abs(self.header[b'foff'])
        self.f_stop = self.f_begin + self.chan_stop_idx * abs(self.header[b'foff'])
    else:
        self.f_start = self.f_end - self.chan_stop_idx * abs(self.header[b'foff'])
        self.f_stop = self.f_end - self.chan_start_idx * abs(self.header[b'foff'])",False,"[""self.header[b'foff'] > 0""]",___________________,0.0
"def make_fuzzy(word, max=1):
    """"""Naive neighborhoods algo.""""""
    neighbors = []
    for i in range(0, len(word) - 1):
        neighbor = list(word)
        neighbor[i], neighbor[i + 1] = (neighbor[i + 1], neighbor[i])
        neighbors.append(''.join(neighbor))
    for letter in string.ascii_lowercase:
        for i in range(0, len(word)):
            neighbor = list(word)
            if '<MASK>':
                neighbor[i] = letter
                neighbors.append(''.join(neighbor))
    for letter in string.ascii_lowercase:
        for i in range(0, len(word) + 1):
            neighbor = list(word)
            neighbor.insert(i, letter)
            neighbors.append(''.join(neighbor))
    if '<MASK>':
        for i in range(0, len(word)):
            neighbor = list(word)
            del neighbor[i]
            neighbors.append(''.join(neighbor))
    return neighbors",False,"['len(word) > 3', 'letter != neighbor[i]']","(())
        """""" """"""..)
        """"""..))
",0.0
"def resize_width(image, size, resample=Image.LANCZOS):
    """"""
    Resize image according to size.
    image:      a Pillow image instance
    size:       an integer or a list or tuple of two integers [width, height]
    """"""
    try:
        width = size[0]
    except:
        width = size
    img_format = image.format
    img = image.copy()
    img_size = img.size
    if '<MASK>':
        return image
    new_height = int(math.ceil(width / img_size[0] * img_size[1]))
    img.thumbnail((width, new_height), resample)
    img.format = img_format
    return img",False,['img_size[0] == width'],___________________,0.0
"def create_endpoint(port=0, service_name='unknown', ipv4=None, ipv6=None):
    """"""Create a zipkin Endpoint object.

    An Endpoint object holds information about the network context of a span.

    :param port: int value of the port. Defaults to 0
    :param service_name: service name as a str. Defaults to 'unknown'
    :param ipv4: ipv4 host address
    :param ipv6: ipv6 host address
    :returns: thrift Endpoint object
    """"""
    ipv4_int = 0
    ipv6_binary = None
    if '<MASK>':
        ipv4_int = struct.unpack('!i', socket.inet_pton(socket.AF_INET, ipv4))[0]
    if '<MASK>':
        ipv6_binary = socket.inet_pton(socket.AF_INET6, ipv6)
    port = struct.unpack('h', struct.pack('H', port))[0]
    return zipkin_core.Endpoint(ipv4=ipv4_int, ipv6=ipv6_binary, port=port, service_name=service_name)",False,"['ipv4', 'ipv6']",___________________,0.0
"def _ask_for_credentials():
    """"""
    Asks the user for their email and password.
    """"""
    _print_msg('Please enter your SolveBio credentials')
    domain = raw_input('Domain (e.g. <domain>.solvebio.com): ')
    try:
        account = client.request('get', '/p/accounts/{}'.format(domain))
        auth = account['authentication']
    except:
        raise SolveError('Invalid domain: {}'.format(domain))
    if '<MASK>':
        email = raw_input('Email: ')
        password = getpass.getpass('Password (typing will be hidden): ')
        return (domain, email, password)
    else:
        _print_msg('Your domain uses Single Sign-On (SSO). Please visit https://{}.solvebio.com/settings/security for instructions on how to log in.'.format(domain))
        sys.exit(1)",False,"[""auth.get('login') or auth.get('SAML', {}).get('simple_login')""]",___________________,0.0
"def resolve_implicit_levels(storage, debug):
    """"""Resolving implicit levels (I1, I2)

    See: http://unicode.org/reports/tr9/#Resolving_Implicit_Levels

    """"""
    for run in storage['runs']:
        start, length = (run['start'], run['length'])
        chars = storage['chars'][start:start + length]
        for _ch in chars:
            assert _ch['type'] in ('L', 'R', 'EN', 'AN'), '%s not allowed here' % _ch['type']
            if '<MASK>':
                if '<MASK>':
                    _ch['level'] += 1
                elif '<MASK>':
                    _ch['level'] += 2
            elif '<MASK>':
                _ch['level'] += 1
    if '<MASK>':
        debug_storage(storage, runs=True)",False,"['debug', ""_embedding_direction(_ch['level']) == 'L'"", ""_ch['type'] == 'R'"", ""_ch['type'] != 'R'"", ""_ch['type'] != 'L'""]",___________________,0.0
"def decode(self, packet):
    """"""
        Decode a CONNECT control packet. 
        """"""
    self.encoded = packet
    lenLen = 1
    while packet[lenLen] & 128:
        lenLen += 1
    packet_remaining = packet[lenLen + 1:]
    version_str, packet_remaining = decodeString(packet_remaining)
    version_id = int(packet_remaining[0])
    if '<MASK>':
        self.version = v31
    else:
        self.version = v311
    flags = packet_remaining[1]
    self.cleanStart = flags & 2 != 0
    willFlag = flags & 4 != 0
    willQoS = flags >> 3 & 3
    willRetain = flags & 32 != 0
    userFlag = flags & 128 != 0
    passFlag = flags & 64 != 0
    packet_remaining = packet_remaining[2:]
    self.keepalive = decode16Int(packet_remaining)
    packet_remaining = packet_remaining[2:]
    self.clientId, packet_remaining = decodeString(packet_remaining)
    if '<MASK>':
        self.willRetain = willRetain
        self.willQoS = willQoS
        self.willTopic, packet_remaining = decodeString(packet_remaining)
        self.willMessage, packet_remaining = decodeString(packet_remaining)
    if '<MASK>':
        self.username, packet_remaining = decodeString(packet_remaining)
    if '<MASK>':
        l = decode16Int(packet_remaining)
        self.password = packet_remaining[2:2 + l]",False,"[""version_id == v31['level']"", 'willFlag', 'userFlag', 'passFlag']",___________________,0.0
"def request(self, method, url, erc, **kwargs):
    """"""Abstract base method for making requests to the Webex Teams APIs.

        This base method:
            * Expands the API endpoint URL to an absolute URL
            * Makes the actual HTTP request to the API endpoint
            * Provides support for Webex Teams rate-limiting
            * Inspects response codes and raises exceptions as appropriate

        Args:
            method(basestring): The request-method type ('GET', 'POST', etc.).
            url(basestring): The URL of the API endpoint to be called.
            erc(int): The expected response code that should be returned by the
                Webex Teams API endpoint to indicate success.
            **kwargs: Passed on to the requests package.

        Raises:
            ApiError: If anything other than the expected response code is
                returned by the Webex Teams API endpoint.

        """"""
    abs_url = self.abs_url(url)
    kwargs.setdefault('timeout', self.single_request_timeout)
    while True:
        response = self._req_session.request(method, abs_url, **kwargs)
        try:
            check_response_code(response, erc)
        except RateLimitError as e:
            if '<MASK>':
                warnings.warn(RateLimitWarning(response))
                time.sleep(e.retry_after)
                continue
            else:
                raise
        else:
            return response",False,['self.wait_on_rate_limit'],___________________,0.0
"def add_peddy_information(config_data):
    """"""Add information from peddy outfiles to the individuals""""""
    ped_info = {}
    ped_check = {}
    sex_check = {}
    relations = []
    if '<MASK>':
        file_handle = open(config_data['peddy_ped'], 'r')
        for ind_info in parse_peddy_ped(file_handle):
            ped_info[ind_info['sample_id']] = ind_info
    if '<MASK>':
        file_handle = open(config_data['peddy_ped_check'], 'r')
        for pair_info in parse_peddy_ped_check(file_handle):
            ped_check[pair_info['sample_a'], pair_info['sample_b']] = pair_info
    if '<MASK>':
        file_handle = open(config_data['peddy_sex_check'], 'r')
        for ind_info in parse_peddy_sex_check(file_handle):
            sex_check[ind_info['sample_id']] = ind_info
    if '<MASK>':
        return
    analysis_inds = {}
    for ind in config_data['samples']:
        ind_id = ind['sample_id']
        analysis_inds[ind_id] = ind
    for ind_id in analysis_inds:
        ind = analysis_inds[ind_id]
        if '<MASK>':
            ind['predicted_ancestry'] = ped_info[ind_id].get('ancestry-prediction', 'UNKNOWN')
        if '<MASK>':
            if '<MASK>':
                ind['confirmed_sex'] = False
            else:
                ind['confirmed_sex'] = True
        for parent in ['mother', 'father']:
            if '<MASK>':
                for pair in ped_check:
                    if '<MASK>':
                        if '<MASK>':
                            analysis_inds[ind[parent]]['confirmed_parent'] = False
                        elif '<MASK>':
                            analysis_inds[ind[parent]]['confirmed_parent'] = True",False,"[""config_data.get('peddy_ped')"", ""config_data.get('peddy_ped_check')"", ""config_data.get('peddy_sex_check')"", 'not ped_info', 'ind_id in ped_info', 'ind_id in sex_check', ""sex_check[ind_id]['error']"", ""ind[parent] != '0'"", 'ind_id in pair and ind[parent] in pair', ""ped_check[pair]['parent_error']"", ""'confirmed_parent' not in analysis_inds[ind[parent]]""]",___________________,0.0
"def resolve_address_location(proc, location):
    """"""Expand fields in Location namedtuple. If:
       '.':  get fields from stack
       function/module: get fields from evaluation/introspection
       location file and line number: use that
    """"""
    curframe = proc.curframe
    if '<MASK>':
        filename = Mstack.frame2file(proc.core, curframe, canonic=False)
        offset = curframe.f_lasti
        is_address = True
        return Location(filename, offset, False, None)
    assert isinstance(location, Location)
    is_address = True
    if '<MASK>':
        g = curframe.f_globals
        l = curframe.f_locals
    else:
        g = globals()
        l = locals()
        pass
    if '<MASK>':
        filename = offset = None
        msg = 'Object %s is not known yet as a function, ' % location.method
        try:
            modfunc = eval(location.method, g, l)
        except:
            proc.errmsg(msg)
            return INVALID_LOCATION
        try:
            if '<MASK>':
                pass
            else:
                proc.errmsg(msg)
                return INVALID_LOCATION
        except:
            proc.errmsg(msg)
            return INVALID_LOCATION
        filename = proc.core.canonic(modfunc.func_code.co_filename)
        offset = 0
    elif '<MASK>':
        filename = proc.core.canonic(location.path)
        offset = location.line_number
        is_address = location.is_address
        modfunc = None
        msg = '%s is not known as a file' % location.path
        if '<MASK>':
            try:
                modfunc = eval(location.path, g, l)
            except:
                msg = ""Don't see '%s' as a existing file or as an module"" % location.path
                proc.errmsg(msg)
                return INVALID_LOCATION
            pass
            is_address = location.is_address
            if '<MASK>':
                if '<MASK>':
                    filename = pyficache.pyc2py(modfunc.__file__)
                    filename = proc.core.canonic(filename)
                    if '<MASK>':
                        offset = 0
                        is_address = True
                    return Location(filename, offset, is_address, modfunc)
                else:
                    msg = ""module '%s' doesn't have a file associated with it"" % location.path
            proc.errmsg(msg)
            return INVALID_LOCATION
        maxline = pyficache.maxline(filename)
        if '<MASK>':
            proc.errmsg('Line number %d out of range; %s has %d lines.' % (offset, filename, maxline))
            return INVALID_LOCATION
    elif '<MASK>':
        filename = Mstack.frame2file(proc.core, curframe, canonic=False)
        offset = location.line_number
        is_address = location.is_address
        modfunc = proc.list_object
    return Location(filename, offset, is_address, modfunc)",False,"[""location == '.'"", 'proc.curframe', 'location.method', 'location.path', ""inspect.isfunction(modfunc) or hasattr(modfunc, 'im_func')"", 'not osp.isfile(filename)', 'maxline and offset > maxline', 'location.line_number is not None', 'inspect.ismodule(modfunc)', ""hasattr(modfunc, '__file__')"", 'not offset']",___________________,0.0
"def build(self, pre=None, shortest=False):
    """"""Build the ``And`` instance

        :param list pre: The prerequisites list
        :param bool shortest: Whether or not the shortest reference-chain (most minimal) version of the field should be generated.
        """"""
    if '<MASK>':
        pre = []
    res = deque()
    for x in self.values:
        try:
            res.append(utils.val(x, pre, shortest=shortest))
        except errors.OptGram as e:
            continue
        except errors.FlushGrams as e:
            prev = ''.join(res)
            res.clear()
            if '<MASK>':
                pre.append(prev)
            else:
                stmts = self.fuzzer._curr_scope.setdefault('prev_append', deque())
                stmts.extend(pre)
                stmts.append(prev)
                pre.clear()
            continue
    return self.sep.join(res)",False,"['pre is None', 'len(self.fuzzer._scope_stack) == 1']",(((((((((((((((((((,0.0
"def onMessage(self, payload, isBinary):
    """"""
        Send the payload onto the {slack.[payload['type]'} channel.
        The message is transalated from IDs to human-readable identifiers.

        Note: The slack API only sends JSON, isBinary will always be false.
        """"""
    msg = self.translate(unpack(payload))
    if '<MASK>':
        channel_name = 'slack.{}'.format(msg['type'])
        print('Sending on {}'.format(channel_name))
        channels.Channel(channel_name).send({'text': pack(msg)})",False,"[""'type' in msg""]","_name_name_name)
        """"""
        """"""
        """"""
        """"""",0.0
"def save_token(self, token_file=None):
    """"""
        Obtain the user's long-lived API token and save it in a local file.
        If the user has no long-lived API token, one will be created.
        Returns the token that was saved.
        """"""
    tokens = self._json_request('get', self.root_url + '/user/tokens/')
    long_lived = [token['type'] == 'long_lived' for token in tokens]
    if '<MASK>':
        dic = tokens[long_lived.index(True)]
    else:
        dic = self._json_request('post', self.root_url + '/user/tokens/')
    token = dic['token']
    token_file = token_file or get_token_filename()
    if '<MASK>':
        saved_tokens = json.load(open(token_file))
    else:
        saved_tokens = {}
    saved_tokens[urlparse(self.root_url).netloc] = token
    directory, filename = os.path.split(token_file)
    if '<MASK>':
        os.makedirs(directory)
    with open(token_file, 'w') as f:
        json.dump(saved_tokens, f)
    return token",False,"['any(long_lived)', 'os.path.exists(token_file)', 'directory and (not os.path.exists(directory))']",___________________,0.0
"def crscode_to_string(codetype, code, format):
    """"""
    Lookup crscode on spatialreference.org and return in specified format.

    Arguments:

    - *codetype*: ""epsg"", ""esri"", or ""sr-org"".
    - *code*: The code.
    - *format*: The crs format of the returned string. One of ""ogcwkt"", ""esriwkt"", or ""proj4"", but also several others...

    Returns:

    - Crs string in the specified format. 
    """"""
    link = 'http://spatialreference.org/ref/%s/%s/%s/' % (codetype, code, format)
    result = urllib2.urlopen(link).read()
    if '<MASK>':
        result = result.decode()
    return result",False,"['not isinstance(result, str)']",___________________,0.0
"def _format_lazy(value):
    """"""
    Expand a _(""TEST"") call to something meaningful.
    """"""
    args = value._proxy____args
    kw = value._proxy____kw
    if '<MASK>':
        return LiteralStr(u'ugettext_lazy({0})'.format(repr(value._proxy____args[0])))
    return value",False,"['not kw and len(args) == 1 and isinstance(args[0], six.string_types)']",___________________,0.0
"def _read_coll(ctx: ReaderContext, f: Callable[[Collection[Any]], Union[llist.List, lset.Set, vector.Vector]], end_token: str, coll_name: str):
    """"""Read a collection from the input stream and create the
    collection using f.""""""
    coll: List = []
    reader = ctx.reader
    while True:
        token = reader.peek()
        if '<MASK>':
            raise SyntaxError(f'Unexpected EOF in {coll_name}')
        if '<MASK>':
            reader.advance()
            continue
        if '<MASK>':
            reader.next_token()
            return f(coll)
        elem = _read_next(ctx)
        if '<MASK>':
            continue
        coll.append(elem)",False,"[""token == ''"", 'whitespace_chars.match(token)', 'token == end_token', 'elem is COMMENT']","______)
    """""".........",0.0
"def _call(self, method, params=None, request_id=None):
    """""" Calls the JSON-RPC endpoint. """"""
    params = params or []
    rid = request_id or self._id_counter
    if '<MASK>':
        self._id_counter += 1
    payload = {'jsonrpc': '2.0', 'method': method, 'params': params, 'id': rid}
    headers = {'Content-Type': 'application/json'}
    scheme = 'https' if self.tls else 'http'
    url = '{}://{}:{}'.format(scheme, self.host, self.port)
    try:
        response = self.session.post(url, headers=headers, data=json.dumps(payload))
        response.raise_for_status()
    except HTTPError:
        raise TransportError('Got unsuccessful response from server (status code: {})'.format(response.status_code), response=response)
    try:
        response_data = response.json()
    except ValueError as e:
        raise ProtocolError('Unable to deserialize response body: {}'.format(e), response=response)
    if '<MASK>':
        code = response_data['error'].get('code', '')
        message = response_data['error'].get('message', '')
        raise ProtocolError('Error[{}] {}'.format(code, message), response=response, data=response_data)
    elif '<MASK>':
        raise ProtocolError('Response is empty (result field is missing)', response=response, data=response_data)
    return response_data['result']",False,"['request_id is None', ""response_data.get('error')"", ""'result' not in response_data""]",___________________,0.0
"def read_tdms(tdms_file):
    """"""Read tdms file and return channel names and data""""""
    tdms_file = nptdms.TdmsFile(tdms_file)
    ch_names = []
    ch_data = []
    for o in tdms_file.objects.values():
        if '<MASK>':
            chn = o.path.split('/')[-1].strip(""'"")
            if '<MASK>':
                unit = o.properties['unit_string']
                ch_names.append('{} [{}]'.format(chn, unit))
            else:
                ch_names.append(chn)
            ch_data.append(o.data)
    return (ch_names, ch_data)",False,"['o.data is not None and len(o.data)', ""'unit_string' in o.properties""]","_()
    """""" """""".)
    """""".)
    """""".)",0.0
"def cli(context, mongodb, username, password, authdb, host, port, loglevel, config, demo):
    """"""scout: manage interactions with a scout instance.""""""
    log_format = None
    coloredlogs.install(level=loglevel, fmt=log_format)
    LOG.info('Running scout version %s', __version__)
    LOG.debug('Debug logging enabled.')
    mongo_config = {}
    cli_config = {}
    if '<MASK>':
        LOG.debug('Use config file %s', config)
        with open(config, 'r') as in_handle:
            cli_config = yaml.load(in_handle)
    mongo_config['mongodb'] = mongodb or cli_config.get('mongodb') or 'scout'
    if '<MASK>':
        mongo_config['mongodb'] = 'scout-demo'
    mongo_config['host'] = host or cli_config.get('host') or 'localhost'
    mongo_config['port'] = port or cli_config.get('port') or 27017
    mongo_config['username'] = username or cli_config.get('username')
    mongo_config['password'] = password or cli_config.get('password')
    mongo_config['authdb'] = authdb or cli_config.get('authdb') or mongo_config['mongodb']
    mongo_config['omim_api_key'] = cli_config.get('omim_api_key')
    if '<MASK>':
        mongo_config['adapter'] = None
    else:
        LOG.info('Setting database name to %s', mongo_config['mongodb'])
        LOG.debug('Setting host to %s', mongo_config['host'])
        LOG.debug('Setting port to %s', mongo_config['port'])
        try:
            client = get_connection(**mongo_config)
        except ConnectionFailure:
            context.abort()
        database = client[mongo_config['mongodb']]
        LOG.info('Setting up a mongo adapter')
        mongo_config['client'] = client
        adapter = MongoAdapter(database)
        mongo_config['adapter'] = adapter
        LOG.info('Check if authenticated...')
        try:
            for ins_obj in adapter.institutes():
                pass
        except OperationFailure as err:
            LOG.info('User not authenticated')
            context.abort()
    context.obj = mongo_config",False,"['config', 'demo', ""context.invoked_subcommand in ('setup', 'serve')""]",_config_config_config_config_config_config_config_config_config_,0.0
"def visit_name(self, node):
    """"""check if the name handle an access to a class member
        if so, register it
        """"""
    if '<MASK>':
        self._meth_could_be_func = False",False,['self._first_attrs and (node.name == self._first_attrs[-1] or not self._first_attrs[-1])'],"_name(self,,,,,,,,,,,,,,,",0.0
"def set_max_in_flight(self, max_in_flight):
    """"""Dynamically adjust the reader max_in_flight. Set to 0 to immediately disable a Reader""""""
    assert isinstance(max_in_flight, int)
    self.max_in_flight = max_in_flight
    if '<MASK>':
        for conn in itervalues(self.conns):
            if '<MASK>':
                logger.debug('[%s:%s] rdy: %d -> 0', conn.id, self.name, conn.rdy)
                self._send_rdy(conn, 0)
        self.total_rdy = 0
    else:
        self.need_rdy_redistributed = True
        self._redistribute_rdy_state()",False,"['max_in_flight == 0', 'conn.rdy > 0']",___________________,0.0
"def _loadFromHStruct(self, dtype: HdlType, bitAddr: int):
    """"""
        Parse HStruct type to this transaction template instance

        :return: address of it's end
        """"""
    for f in dtype.fields:
        t = f.dtype
        origin = f
        isPadding = f.name is None
        if '<MASK>':
            width = t.bit_length()
            bitAddr += width
        else:
            fi = TransTmpl(t, bitAddr, parent=self, origin=origin)
            self.children.append(fi)
            bitAddr = fi.bitAddrEnd
    return bitAddr",False,['isPadding'],"_,,,,,,,,,,,,,,,,,,",0.0
"@wraps(f)
def decorated_function(*args, **kwargs):
    template_name = template
    if '<MASK>':
        template_name = request.endpoint.replace('.', '/') + '.html'
    context = f(*args, **kwargs)
    if '<MASK>':
        context = {}
    elif '<MASK>':
        return context
    return render_template(template_name, **context)",False,"['template_name is None', 'context is None', 'not isinstance(context, dict)']","_name,_name, **,, **,, **,, **,,,",0.0
"def do_fuzzyindex(self, word):
    """"""Compute fuzzy extensions of word that exist in index.
    FUZZYINDEX lilas""""""
    word = list(preprocess_query(word))[0]
    token = Token(word)
    neighbors = make_fuzzy(token)
    neighbors = [(n, DB.zcard(dbkeys.token_key(n))) for n in neighbors]
    neighbors.sort(key=lambda n: n[1], reverse=True)
    for token, freq in neighbors:
        if '<MASK>':
            break
        print(white(token), blue(freq))",False,['freq == 0'],_((((((((((((((((((,0.0
"def date_fixup_pre_processor(transactions, tag, tag_dict, *args):
    """"""
    Replace illegal February 29, 30 dates with the last day of February.

    German banks use a variant of the 30/360 interest rate calculation,
    where each month has always 30 days even February. Python's datetime
    module won't accept such dates.
    """"""
    if '<MASK>':
        year = int(tag_dict['year'], 10)
        _, max_month_day = calendar.monthrange(year, 2)
        if '<MASK>':
            tag_dict['day'] = str(max_month_day)
    return tag_dict",False,"[""tag_dict['month'] == '02'"", ""int(tag_dict['day'], 10) > max_month_day""]",_((((((((((((((((((,0.0
"def read_input_file(self, fn):
    """"""
        This method may be overridden to implement a custom lookup mechanism when
        encountering ``\\input`` or ``\\include`` directives.

        The default implementation looks for a file of the given name relative
        to the directory set by :py:meth:`set_tex_input_directory()`.  If
        `strict_input=True` was set, we ensure strictly that the file resides in
        a subtree of the reference input directory (after canonicalizing the
        paths and resolving all symlinks).

        You may override this method to obtain the input data in however way you
        see fit.  (In that case, a call to `set_tex_input_directory()` may not
        be needed as that function simply sets properties which are used by the
        default implementation of `read_input_file()`.)

        This function accepts the referred filename as argument (the argument to
        the ``\\input`` macro), and should return a string with the file
        contents (or generate a warning or raise an error).
        """"""
    fnfull = os.path.realpath(os.path.join(self.tex_input_directory, fn))
    if '<MASK>':
        dirfull = os.path.realpath(self.tex_input_directory)
        if '<MASK>':
            logger.warning(""Can't access path '%s' leading outside of mandated directory [strict input mode]"", fn)
            return ''
    if '<MASK>':
        fnfull = fnfull + '.tex'
    if '<MASK>':
        fnfull = fnfull + '.latex'
    if '<MASK>':
        logger.warning(u""Error, file doesn't exist: '%s'"", fn)
        return ''
    logger.debug('Reading input file %r', fnfull)
    try:
        with open(fnfull) as f:
            return f.read()
    except IOError as e:
        logger.warning(u""Error, can't access '%s': %s"", fn, e)
        return ''",False,"['self.strict_input', ""not os.path.exists(fnfull) and os.path.exists(fnfull + '.tex')"", ""not os.path.exists(fnfull) and os.path.exists(fnfull + '.latex')"", 'not os.path.isfile(fnfull)', 'not fnfull.startswith(dirfull)']",___________________,0.0
"def get_tmpdir(requested_tmpdir=None, prefix='', create=True):
    """"""get a temporary directory for an operation. If SREGISTRY_TMPDIR
       is set, return that. Otherwise, return the output of tempfile.mkdtemp

       Parameters
       ==========
       requested_tmpdir: an optional requested temporary directory, first
       priority as is coming from calling function.
       prefix: Given a need for a sandbox (or similar), we will need to 
       create a subfolder *within* the SREGISTRY_TMPDIR.
       create: boolean to determine if we should create folder (True)
    """"""
    from sregistry.defaults import SREGISTRY_TMPDIR
    tmpdir = requested_tmpdir or SREGISTRY_TMPDIR
    prefix = prefix or 'sregistry-tmp'
    prefix = '%s.%s' % (prefix, next(tempfile._get_candidate_names()))
    tmpdir = os.path.join(tmpdir, prefix)
    if '<MASK>':
        os.mkdir(tmpdir)
    return tmpdir",False,['not os.path.exists(tmpdir) and create is True'],___________________,0.0
"def plot_histogram(self, filename=None):
    """""" Plot a histogram of data values """"""
    header, data = self.read_next_data_block()
    data = data.view('float32')
    plt.figure('Histogram')
    plt.hist(data.flatten(), 65, facecolor='#cc0000')
    if '<MASK>':
        plt.savefig(filename)
    plt.show()",False,['filename'],___________________,0.0
"def clean_code(code, comments=True, macros=False, pragmas=False):
    """"""
    Naive comment and macro striping from source code

    :param comments: If True, all comments are stripped from code
    :param macros: If True, all macros are stripped from code
    :param pragmas: If True, all pragmas are stripped from code

    :return: cleaned code. Line numbers are preserved with blank lines,
    and multiline comments and macros are supported. BUT comment-like
    strings are (wrongfully) treated as comments.
    """"""
    if '<MASK>':
        lines = code.split('\n')
        in_macro = False
        in_pragma = False
        for i in range(len(lines)):
            l = lines[i].strip()
            if '<MASK>':
                lines[i] = ''
                in_macro = l.endswith('\\')
            if '<MASK>':
                lines[i] = ''
                in_pragma = l.endswith('\\')
        code = '\n'.join(lines)
    if '<MASK>':
        idx = 0
        comment_start = None
        while idx < len(code) - 1:
            if '<MASK>':
                end_idx = code.find('\n', idx)
                code = code[:idx] + code[end_idx:]
                idx -= end_idx - idx
            elif '<MASK>':
                comment_start = idx
            elif '<MASK>':
                code = code[:comment_start] + '\n' * code[comment_start:idx].count('\n') + code[idx + 2:]
                idx -= idx - comment_start
                comment_start = None
            idx += 1
    return code",False,"['macros or pragmas', 'comments', ""macros and (l.startswith('#') and (not l.startswith('#pragma')) or in_macro)"", ""pragmas and (l.startswith('#pragma') or in_pragma)"", ""comment_start is None and code[idx:idx + 2] == '//'"", ""comment_start is None and code[idx:idx + 2] == '/*'"", ""comment_start is not None and code[idx:idx + 2] == '*/'""]",___________________,0.0
"def convert_context_to_json(self, context):
    """"""
        Get what we want out of the context dict and convert that to a JSON
        object. Note that this does no object serialization b/c we're
        not sending any objects.
        """"""
    if '<MASK>':
        return dumps(self.get_month_calendar_dict(context))
    elif '<MASK>':
        return dumps(self.get_month_event_list_dict(context))
    elif '<MASK>':
        cal = self.get_month_calendar_dict(context)
        l = self.get_month_event_list_dict(context)
        cal.update(l)
        return dumps(cal)
    else:
        for key, val in context.items():
            if '<MASK>':
                context[key] = force_text(val)
        return dumps(self.get_day_context_dict(context))",False,"[""'month/shift' in self.request.path"", ""'event-list/shift' in self.request.path"", ""'cal-and-list/shift' in self.request.path"", 'isinstance(val, Promise)']",___________________,0.0
"def compile_and_exec_form(form: ReaderForm, ctx: CompilerContext, module: types.ModuleType, wrapped_fn_name: str=_DEFAULT_FN, collect_bytecode: Optional[BytecodeCollector]=None) -> Any:
    """"""Compile and execute the given form. This function will be most useful
    for the REPL and testing purposes. Returns the result of the executed expression.

    Callers may override the wrapped function name, which is used by the
    REPL to evaluate the result of an expression and print it back out.""""""
    if '<MASK>':
        return None
    if '<MASK>':
        _bootstrap_module(ctx.generator_context, ctx.py_ast_optimizer, module)
    final_wrapped_name = genname(wrapped_fn_name)
    lisp_ast = parse_ast(ctx.parser_context, form)
    py_ast = gen_py_ast(ctx.generator_context, lisp_ast)
    form_ast = list(map(_statementize, itertools.chain(py_ast.dependencies, [_expressionize(GeneratedPyAST(node=py_ast.node), final_wrapped_name)])))
    ast_module = ast.Module(body=form_ast)
    ast_module = ctx.py_ast_optimizer.visit(ast_module)
    ast.fix_missing_locations(ast_module)
    _emit_ast_string(ast_module)
    bytecode = compile(ast_module, ctx.filename, 'exec')
    if '<MASK>':
        collect_bytecode(bytecode)
    exec(bytecode, module.__dict__)
    return getattr(module, final_wrapped_name)()",False,"['form is None', 'not module.__basilisp_bootstrapped__', 'collect_bytecode']",___________________,0.0
"def upload(args):
    """"""
    Given a folder or file, upload all the folders and files contained
    within it, skipping ones that already exist on the remote.
    """"""
    base_remote_path, path_dict = Object.validate_full_path(args.full_path, vault=args.vault, path=args.path)
    vault = Vault.get_by_full_path(path_dict['vault_full_path'])
    if '<MASK>':
        Object.get_by_full_path(base_remote_path, assert_type='folder')
    for local_path in args.local_path:
        local_path = local_path.rstrip('/')
        local_start = os.path.basename(local_path)
        if '<MASK>':
            _upload_folder(path_dict['domain'], vault, base_remote_path, local_path, local_start)
        else:
            Object.upload_file(local_path, path_dict['path'], vault.full_path)",False,"[""path_dict['path'] != '/'"", 'os.path.isdir(local_path)']",___________________,0.0
"def eval(self, operator, simulator=None):
    """"""Load all operands and process them by self._evalFn""""""

    def getVal(v):
        while not isinstance(v, Value):
            v = v._val
        return v
    operands = list(map(getVal, operator.operands))
    if '<MASK>':
        operands.append(simulator.now)
    elif '<MASK>':
        operands.append(operator.result._dtype)
    return self._evalFn(*operands)",False,"['isEventDependentOp(operator.operator)', 'operator.operator == AllOps.IntToBits']",(((((((((((((((((((,0.0
"def next_token(self) -> str:
    """"""Advance the stream forward by one character and return the
        next token in the stream.""""""
    if '<MASK>':
        self._idx += 1
    else:
        c = self._stream.read(1)
        self._update_loc(c)
        self._buffer.append(c)
    return self.peek()",False,['self._idx < StreamReader.DEFAULT_INDEX'],(((((((((((((((((((,0.0
"def getMaxStmIdForStm(stm):
    """"""
    Get maximum _instId from all assigments in statement
    """"""
    maxId = 0
    if '<MASK>':
        return stm._instId
    elif '<MASK>':
        return maxId
    else:
        for _stm in stm._iter_stms():
            maxId = max(maxId, getMaxStmIdForStm(_stm))
        return maxId",False,"['isinstance(stm, Assignment)', 'isinstance(stm, WaitStm)']",___________________,0.0
"def wait(self):
    """"""Wait until there are no more inprogress transfers

        This will not stop when failures are encountered and not propogate any
        of these errors from failed transfers, but it can be interrupted with
        a KeyboardInterrupt.
        """"""
    try:
        transfer_coordinator = None
        for transfer_coordinator in self.tracked_transfer_coordinators:
            transfer_coordinator.result()
    except KeyboardInterrupt:
        logger.debug('Received KeyboardInterrupt in wait()')
        if '<MASK>':
            logger.debug('On KeyboardInterrupt was waiting for %s', transfer_coordinator)
        raise
    except Exception:
        pass",False,['transfer_coordinator'],___________________,0.0
"def get_global_option(checker, option, default=None):
    """""" Retrieve an option defined by the given *checker* or
    by all known option providers.

    It will look in the list of all options providers
    until the given *option* will be found.
    If the option wasn't found, the *default* value will be returned.
    """"""
    try:
        return getattr(checker.config, option.replace('-', '_'))
    except AttributeError:
        pass
    for provider in checker.linter.options_providers:
        for options in provider.options:
            if '<MASK>':
                return getattr(provider.config, option.replace('-', '_'))
    return default",False,['options[0] == option'],_((((((((((((((((((,0.0
"def build_upstream(self, process_descriptions, task, all_tasks, task_pipeline, count_forks, total_tasks, forks):
    """"""Builds the upstream pipeline of the current process

        Checks for the upstream processes to the current process and
        adds them to the current pipeline fragment if they were provided in
        the process list.

        Parameters
        ----------
        process_descriptions : dict
            Information of processes input, output and if is forkable
        task : str
            Current process
        all_tasks : list
            A list of all provided processes
        task_pipeline : list
            Current pipeline fragment
        count_forks : int
            Current number of forks
        total_tasks : str
            All space separated processes
        forks : list
            Current forks
        Returns
        -------
        list : resulting pipeline fragment
        """"""
    if '<MASK>':
        if '<MASK>':
            if '<MASK>':
                local_forks = process_descriptions[task][1].split('|')
                for local_fork in local_forks:
                    if '<MASK>':
                        count_forks += 1
                        task_pipeline.insert(0, process_descriptions[task][1])
                        self.define_pipeline_string(process_descriptions, local_fork, False, True, count_forks, total_tasks, forks)
                return task_pipeline
            else:
                if '<MASK>':
                    task_pipeline.insert(0, process_descriptions[task][1].split('|')[0])
                    self.build_upstream(process_descriptions, process_descriptions[task][1].split('|')[0], all_tasks, task_pipeline, count_forks, total_tasks, forks)
                else:
                    logger.error(colored_print('{} not in provided protocols as input for {}'.format(process_descriptions[task][1], task), 'red_bold'))
                    sys.exit()
                return task_pipeline
        else:
            return task_pipeline",False,"['task in process_descriptions', 'process_descriptions[task][1] is not None', ""len(process_descriptions[task][1].split('|')) > 1"", 'process_descriptions[task][1] in total_tasks', 'local_fork in total_tasks']","__)
                                                                                                         ",0.0
"def get_simulated_epw_path():
    """"""
    Returns
    -------
    None if epw can be anywhere
    """"""
    from oplus import CONF
    if '<MASK>':
        return os.path.join(CONF.eplus_base_dir_path, 'WeatherData', '%s.epw' % CONF.default_model_name)",False,"[""OS_NAME == 'windows'""]","__path,
    """"""
    """"""
    """"""
    """"""
    """"""",0.0
"def run(self):
    """"""Run.

        :raises BuildFailed: extension build failed and need to skip cython part.
        """"""
    try:
        build_ext.build_ext.run(self)
        build_dir = os.path.abspath(self.build_lib)
        root_dir = os.path.abspath(os.path.join(__file__, '..'))
        target_dir = build_dir if not self.inplace else root_dir
        src_file = os.path.join('advanced_descriptors', '__init__.py')
        src = os.path.join(root_dir, src_file)
        dst = os.path.join(target_dir, src_file)
        if '<MASK>':
            shutil.copyfile(src, dst)
    except (distutils.errors.DistutilsPlatformError, FileNotFoundError):
        raise BuildFailed()",False,['src != dst'],___________________,0.0
"def _onDeviceStatus(self, client, userdata, pahoMessage):
    """"""
        Internal callback for device status messages, parses source device from topic string and
        passes the information on to the registerd device status callback
        """"""
    try:
        status = Status(pahoMessage)
        self.logger.debug('Received %s action from %s' % (status.action, status.clientId))
        if '<MASK>':
            self.deviceStatusCallback(status)
    except InvalidEventException as e:
        self.logger.critical(str(e))",False,['self.deviceStatusCallback'],___________________,0.0
"def process(in_path, boundaries_id=msaf.config.default_bound_id, labels_id=msaf.config.default_label_id, annot_beats=False, framesync=False, feature='pcp', hier=False, save=False, out_file=None, n_jobs=4, annotator_id=0, config=None):
    """"""Main process to evaluate algorithms' results.

    Parameters
    ----------
    in_path : str
        Path to the dataset root folder.
    boundaries_id : str
        Boundaries algorithm identifier (e.g. siplca, cnmf)
    labels_id : str
        Labels algorithm identifier (e.g. siplca, cnmf)
    ds_name : str
        Name of the dataset to be evaluated (e.g. SALAMI). * stands for all.
    annot_beats : boolean
        Whether to use the annotated beats or not.
    framesync: str
        Whether to use framesync features or not (default: False -> beatsync)
    feature: str
        String representing the feature to be used (e.g. pcp, mfcc, tonnetz)
    hier : bool
        Whether to compute a hierarchical or flat segmentation.
    save: boolean
        Whether to save the results into the `out_file` csv file.
    out_file: str
        Path to the csv file to save the results (if `None` and `save = True`
        it will save the results in the default file name obtained by
        calling `get_results_file_name`).
    n_jobs: int
        Number of processes to run in parallel. Only available in collection
        mode.
    annotator_id : int
        Number identifiying the annotator.
    config: dict
        Dictionary containing custom configuration parameters for the
        algorithms.  If None, the default parameters are used.

    Return
    ------
    results : pd.DataFrame
        DataFrame containing the evaluations for each file.
    """"""
    if '<MASK>':
        config = io.get_configuration(feature, annot_beats, framesync, boundaries_id, labels_id)
    config['hier'] = hier
    config.pop('features', None)
    if '<MASK>':
        out_file = get_results_file_name(boundaries_id, labels_id, config, annotator_id)
    if '<MASK>':
        logging.warning('Results already exists, reading from file %s' % out_file)
        results = pd.read_csv(out_file)
        print_results(results)
        return results
    if '<MASK>':
        evals = [process_track(in_path, boundaries_id, labels_id, config, annotator_id=annotator_id)]
    else:
        file_structs = io.get_dataset_files(in_path)
        logging.info('Evaluating %d tracks...' % len(file_structs))
        evals = Parallel(n_jobs=n_jobs)((delayed(process_track)(file_struct, boundaries_id, labels_id, config, annotator_id=annotator_id) for file_struct in file_structs[:]))
    results = pd.DataFrame()
    for e in evals:
        if '<MASK>':
            results = results.append(e, ignore_index=True)
    logging.info('%d tracks analyzed' % len(results))
    print_results(results)
    if '<MASK>':
        logging.info('Writing results in %s' % out_file)
        results.to_csv(out_file)
    return results",False,"['config is None', 'out_file is None', 'os.path.exists(out_file)', 'os.path.isfile(in_path)', 'save', 'e != []']",___________________,0.0
"def match_delta(self, value):
    """"""Search for timedelta information in the string""""""
    m = self.REGEX_DELTA.search(value)
    delta = datetime.timedelta(days=0)
    if '<MASK>':
        d = int(m.group(1))
        if '<MASK>':
            d = -d
        if '<MASK>':
            delta = datetime.timedelta(minutes=d)
        elif '<MASK>':
            delta = datetime.timedelta(hours=d)
        elif '<MASK>':
            delta = datetime.timedelta(days=d)
        elif '<MASK>':
            delta = datetime.timedelta(weeks=d)
        value = self.REGEX_DELTA.sub('', value)
    return (delta, value)",False,"['m', ""m.group(3) == 'ago' or m.group(3) == 'before'"", ""m.group(2) == 'minute'"", ""m.group(2) == 'hour'"", ""m.group(2) == 'day'"", ""m.group(2) == 'week'""]",___________________,0.0
"def add_dashboard_panel(self, dashboard, name, panel_type, metrics, scope=None, sort_by=None, limit=None, layout=None):
    """"""**Description**
            Adds a panel to the dashboard. A panel can be a time series, or a top chart (i.e. bar chart), or a number panel.

        **Arguments**
            - **dashboard**: dashboard to edit
            - **name**: name of the new panel
            - **panel_type**: type of the new panel. Valid values are: ``timeSeries``, ``top``, ``number``
            - **metrics**:  a list of dictionaries, specifying the metrics to show in the panel, and optionally, if there is only one metric, a grouping key to segment that metric by. A metric is any of the entries that can be found in the *Metrics* section of the Explore page in Sysdig Monitor. Metric entries require an *aggregations* section specifying how to aggregate the metric across time and groups of containers/hosts. A grouping key is any of the entries that can be found in the *Show* or *Segment By* sections of the Explore page in Sysdig Monitor. Refer to the examples section below for ready to use code snippets. Note, certain panels allow certain combinations of metrics and grouping keys:
                - ``timeSeries``: 1 or more metrics OR 1 metric + 1 grouping key
                - ``top``: 1 or more metrics OR 1 metric + 1 grouping key
                - ``number``: 1 metric only
            - **scope**: filter to apply to the panel; must be based on metadata available in Sysdig Monitor; Example: *kubernetes.namespace.name='production' and container.image='nginx'*.
            - **sort_by**: Data sorting; The parameter is optional and it's a dictionary of ``metric`` and ``mode`` (it can be ``desc`` or ``asc``)
            - **limit**: This parameter sets the limit on the number of lines/bars shown in a ``timeSeries`` or ``top`` panel. In the case of more entities being available than the limit, the top entities according to the sort will be shown. The default value is 10 for ``top`` panels (for ``timeSeries`` the default is defined by Sysdig Monitor itself). Note that increasing the limit above 10 is not officially supported and may cause performance and rendering issues
            - **layout**: Size and position of the panel. The dashboard layout is defined by a grid of 12 columns, each row height is equal to the column height. For example, say you want to show 2 panels at the top: one panel might be 6 x 3 (half the width, 3 rows height) located in row 1 and column 1 (top-left corner of the viewport), the second panel might be 6 x 3 located in row 1 and position 7. The location is specified by a dictionary of ``row`` (row position), ``col`` (column position), ``size_x`` (width), ``size_y`` (height).

        **Success Return Value**
            A dictionary showing the details of the edited dashboard.

        **Example**
            `examples/dashboard.py <https://github.com/draios/python-sdc-client/blob/master/examples/dashboard.py>`_
        """"""
    panel_configuration = {'name': name, 'showAs': None, 'showAsType': None, 'metrics': [], 'gridConfiguration': {'col': 1, 'row': 1, 'size_x': 12, 'size_y': 6}}
    if '<MASK>':
        metrics = copy.copy(metrics)
        metrics.insert(0, {'id': 'timestamp'})
    property_names = {}
    k_count = 0
    v_count = 0
    for i, metric in enumerate(metrics):
        property_name = 'v' if 'aggregations' in metric else 'k'
        if '<MASK>':
            i = k_count
            k_count += 1
        else:
            i = v_count
            v_count += 1
        property_names[metric['id']] = property_name + str(i)
        panel_configuration['metrics'].append({'metricId': metric['id'], 'aggregation': metric['aggregations']['time'] if 'aggregations' in metric else None, 'groupAggregation': metric['aggregations']['group'] if 'aggregations' in metric else None, 'propertyName': property_name + str(i)})
    panel_configuration['scope'] = scope
    panel_configuration['overrideFilter'] = 'scope' in dashboard and dashboard['scope'] != scope or ('scope' not in dashboard and scope != None)
    if '<MASK>':
        panel_configuration['showAs'] = 'timeSeries'
        panel_configuration['showAsType'] = 'line'
        if '<MASK>':
            panel_configuration['paging'] = {'from': 0, 'to': limit - 1}
    elif '<MASK>':
        panel_configuration['showAs'] = 'summary'
        panel_configuration['showAsType'] = 'summary'
    elif '<MASK>':
        panel_configuration['showAs'] = 'top'
        panel_configuration['showAsType'] = 'bars'
        if '<MASK>':
            panel_configuration['sorting'] = [{'id': 'v0', 'mode': 'desc'}]
        else:
            panel_configuration['sorting'] = [{'id': property_names[sort_by['metric']], 'mode': sort_by['mode']}]
        if '<MASK>':
            panel_configuration['paging'] = {'from': 0, 'to': 10}
        else:
            panel_configuration['paging'] = {'from': 0, 'to': limit - 1}
    if '<MASK>':
        panel_configuration['gridConfiguration'] = layout
    dashboard_configuration = copy.deepcopy(dashboard)
    dashboard_configuration['id'] = None
    dashboard_configuration['items'].append(panel_configuration)
    res = requests.put(self.url + self._dashboards_api_endpoint + '/' + str(dashboard['id']), headers=self.hdrs, data=json.dumps({'dashboard': dashboard_configuration}), verify=self.ssl_verify)
    return self._request_result(res)",False,"[""panel_type == 'timeSeries'"", ""panel_type == 'timeSeries'"", 'layout != None', ""property_name == 'k'"", 'limit != None', ""panel_type == 'number'"", ""panel_type == 'top'"", 'sort_by is None', 'limit is None']",___________________,0.0
"def register_jinja(app):
    """"""Register jinja filters, vars, functions.""""""
    import jinja2
    from .utils import filters, permissions, helpers
    if '<MASK>':
        my_loader = jinja2.ChoiceLoader([app.jinja_loader, jinja2.FileSystemLoader([os.path.join(app.config.get('PROJECT_PATH'), 'application/macros'), os.path.join(app.config.get('PROJECT_PATH'), 'application/pages')])])
    else:
        my_loader = jinja2.ChoiceLoader([app.jinja_loader, jinja2.FileSystemLoader([os.path.join(app.config.get('PROJECT_PATH'), 'output/macros'), os.path.join(app.config.get('PROJECT_PATH'), 'output/pages')])])
    app.jinja_loader = my_loader
    app.jinja_env.filters.update({'timesince': filters.timesince})

    def url_for_other_page(page):
        """"""Generate url for pagination.""""""
        view_args = request.view_args.copy()
        args = request.args.copy().to_dict()
        combined_args = dict(view_args.items() + args.items())
        combined_args['page'] = page
        return url_for(request.endpoint, **combined_args)
    rules = {}
    for endpoint, _rules in iteritems(app.url_map._rules_by_endpoint):
        if '<MASK>':
            continue
        rules[endpoint] = [{'rule': rule.rule} for rule in _rules]
    app.jinja_env.globals.update({'absolute_url_for': helpers.absolute_url_for, 'url_for_other_page': url_for_other_page, 'rules': rules, 'permissions': permissions})",False,"['app.debug or app.testing', ""any((item in endpoint for item in ['_debug_toolbar', 'debugtoolbar', 'static']))""]",___________________,0.0
"def update(self, t_obj):
    """"""[update table]

        Arguments:
            t_obj {[objs of DeclarativeMeta]} -- [update the table]
        """"""
    if '<MASK>':
        self._session.add_all(t_obj)
    else:
        self._session.add(t_obj)",False,"['isinstance(t_obj, Iterable)']","_,_,_,_,_,_,_,_,_,_",0.0
"def run_in_separate_process(func, *args, **kwargs):
    """"""Runs function in separate process.

    This function is used instead of a decorator, since Python multiprocessing
    module can't serialize decorated function on all platforms.
    """"""
    manager = multiprocessing.Manager()
    manager_dict = manager.dict()
    process = ProcessWithException(manager_dict, target=func, args=args, kwargs=kwargs)
    process.start()
    process.join()
    exc = process.exception
    if '<MASK>':
        raise exc
    return process.output",False,['exc'],___________________,0.0
"def is_fourfold_repetition(self):
    """"""
        a game is ended if a position occurs for the fourth time
        on consecutive alternating moves.
        """"""
    zobrist_hash = self.zobrist_hash()
    if '<MASK>':
        return False
    return True",False,['self.transpositions[zobrist_hash] < 4'],___________________,0.0
"def _set_secondary_channels(self):
    """"""Sets the secondary channels for the pipeline

        This will iterate over the
        :py:attr:`NextflowGenerator.secondary_channels` dictionary that is
        populated when executing
        :func:`~NextflowGenerator._update_secondary_channels` method.
        """"""
    logger.debug('==========================')
    logger.debug('Setting secondary channels')
    logger.debug('==========================')
    logger.debug('Setting secondary channels: {}'.format(self.secondary_channels))
    for source, lanes in self.secondary_channels.items():
        for vals in lanes.values():
            if '<MASK>':
                logger.debug('[{}] No secondary links to setup'.format(vals['p'].template))
                continue
            logger.debug('[{}] Setting secondary links for source {}: {}'.format(vals['p'].template, source, vals['end']))
            vals['p'].set_secondary_channel(source, vals['end'])",False,"[""not vals['end']""]",___________________,0.0
"def generate_hpo_gene_list(self, *hpo_terms):
    """"""Generate a sorted list with namedtuples of hpogenes

            Each namedtuple of the list looks like (hgnc_id, count)

            Args:
                hpo_terms(iterable(str))

            Returns:
                hpo_genes(list(HpoGene))
        """"""
    genes = {}
    for term in hpo_terms:
        hpo_obj = self.hpo_term(term)
        if '<MASK>':
            for hgnc_id in hpo_obj['genes']:
                if '<MASK>':
                    genes[hgnc_id] += 1
                else:
                    genes[hgnc_id] = 1
        else:
            LOG.warning('Term %s could not be found', term)
    sorted_genes = sorted(genes.items(), key=operator.itemgetter(1), reverse=True)
    return sorted_genes",False,"['hpo_obj', 'hgnc_id in genes']","_,,,,,,,,,,,,,,,,,,",0.0
"def refresh_token(self, grant_type, client_id, client_secret, refresh_token, **params):
    """"""Generate access token HTTP response from a refresh token.

        :param grant_type: Desired grant type. Must be ""refresh_token"".
        :type grant_type: str
        :param client_id: Client ID.
        :type client_id: str
        :param client_secret: Client secret.
        :type client_secret: str
        :param refresh_token: Refresh token.
        :type refresh_token: str
        :rtype: requests.Response
        """"""
    if '<MASK>':
        return self._make_json_error_response('unsupported_grant_type')
    is_valid_client_id = self.validate_client_id(client_id)
    is_valid_client_secret = self.validate_client_secret(client_id, client_secret)
    scope = params.get('scope', '')
    is_valid_scope = self.validate_scope(client_id, scope)
    data = self.from_refresh_token(client_id, refresh_token, scope)
    is_valid_refresh_token = data is not None
    if '<MASK>':
        return self._make_json_error_response('invalid_client')
    if '<MASK>':
        return self._make_json_error_response('invalid_scope')
    if '<MASK>':
        return self._make_json_error_response('invalid_grant')
    self.discard_refresh_token(client_id, refresh_token)
    access_token = self.generate_access_token()
    token_type = self.token_type
    expires_in = self.token_expires_in
    refresh_token = self.generate_refresh_token()
    self.persist_token_information(client_id=client_id, scope=scope, access_token=access_token, token_type=token_type, expires_in=expires_in, refresh_token=refresh_token, data=data)
    return self._make_json_response({'access_token': access_token, 'token_type': token_type, 'expires_in': expires_in, 'refresh_token': refresh_token})",False,"[""grant_type != 'refresh_token'"", 'not (is_valid_client_id and is_valid_client_secret)', 'not is_valid_scope', 'not is_valid_refresh_token']",___________________,0.0
"def pandas(self, mols, nproc=None, nmols=None, quiet=False, ipynb=False, id=-1):
    """"""Calculate descriptors over mols.

        Returns:
            pandas.DataFrame

        """"""
    from .pandas_module import MordredDataFrame, Series
    if '<MASK>':
        index = mols.index
    else:
        index = None
    return MordredDataFrame((list(r) for r in self.map(mols, nproc, nmols, quiet, ipynb, id)), columns=[str(d) for d in self.descriptors], index=index)",False,"['isinstance(mols, Series)']","_,_,_,_,_,_,_,_,_,_",0.0
"def p_proposition_sum(self, p):
    """"""word_sum : word_sum PLUS word
                    | word
            clauses_sum : clauses_sum PLUS clause
                    | clause
            superclauses_sum : superclauses_sum PLUS superclause
                    | superclause""""""
    if '<MASK>':
        p[0] = p[1] + [p[3]]
    elif '<MASK>':
        p[0] = p[1] + [p[2]]
    else:
        p[0] = [p[1]]",False,"['len(p) == 4', 'len(p) == 3']",___________________,0.0
"def normalize(hash_):
    """"""
    ""Normalizes"" a hash, converting it into a sequence of integers
    (not trits!) suitable for use in signature generation/validation.

    The hash is divided up into 3 parts, each of which is ""balanced""
    (sum of all the values is equal to zero).
    """"""
    normalized = []
    source = hash_.as_integers()
    chunk_size = 27
    for i in range(Hash.LEN // chunk_size):
        start = i * chunk_size
        stop = start + chunk_size
        chunk = source[start:stop]
        chunk_sum = sum(chunk)
        while chunk_sum > 0:
            chunk_sum -= 1
            for j in range(chunk_size):
                if '<MASK>':
                    chunk[j] -= 1
                    break
        while chunk_sum < 0:
            chunk_sum += 1
            for j in range(chunk_size):
                if '<MASK>':
                    chunk[j] += 1
                    break
        normalized.append(chunk)
    return normalized",False,"['chunk[j] > -13', 'chunk[j] < 13']",___________________,0.0
"def __get_obj_source(self, instance: typing.Any, owner: typing.Optional[type]=None) -> str:
    """"""Get object repr block.""""""
    if '<MASK>':
        return f'{instance!r}'
    return f'<{(owner.__name__ if owner is not None else instance.__class__.__name__)}() at 0x{id(instance):X}>'",False,['self.log_object_repr'],"_,_,_,_,_,_,_,_,_,_",0.0
"def assert_title(self, title, **kwargs):
    """"""
        Asserts that the page has the given title.

        Args:
            title (str | RegexObject): The string or regex that the title should match.
            **kwargs: Arbitrary keyword arguments for :class:`TitleQuery`.

        Returns:
            True

        Raises:
            ExpectationNotMet: If the assertion hasn't succeeded during the wait time.
        """"""
    query = TitleQuery(title, **kwargs)

    @self.synchronize(wait=query.wait)
    def assert_title():
        if '<MASK>':
            raise ExpectationNotMet(query.failure_message)
        return True
    return assert_title()",False,['not query.resolves_for(self)'],___________________,0.0
"def _build_host_list(self, host_list_str):
    """"""
        This internal function takes the host string from the config file
        and turns it into a list
        :return: LIST host list
        """"""
    host_list = []
    if '<MASK>':
        host_list = host_list_str.replace(' ', '').split(',')
    else:
        raise TypeError(""Unrecognized variable type provided for host list string. 'String' type expected but '"" + str(type(host_list_str)) + ""' received"")
    return host_list",False,"['isinstance(host_list_str, str)']","_):
    """"""
    """"""
    """"""
    """"""
    """"""
   ",0.0
"def add(self):
    """"""Add tasks""""""
    payload = cherrypy.request.json
    logger.debug('Reading tasks...')
    for task_data in payload['tasks']:
        try:
            category = task_data['category']
            backend_args = task_data['backend_args']
            archive_args = task_data.get('archive', None)
            sched_args = task_data.get('scheduler', None)
        except KeyError as ex:
            logger.error('Task badly formed')
            raise ex
        from_date = backend_args.get('from_date', None)
        if '<MASK>':
            backend_args['from_date'] = str_to_datetime(from_date)
        super().add_task(task_data['task_id'], task_data['backend'], category, backend_args, archive_args=archive_args, sched_args=sched_args)
    logger.debug('Done. Ready to work!')
    return 'Tasks added'",False,['from_date'],_args_args_args_args_args_args_args_args_args_,0.0
"def parse(self, *args):
    """"""Parse a list of arguments.

        Parse argument strings needed to run a backend command. The result
        will be a `argparse.Namespace` object populated with the values
        obtained after the validation of the parameters.

        :param args: argument strings

        :result: an object with the parsed values
        """"""
    parsed_args = self.parser.parse_args(args)
    if '<MASK>':
        delattr(parsed_args, 'category')
    if '<MASK>':
        parsed_args.from_date = str_to_datetime(parsed_args.from_date)
    if '<MASK>':
        parsed_args.to_date = str_to_datetime(parsed_args.to_date)
    if '<MASK>':
        parsed_args.archived_since = str_to_datetime(parsed_args.archived_since)
    if '<MASK>':
        raise AttributeError('fetch-archive and no-archive arguments are not compatible')
    if '<MASK>':
        raise AttributeError('fetch-archive needs a category to work with')
    for alias, arg in self.aliases.items():
        if '<MASK>':
            value = getattr(parsed_args, arg, None)
            setattr(parsed_args, alias, value)
    return parsed_args",False,"['parsed_args.category is None', 'self._from_date', 'self._to_date and parsed_args.to_date', 'self._archive and parsed_args.archived_since', 'self._archive and parsed_args.fetch_archive and parsed_args.no_archive', 'self._archive and parsed_args.fetch_archive and (not parsed_args.category)', 'alias not in parsed_args and arg in parsed_args']","_args,_args,_args,_args,_args,_args,_",0.0
"def either(*funcs):
    """"""
    A utility function for selecting the first non-null query.

    Parameters:

      funcs: One or more functions

    Returns:

       A function that, when called with a :class:`Node`, will
       pass the input to each `func`, and return the first non-Falsey
       result.

    Examples:

       >>> s = Soupy(""<p>hi</p>"")
       >>> s.apply(either(Q.find('a'), Q.find('p').text))
       Scalar('hi')
    """"""

    def either(val):
        for func in funcs:
            result = val.apply(func)
            if '<MASK>':
                return result
        return Null()
    return either",False,['result'],___________________,0.0
"def anneal(self):
    """"""
        Fill in empty intervals by growing from top and base.

        Note that this operation happens in-place and destroys any information
        about the ``Position`` (e.g. metadata associated with the top or base).
        See GitHub issue #54.
        """"""
    strip = self.copy()
    gaps = strip.find_gaps(index=True)
    if '<MASK>':
        return
    for gap in gaps:
        before = strip[gap]
        after = strip[gap + 1]
        if '<MASK>':
            t = (after.top.z - before.base.z) / 2
            before.base = before.base.z + t
            after.top = after.top.z - t
        else:
            t = (after.base - before.top) / 2
            before.top = before.top.z + t
            after.base = after.base.z - t
    return strip",False,"['not gaps', ""strip.order == 'depth'""]","_,,,,,,,,,,,,,,,,,,",0.0
"def get_summary(self):
    """"""
        Compat: drf-yasg 1.11
        """"""
    title = None
    method_name = getattr(self.view, 'action', self.method.lower())
    action = getattr(self.view, method_name, None)
    action_kwargs = getattr(action, 'kwargs', None)
    if '<MASK>':
        title = action_kwargs.get('name')
    if '<MASK>':
        title = _(self.view.action.replace('_', ' ')).capitalize()
    if '<MASK>':
        meta = self.view.get_admin_meta()
        if '<MASK>':
            title = str(meta.get('verbose_name') or meta.name)
        elif '<MASK>':
            title = meta.get('verbose_name')
            if '<MASK>':
                title = str(_('Add')) + ' ' + str(title).lower()
            else:
                title = meta.name
        elif '<MASK>':
            title = str(meta.get('verbose_name_plural') or meta.name)
        else:
            title = str(meta.name)
    return title",False,"['action_kwargs', 'not title and is_custom_action(self.view.action)', 'not title', ""self.view.action in ['retrieve', 'update', 'partial_update']"", ""self.view.action == 'create'"", 'title', ""self.view.action == 'list'""]",___________________,0.0
"def close(self):
    """"""called before visiting project (i.e set of modules)""""""
    if '<MASK>':
        graph = self._import_graph_without_ignored_edges()
        vertices = list(graph)
        for cycle in get_cycles(graph, vertices=vertices):
            self.add_message('cyclic-import', args=' -> '.join(cycle))",False,"[""self.linter.is_message_enabled('cyclic-import')""]",(((((((((((((((((((,0.0
"def gene_edit(panel_id, hgnc_id):
    """"""Edit additional information about a panel gene.""""""
    panel_obj = store.panel(panel_id)
    hgnc_gene = store.hgnc_gene(hgnc_id)
    panel_gene = controllers.existing_gene(store, panel_obj, hgnc_id)
    form = PanelGeneForm()
    transcript_choices = []
    for transcript in hgnc_gene['transcripts']:
        if '<MASK>':
            refseq_id = transcript.get('refseq_id')
            transcript_choices.append((refseq_id, refseq_id))
    form.disease_associated_transcripts.choices = transcript_choices
    if '<MASK>':
        action = 'edit' if panel_gene else 'add'
        info_data = form.data.copy()
        if '<MASK>':
            del info_data['csrf_token']
        store.add_pending(panel_obj, hgnc_gene, action=action, info=info_data)
        return redirect(url_for('.panel', panel_id=panel_id))
    if '<MASK>':
        for field_key in ['disease_associated_transcripts', 'reduced_penetrance', 'mosaicism', 'inheritance_models', 'database_entry_version', 'comment']:
            form_field = getattr(form, field_key)
            if '<MASK>':
                panel_value = panel_gene.get(field_key)
                if '<MASK>':
                    form_field.process_data(panel_value)
    return dict(panel=panel_obj, form=form, gene=hgnc_gene, panel_gene=panel_gene)",False,"['form.validate_on_submit()', 'panel_gene', ""transcript.get('refseq_id')"", ""'csrf_token' in info_data"", 'not form_field.data', 'panel_value is not None']",___________________,0.0
"def validate_realms(self, client_key, token, request, uri=None, realms=None):
    """"""Check if the token has permission on those realms.""""""
    log.debug('Validate realms %r for %r', realms, client_key)
    if '<MASK>':
        tok = request.access_token
    else:
        tok = self._tokengetter(client_key=client_key, token=token)
        request.access_token = tok
    if '<MASK>':
        return False
    return set(tok.realms).issuperset(set(realms))",False,"['request.access_token', 'not tok']",",,,,,,,,,,,,,,,,,,,",0.0
"def rename(self, image_name, path):
    """"""rename performs a move, but ensures the path is maintained in storage

       Parameters
       ==========
       image_name: the image name (uri) to rename to.
       path: the name to rename (basename is taken)

    """"""
    container = self.get(image_name, quiet=True)
    if '<MASK>':
        if '<MASK>':
            dirname = os.path.dirname(container.image)
            names = parse_image_name(remove_uri(path))
            storage = os.path.join(self.storage, os.path.dirname(names['storage']))
            if '<MASK>':
                os.mkdir(storage)
            fullpath = os.path.abspath(os.path.join(dirname, names['storage']))
            container = self.cp(move_to=fullpath, container=container, command='rename')
            if '<MASK>':
                container.uri = names['uri']
                self.session.commit()
                return container
    bot.warning('%s not found' % image_name)",False,"['container is not None', 'container.image is not None', 'not os.path.exists(storage)', 'container is not None']","_name,_name,_name,_name,_name,_name,_",0.0
"def _get_volume(self, volume_id):
    """"""Returns a specific volume""""""
    if '<MASK>':
        for volume in self._data['volumes']:
            if '<MASK>':
                return volume",False,"['self._data is not None', ""volume['id'] == volume_id""]","_id):
        """""".............",0.0
"def get_realms(self, token, request):
    """"""Realms for this request token.""""""
    log.debug('Get realms of %r', token)
    tok = request.request_token or self._grantgetter(token=token)
    if '<MASK>':
        return []
    request.request_token = tok
    if '<MASK>':
        return tok.realms or []
    return []",False,"['not tok', ""hasattr(tok, 'realms')""]",",,,,,,,,,,,,,,,,,,,",0.0
"def current_host(self):
    """""" str: Host of the current page. """"""
    if '<MASK>':
        return
    result = urlparse(self.current_url)
    scheme, netloc = (result.scheme, result.netloc)
    host = netloc.split(':')[0] if netloc else None
    return '{0}://{1}'.format(scheme, host) if host else None",False,['not self.current_url'],"__url(self):
    """"""..........",0.0
"def get_annotation_data_after_time(self, id_tier, time):
    """"""Give the annotation before a given time. When the tier contains
        reference annotations this will be returned, check
        :func:`get_ref_annotation_data_before_time` for the format. If an
        annotation overlaps with ``time`` that annotation will be returned.

        :param str id_tier: Name of the tier.
        :param int time: Time to get the annotation before.
        :raises KeyError: If the tier is non existent.
        """"""
    if '<MASK>':
        return self.get_ref_annotation_after_time(id_tier, time)
    befores = self.get_annotation_data_between_times(id_tier, time, self.get_full_time_interval()[1])
    if '<MASK>':
        return [min(befores, key=lambda x: x[0])]
    else:
        return []",False,"['self.tiers[id_tier][1]', 'befores']",___________________,0.0
"def flatten_dictionary(nested_dict, separator):
    """"""Flattens a nested dictionary.

    New keys are concatenations of nested keys with the `separator` in between.

    """"""
    flat_dict = {}
    for key, val in nested_dict.items():
        if '<MASK>':
            new_flat_dict = flatten_dictionary(val, separator)
            for flat_key, inval in new_flat_dict.items():
                new_key = key + separator + flat_key
                flat_dict[new_key] = inval
        else:
            flat_dict[key] = val
    return flat_dict",False,"['isinstance(val, dict)']",_((((((((((((((((((,0.0
"def parse_env_var(value):
    """"""
    Split a env var text like

    ENV_VAR_NAME=env_var_value

    into a tuple ('ENV_VAR_NAME', 'env_var_value')
    """"""
    k, _, v = value.partition('=')
    k, v = (k.strip(), v.strip().encode('unicode-escape').decode('ascii'))
    if '<MASK>':
        v = __escape_decoder(v[1:-1])[0]
    return (k, v)",False,"['v and v[0] == v[-1] in [\'""\', ""\'""]']","_,,,,,,,,,,,,,,,,,,",0.0
"def get(self, record_name):
    """"""
        Will return a matching record or raise KeyError is no record is found.

        If the record name is a full name we will first check for a record matching the full name.
        If no such record is found any record matching the last part of the full name (without the namespace) will
        be returned.
        """"""
    if '<MASK>':
        return self._schema_map[record_name]
    else:
        last_name = record_name.split('.')[-1]
        return self._schema_map[last_name]",False,['record_name in self._schema_map'],"_name_name_name_name_name]
        """"""
        """"""
       ",0.0
"def get_authorization(self):
    """"""Get authorization object representing status of authentication.""""""
    auth = self.authorization_class()
    header = self.get_authorization_header()
    if '<MASK>':
        return auth
    header = header.split()
    if '<MASK>':
        auth.is_oauth = True
        access_token = header[1]
        self.validate_access_token(access_token, auth)
        if '<MASK>':
            auth.error = 'access_denied'
    return auth",False,"['not header or not header.split', ""len(header) > 1 and header[0] == 'Bearer'"", 'not auth.is_valid']",___________________,0.0
"def _read(self, fileobj, amount, truncate=True):
    """"""
        Reads a specific amount of data from a stream and returns it. If there
        is any data in initial_data, that will be popped out first.

        :type fileobj: A file-like object that implements read
        :param fileobj: The stream to read from.

        :type amount: int
        :param amount: The number of bytes to read from the stream.

        :type truncate: bool
        :param truncate: Whether or not to truncate initial_data after
            reading from it.

        :return: Generator which generates part bodies from the initial data.
        """"""
    if '<MASK>':
        return fileobj.read(amount)
    if '<MASK>':
        data = self._initial_data[:amount]
        if '<MASK>':
            self._initial_data = self._initial_data[amount:]
        return data
    amount_to_read = amount - len(self._initial_data)
    data = self._initial_data + fileobj.read(amount_to_read)
    if '<MASK>':
        self._initial_data = b''
    return data",False,"['len(self._initial_data) == 0', 'amount <= len(self._initial_data)', 'truncate', 'truncate']",___________________,0.0
"def validate_code(self, client_id, code, client, request, *args, **kwargs):
    """"""Ensure the grant code is valid.""""""
    client = client or self._clientgetter(client_id)
    log.debug('Validate code for client %r and code %r', client.client_id, code)
    grant = self._grantgetter(client_id=client.client_id, code=code)
    if '<MASK>':
        log.debug('Grant not found.')
        return False
    if '<MASK>':
        log.debug('Grant is expired.')
        return False
    request.state = kwargs.get('state')
    request.user = grant.user
    request.scopes = grant.scopes
    return True",False,"['not grant', ""hasattr(grant, 'expires') and datetime.datetime.utcnow() > grant.expires""]",___________________,0.0
"def _group_sentences(total_nb_sentences, group_length):
    """""" Split sentences in groups, given a specific group length.

        Args:
            total_nb_sentences (int): Total available sentences.
            group_length (int): Limit of length for each group.

        Returns:
            list: Contains groups (lists) of sentences.
        """"""
    sentences_groups = []
    current_sentence_group = []
    for i in range(0, total_nb_sentences):
        if '<MASK>':
            if '<MASK>':
                sentences_groups.append(current_sentence_group)
            current_sentence_group = [i]
        else:
            current_sentence_group.append(i)
    if '<MASK>':
        sentences_groups.append(current_sentence_group)
    return sentences_groups",False,"['len(current_sentence_group) > 0', 'i % group_length == 0', 'len(current_sentence_group) > 0']",___________________,0.0
"def run(self, func, tasks, func2=None):
    """"""run will send a list of tasks,
        a tuple with arguments, through a function.
        the arguments should be ordered correctly.
        :param func: the function to run with multiprocessing.pool
        :param tasks: a list of tasks, each a tuple
                      of arguments to process
        :param func2: filter function to run result
                      from func through (optional)
        """"""
    progress = 1
    total = len(tasks)
    if '<MASK>':
        return
    if '<MASK>':
        total = total * 2
    finished = []
    level1 = []
    results = []
    try:
        prefix = '[%s/%s]' % (progress, total)
        bot.show_progress(0, total, length=35, prefix=prefix)
        pool = multiprocessing.Pool(self.workers, init_worker)
        self.start()
        for task in tasks:
            result = pool.apply_async(multi_wrapper, multi_package(func, [task]))
            results.append(result)
            level1.append(result._job)
        while len(results) > 0:
            result = results.pop()
            result.wait()
            bot.show_progress(progress, total, length=35, prefix=prefix)
            progress += 1
            prefix = '[%s/%s]' % (progress, total)
            if '<MASK>':
                result = pool.apply_async(multi_wrapper, multi_package(func2, [(result.get(),)]))
                results.append(result)
            else:
                finished.append(result.get())
        self.end()
        pool.close()
        pool.join()
    except (KeyboardInterrupt, SystemExit):
        bot.error('Keyboard interrupt detected, terminating workers!')
        pool.terminate()
        sys.exit(1)
    except Exception as e:
        bot.error(e)
    return finished",False,"['len(tasks) == 0', 'func2 is not None', 'func2 is not None and result._job in level1']","_,,,,,,,,,,,,,,,,,,",0.0
"def sendall(self, buf, flags=0):
    """"""
        Send ""all"" data on the connection. This calls send() repeatedly until
        all data is sent. If an error occurs, it's impossible to tell how much
        data has been sent.

        :param buf: The string, buffer or memoryview to send
        :param flags: (optional) Included for compatibility with the socket
                      API, the value is ignored
        :return: The number of bytes written
        """"""
    buf = _text_to_bytes_and_warn('buf', buf)
    if '<MASK>':
        buf = buf.tobytes()
    if '<MASK>':
        buf = str(buf)
    if '<MASK>':
        raise TypeError('buf must be a memoryview, buffer or byte string')
    left_to_send = len(buf)
    total_sent = 0
    data = _ffi.new('char[]', buf)
    while left_to_send:
        result = _lib.SSL_write(self._ssl, data + total_sent, min(left_to_send, 2147483647))
        self._raise_ssl_error(self._ssl, result)
        total_sent += result
        left_to_send -= result",False,"['isinstance(buf, memoryview)', 'isinstance(buf, _buffer)', 'not isinstance(buf, bytes)']","_,,,,,,,,,,,,,,,,,,",0.0
"def clock_plot(self, add_internal=False, ax=None, regression=None, confidence=True, n_sigma=2, fs=14):
    """"""Plot root-to-tip distance vs time as a basic time-tree diagnostic

        Parameters
        ----------
        add_internal : bool, optional
            add internal nodes. this will only work if the tree has been dated already
        ax : None, optional
            an matplotlib axis to plot into. if non provided, a new figure is opened
        regression : None, optional
            a dict containing parameters of a root-to-tip vs time regression as
            returned by the function base_regression
        confidence : bool, optional
            add confidence area to the regression line
        n_sigma : int, optional
            number of standard deviations for the confidence area.
        fs : int, optional
            fontsize

        """"""
    import matplotlib.pyplot as plt
    if '<MASK>':
        plt.figure()
        ax = plt.subplot(111)
    self.tree.root._v = 0
    for n in self.tree.get_nonterminals(order='preorder'):
        for c in n:
            c._v = n._v + self.branch_value(c)
    tips = self.tree.get_terminals()
    internal = self.tree.get_nonterminals()
    xi = np.array([self.tip_value(n) for n in tips])
    yi = np.array([n._v for n in tips])
    ind = np.array([n.bad_branch if hasattr(n, 'bad_branch') else False for n in tips])
    if '<MASK>':
        xi_int = np.array([n.numdate for n in internal])
        yi_int = np.array([n._v for n in internal])
        ind_int = np.array([n.bad_branch if hasattr(n, 'bad_branch') else False for n in internal])
    if '<MASK>':
        t_mrca = -regression['intercept'] / regression['slope']
        if '<MASK>':
            time_span = np.max(xi_int[~ind_int]) - np.min(xi_int[~ind_int])
            x_vals = np.array([max(np.min(xi_int[~ind_int]), t_mrca) - 0.1 * time_span, np.max(xi[~ind]) + 0.05 * time_span])
        else:
            time_span = np.max(xi[~ind]) - np.min(xi[~ind])
            x_vals = np.array([max(np.min(xi[~ind]), t_mrca) - 0.1 * time_span, np.max(xi[~ind] + 0.05 * time_span)])
        if '<MASK>':
            x_vals = np.linspace(x_vals[0], x_vals[1], 100)
            y_vals = regression['slope'] * x_vals + regression['intercept']
            dev = n_sigma * np.array([np.sqrt(regression['cov'][:2, :2].dot(np.array([x, 1])).dot(np.array([x, 1]))) for x in x_vals])
            dev_slope = n_sigma * np.sqrt(regression['cov'][0, 0])
            ax.fill_between(x_vals, y_vals - dev, y_vals + dev, alpha=0.2)
            dp = np.array([regression['intercept'] / regression['slope'] ** 2, -1.0 / regression['slope']])
            dev_rtt = n_sigma * np.sqrt(regression['cov'][:2, :2].dot(dp).dot(dp))
        else:
            dev_rtt = None
            dev_slope = None
        ax.plot(x_vals, regression['slope'] * x_vals + regression['intercept'], label='$y=\\alpha + \\beta t$' + '\n' + '$\\beta=$%1.2e' % regression['slope'] + ('+/- %1.e' % dev_slope if dev_slope else '') + '\nroot date: %1.1f' % (-regression['intercept'] / regression['slope']) + ('+/- %1.2f' % dev_rtt if dev_rtt else ''))
    ax.scatter(xi[~ind], yi[~ind], label='tips' if add_internal else None)
    if '<MASK>':
        try:
            tmp_x = np.array([np.mean(n.raw_date_constraint) if n.raw_date_constraint else None for n in self.tree.get_terminals()])
            ax.scatter(tmp_x[ind], yi[ind], label='ignored tips', c='r')
        except:
            pass
    if '<MASK>':
        ax.scatter(xi_int[~ind_int], yi_int[~ind_int], label='internal nodes')
    ax.set_ylabel('root-to-tip distance', fontsize=fs)
    ax.set_xlabel('date', fontsize=fs)
    ax.ticklabel_format(useOffset=False)
    ax.tick_params(labelsize=fs * 0.8)
    ax.set_ylim([0, 1.1 * np.max(yi)])
    plt.tight_layout()
    plt.legend(fontsize=fs * 0.8)",False,"['ax is None', 'add_internal', 'regression', 'ind.sum()', 'add_internal', 'add_internal', ""confidence and 'cov' in regression""]",___________________,0.0
"def write_org_json(self, date=datetime.date.today(), organization='llnl', dict_to_write={}, path_ending_type='', is_list=False):
    """"""
        Writes stats from the organization to JSON.
        """"""
    path = '../github-data/' + organization + '-org/' + path_ending_type + '/' + str(date) + '.json'
    self.checkDir(path)
    with open(path, 'w') as out_clear:
        out_clear.close()
    with open(path, 'a') as out:
        if '<MASK>':
            out.write('[')
        for item in dict_to_write:
            out.write(json.dumps(dict_to_write[item], sort_keys=True, indent=4, separators=(',', ': ')) + ',')
        out.seek(-1, os.SEEK_END)
        out.truncate()
        if '<MASK>':
            out.write(']')
    out.close()",False,"['is_list', 'is_list']",___________________,0.0
"def add(self, image_path=None, image_uri=None, image_name=None, url=None, metadata=None, save=True, copy=False):
    """"""dummy add simple returns an object that mimics a database entry, so the
       calling function (in push or pull) can interact with it equally. Most 
       variables (other than image_path) are not used.""""""
    if '<MASK>':
        if '<MASK>':
            bot.error('Cannot find %s' % image_path)
            sys.exit(1)
    if '<MASK>':
        bot.error('You must provide an image uri <collection>/<namespace>')
        sys.exit(1)
    names = parse_image_name(remove_uri(image_uri))
    bot.debug('Added %s to filesystem' % names['uri'])

    class DummyContainer:

        def __init__(self, image_path, client_name, url, names):
            self.image = image_path
            self.client = client_name
            self.url = url
            self.name = names['image']
            self.tag = names['tag']
            self.uri = names['uri']
    container = DummyContainer(image_path, self.client_name, url, names)
    bot.info('[container][%s] %s' % (action, names['uri']))
    return container",False,"['image_path is not None', 'image_uri is None', 'not os.path.exists(image_path)']",___________________,0.0
"def create_dataset(args):
    """"""
    Attempt to create a new dataset given the following params:

        * template_id
        * template_file
        * capacity
        * create_vault
        * [argument] dataset name or full path

    NOTE: genome_build has been deprecated and is no longer used.

    """"""
    if '<MASK>':
        full_path, path_dict = Object.validate_full_path('{0}:/{1}/{2}'.format(args.vault, args.path, args.full_path))
    else:
        full_path, path_dict = Object.validate_full_path(args.full_path, vault=args.vault, path=args.path)
    if '<MASK>':
        try:
            tpl = solvebio.DatasetTemplate.retrieve(args.template_id)
        except solvebio.SolveError as e:
            if '<MASK>':
                raise e
            print('No template with ID {0} found!'.format(args.template_id))
            sys.exit(1)
    elif '<MASK>':
        mode = 'r'
        fopen = open
        if '<MASK>':
            mode = 'rb'
            fopen = gzip.open
        with fopen(args.template_file, mode) as fp:
            try:
                tpl_json = json.load(fp)
            except:
                print('Template file {0} could not be loaded. Please pass valid JSON'.format(args.template_file))
                sys.exit(1)
        tpl = solvebio.DatasetTemplate.create(**tpl_json)
        print('A new dataset template was created with id: {0}'.format(tpl.id))
    else:
        print('Creating a new dataset {0} without a template.'.format(full_path))
        tpl = None
        fields = []
        entity_type = None
        description = None
    if '<MASK>':
        print(""Creating new dataset {0} using the template '{1}'."".format(full_path, tpl.name))
        fields = tpl.fields
        entity_type = tpl.entity_type
        description = 'Created with dataset template: {0}'.format(str(tpl.id))
    return solvebio.Dataset.get_or_create_by_full_path(full_path, capacity=args.capacity, entity_type=entity_type, fields=fields, description=description, create_vault=args.create_vault)",False,"[""'/' not in args.full_path and args.vault and args.path"", 'args.template_id', 'tpl', 'args.template_file', 'check_gzip_path(args.template_file)', 'e.status_code != 404']",___________________,0.0
"def useless_pass_line_numbers(source):
    """"""Yield line numbers of unneeded ""pass"" statements.""""""
    sio = io.StringIO(source)
    previous_token_type = None
    last_pass_row = None
    last_pass_indentation = None
    previous_line = ''
    for token in tokenize.generate_tokens(sio.readline):
        token_type = token[0]
        start_row = token[2][0]
        line = token[4]
        is_pass = token_type == tokenize.NAME and line.strip() == 'pass'
        if '<MASK>':
            yield (start_row - 1)
        if '<MASK>':
            last_pass_row = start_row
            last_pass_indentation = get_indentation(line)
        if '<MASK>':
            yield start_row
        previous_token_type = token_type
        previous_line = line",False,"['start_row - 1 == last_pass_row and get_indentation(line) == last_pass_indentation and (token_type in ATOMS) and (not is_pass)', 'is_pass', ""is_pass and previous_token_type != tokenize.INDENT and (not previous_line.rstrip().endswith('\\\\'))""]",___________________,0.0
"def update_expiry(self, commit=True):
    """"""Update token's expiration datetime on every auth action.""""""
    self.expires = update_expiry(self.created)
    if '<MASK>':
        self.save()",False,['commit'],"_exp):
    """""" """""" """""" """""" """""" """""" """""" """""" """""" """""" """""" """""" """""" """"""",0.0
"def is_likely_pathogenic(pvs, ps_terms, pm_terms, pp_terms):
    """"""Check if the criterias for Likely Pathogenic is fullfilled

    The following are descriptions of Likely Pathogenic clasification from ACMG paper:

    Likely pathogenic
      (i) 1 Very strong (PVS1) AND 1 moderate (PM1– PM6) OR
      (ii) 1 Strong (PS1–PS4) AND 1–2 moderate (PM1–PM6) OR
      (iii) 1 Strong (PS1–PS4) AND ≥2 supporting (PP1–PP5) OR
      (iv)  ≥3 Moderate (PM1–PM6) OR
      (v) 2 Moderate (PM1–PM6) AND ≥2 supporting (PP1–PP5) OR
      (vi) 1 Moderate (PM1–PM6) AND ≥4 supportin (PP1–PP5)

    Args:
        pvs(bool): Pathogenic Very Strong
        ps_terms(list(str)): Pathogenic Strong terms
        pm_terms(list(str)): Pathogenic Moderate terms
        pp_terms(list(str)): Pathogenic Supporting terms

    Returns:
        bool: if classification indicates Likely Pathogenic level
    """"""
    if '<MASK>':
        if '<MASK>':
            return True
    if '<MASK>':
        if '<MASK>':
            return True
        if '<MASK>':
            return True
    if '<MASK>':
        if '<MASK>':
            return True
        elif '<MASK>':
            if '<MASK>':
                return True
        elif '<MASK>':
            return True
    return False",False,"['pvs', 'ps_terms', 'pm_terms', 'pm_terms', 'pm_terms', 'len(pp_terms) >= 2', 'len(pm_terms) >= 3', 'len(pm_terms) >= 2', 'len(pp_terms) >= 2', 'len(pp_terms) >= 4']",___________________,0.0
"def create_vars_from_data(self, dataset, split='train'):
    """"""
        Create vars given a dataset and set test values.
        Useful when dataset is already defined.
        """"""
    from deepy.core.neural_var import NeuralVariable
    vars = []
    if '<MASK>':
        data_split = dataset.valid_set()
    elif '<MASK>':
        data_split = dataset.test_set()
    else:
        data_split = dataset.train_set()
    first_data_piece = list(data_split)[0]
    for i, numpy_tensor in enumerate(first_data_piece):
        if '<MASK>':
            numpy_tensor = numpy_tensor.astype('int32')
        if '<MASK>':
            numpy_tensor = numpy_tensor.astype(env.FLOATX)
        type_map = {0: 'scalar', 1: 'vector', 2: 'matrix', 3: 'tensor3', 4: 'tensor4', 5: 'tensor5'}
        tensor_type = type_map[numpy_tensor.ndim] if numpy_tensor.ndim in type_map else type_map[0]
        if '<MASK>':
            tensor_type = 'i' + tensor_type
        theano_tensor = getattr(TT, tensor_type)('input_{}_{}'.format(i + 1, tensor_type))
        last_dim = numpy_tensor.shape[-1]
        var = NeuralVariable(theano_tensor, dim=last_dim)
        var.set_test_value(numpy_tensor)
        vars.append(var)
    return vars",False,"[""split == 'valid'"", ""split == 'test'"", ""numpy_tensor.dtype == 'int64'"", ""numpy_tensor.dtype == 'float64'"", ""numpy_tensor.dtype.kind == 'i'""]",___________________,0.0
"def convertIds(*types, **kwargs):
    """"""
        Class decorator: add helper methods to convert identifier properties into SkypeObjs.

        Args:
            types (str list): simple field types to add properties for (``user``, ``users`` or ``chat``)
            user (str list): attribute names to treat as single user identifier fields
            users (str list): attribute names to treat as user identifier lists
            chat (str list): attribute names to treat as chat identifier fields

        Returns:
            method: decorator function, ready to apply to other methods
        """"""
    user = kwargs.get('user', ())
    users = kwargs.get('users', ())
    chat = kwargs.get('chat', ())

    def userObj(self, field):
        return self.skype.contacts[getattr(self, field)]

    def userObjs(self, field):
        return (self.skype.contacts[id] for id in getattr(self, field))

    def chatObj(self, field):
        return self.skype.chats[getattr(self, field)]

    def attach(cls, fn, field, idField):
        """"""
            Generate the property object and attach it to the class.

            Args:
                cls (type): class to attach the property to
                fn (method): function to be attached
                field (str): attribute name for the new property
                idField (str): reference field to retrieve identifier from
            """"""
        setattr(cls, field, property(functools.wraps(fn)(functools.partial(fn, field=idField))))

    def wrapper(cls):
        for type in types:
            if '<MASK>':
                attach(cls, userObj, 'user', 'userId')
            elif '<MASK>':
                attach(cls, userObjs, 'users', 'userIds')
            elif '<MASK>':
                attach(cls, chatObj, 'chat', 'chatId')
        for field in user:
            attach(cls, userObj, field, '{0}Id'.format(field))
        for field in users:
            attach(cls, userObjs, '{0}s'.format(field), '{0}Ids'.format(field))
        for field in chat:
            attach(cls, chatObj, field, '{0}Id'.format(field))
        return cls
    return wrapper",False,"[""type == 'user'"", ""type == 'users'"", ""type == 'chat'""]",___________________,0.0
"def _skip_lines(src_code, skip_map):
    """"""Skips lines in src_code specified by skip map.""""""
    if '<MASK>':
        return [['line', j + 1, l] for j, l in enumerate(src_code)]
    code_with_skips, i = ([], 0)
    for line, length in skip_map:
        code_with_skips.extend((['line', i + j + 1, l] for j, l in enumerate(src_code[i:line])))
        if '<MASK>':
            code_with_skips[-1][1] += length
        else:
            code_with_skips.append(['skip', length])
        i = line + length
    code_with_skips.extend((['line', i + j + 1, l] for j, l in enumerate(src_code[i:])))
    return code_with_skips",False,"['not skip_map', ""code_with_skips and code_with_skips[-1][0] == 'skip'""]",___________________,0.0
"def get_td_at_index(tr, index):
    """"""
    When calculating the rowspan for a given cell it is required to find all
    table cells 'below' the initial cell with a v_merge. This function will
    return the td element at the passed in index, taking into account colspans.
    """"""
    current = 0
    for td in tr.xpath('.//w:tc', namespaces=tr.nsmap):
        if '<MASK>':
            return td
        current += get_grid_span(td)",False,['index == current'],___________________,0.0
"def pca_plot(pca, dt, xlabs=None, mode='scatter', lognorm=True):
    """"""
    Plot a fitted PCA, and all components.
    """"""
    nc = pca.n_components
    f = np.arange(pca.n_features_)
    cs = list(itertools.combinations(range(nc), 2))
    ind = ~np.apply_along_axis(any, 1, np.isnan(dt))
    cylim = (pca.components_.min(), pca.components_.max())
    yd = cylim[1] - cylim[0]
    fig, axs = plt.subplots(nc, nc, figsize=[3 * nc, nc * 3], tight_layout=True)
    for x, y in zip(*np.triu_indices(nc)):
        if '<MASK>':
            tax = axs[x, y]
            tax.bar(f, pca.components_[x], 0.8)
            tax.set_xticks([])
            tax.axhline(0, zorder=-1, c=(0, 0, 0, 0.6))
            tax.set_ylim(cylim[0] - 0.2 * yd, cylim[1] + 0.2 * yd)
            for xi, yi, lab in zip(f, pca.components_[x], xlabs):
                if '<MASK>':
                    yo = yd * 0.03
                    va = 'bottom'
                else:
                    yo = yd * -0.02
                    va = 'top'
                tax.text(xi, yi + yo, lab, ha='center', va=va, rotation=90, fontsize=8)
        else:
            xv = dt[ind, x]
            yv = dt[ind, y]
            if '<MASK>':
                axs[x, y].scatter(xv, yv, alpha=0.2)
                axs[y, x].scatter(yv, xv, alpha=0.2)
            if '<MASK>':
                if '<MASK>':
                    norm = mpl.colors.LogNorm()
                else:
                    norm = None
                axs[x, y].hist2d(xv, yv, 50, cmap=plt.cm.Blues, norm=norm)
                axs[y, x].hist2d(yv, xv, 50, cmap=plt.cm.Blues, norm=norm)
        if '<MASK>':
            axs[y, x].set_ylabel('PC{:.0f}'.format(y + 1))
        if '<MASK>':
            axs[y, x].set_xlabel('PC{:.0f}'.format(x + 1))
    return (fig, axs, xv, yv)",False,"['x == y', 'x == 0', 'y == nc - 1', ""mode == 'scatter'"", ""mode == 'hist2d'"", 'yi > 0', 'lognorm']",___________________,0.0
"def assure_tree(params, tmp_dir='treetime_tmp'):
    """"""
    Function that attempts to load a tree and build it from the alignment
    if no tree is provided.
    """"""
    if '<MASK>':
        params.tree = os.path.basename(params.aln) + '.nwk'
        print('No tree given: inferring tree')
        utils.tree_inference(params.aln, params.tree, tmp_dir=tmp_dir)
    if '<MASK>':
        shutil.rmtree(tmp_dir)
    try:
        tt = TreeAnc(params.tree)
    except:
        print('Tree loading/building failed.')
        return 1
    return 0",False,"['params.tree is None', 'os.path.isdir(tmp_dir)']",___________________,0.0
"def database(context, institute_name, user_name, user_mail, api_key):
    """"""Setup a scout database.""""""
    LOG.info('Running scout setup database')
    api_key = api_key or context.obj.get('omim_api_key')
    if '<MASK>':
        LOG.warning('Please provide a omim api key with --api-key')
        context.abort()
    institute_name = institute_name or context.obj['institute_name']
    user_name = user_name or context.obj['user_name']
    user_mail = user_mail or context.obj['user_mail']
    adapter = context.obj['adapter']
    LOG.info('Setting up database %s', context.obj['mongodb'])
    setup_scout(adapter=adapter, institute_id=institute_name, user_name=user_name, user_mail=user_mail, api_key=api_key)",False,['not api_key'],"_,_,_,_,_,_,_,_,_,_",0.0
"def write(self, output_stream, kmip_version=enums.KMIPVersion.KMIP_1_0):
    """"""
        Write the data encoding the Rekey request payload to a stream.

        Args:
            output_stream (stream): A data stream in which to encode object
                data, supporting a write method; usually a BytearrayStream
                object.
            kmip_version (KMIPVersion): An enumeration defining the KMIP
                version with which the object will be encoded. Optional,
                defaults to KMIP 1.0.

        Raises:
            ValueError: Raised if the payload is missing the unique identifier.
        """"""
    local_stream = utils.BytearrayStream()
    if '<MASK>':
        self._unique_identifier.write(local_stream, kmip_version=kmip_version)
    else:
        raise ValueError('The Rekey response payload is missing the unique identifier.')
    if '<MASK>':
        self._template_attribute.write(local_stream, kmip_version=kmip_version)
    self.length = local_stream.length()
    super(RekeyResponsePayload, self).write(output_stream, kmip_version=kmip_version)
    output_stream.write(local_stream.buffer)",False,"['self._unique_identifier is not None', 'self._template_attribute is not None']","_,_,_,_,_,_,_,_,_,_",0.0
"def _updateParamsFrom(self, otherObj: 'PropDeclrCollector', updater, exclude: set, prefix: str) -> None:
    """"""
        Update all parameters which are defined on self from otherObj

        :param otherObj: other object which Param instances should be updated
        :param updater: updater function(self, myParameter, onOtherParameterName, otherParameter)
        :param exclude: iterable of parameter on otherObj object which should be excluded
        :param prefix: prefix which should be added to name of paramters of this object before matching
            parameter name on parent
        """"""
    excluded = set()
    if '<MASK>':
        exclude = set(exclude)
    for myP in self._params:
        pPName = prefix + myP._scopes[self][1]
        try:
            otherP = getattr(otherObj, pPName)
            if '<MASK>':
                continue
        except AttributeError:
            continue
        if '<MASK>':
            excluded.add(otherP)
            continue
        updater(self, myP, otherP)
    if '<MASK>':
        assert excluded == exclude",False,"['exclude is not None', 'exclude is not None', 'exclude and otherP in exclude', 'not isinstance(otherP, Param)']",___________________,0.0
"def find_try_except_wrapper_node(node: astroid.node_classes.NodeNG) -> Union[astroid.ExceptHandler, astroid.TryExcept]:
    """"""Return the ExceptHandler or the TryExcept node in which the node is.""""""
    current = node
    ignores = (astroid.ExceptHandler, astroid.TryExcept)
    while current and (not isinstance(current.parent, ignores)):
        current = current.parent
    if '<MASK>':
        return current.parent
    return None",False,"['current and isinstance(current.parent, ignores)']","_,,,,,,,,,,,,,,,,,,",0.0
"def is_exclude_file(filename, exclude):
    """"""Return True if file matches exclude pattern.""""""
    base_name = os.path.basename(filename)
    if '<MASK>':
        return True
    for pattern in exclude:
        if '<MASK>':
            return True
        if '<MASK>':
            return True
    return False",False,"[""base_name.startswith('.')"", 'fnmatch.fnmatch(base_name, pattern)', 'fnmatch.fnmatch(filename, pattern)']",_name_name_name_name_name_name_name_name_name_,0.0
"def success(self):
    """"""Update the timer to reflect a successfull call""""""
    if '<MASK>':
        return
    self.short_interval -= self.short_unit
    self.long_interval -= self.long_unit
    self.short_interval = max(self.short_interval, Decimal(0))
    self.long_interval = max(self.long_interval, Decimal(0))
    self.update_interval()",False,['self.interval == 0.0'],_interval(self.long_interval(self.long_interval(self.long_,0.0
"def _set_affiliation(self, v, load=False):
    """"""
    Setter method for affiliation, mapped from YANG variable /universe/individual/affiliation (identityref)
    If this variable is read-only (config: false) in the
    source YANG file, then _set_affiliation is considered as a private
    method. Backends looking to populate this variable should
    do so via calling thisObj._set_affiliation() directly.
    """"""
    if '<MASK>':
        v = v._utype(v)
    try:
        t = YANGDynClass(v, base=RestrictedClassType(base_type=unicode, restriction_type='dict_key', restriction_arg={u'napalm-star-wars:EMPIRE': {'@namespace': u'https://napalm-yang.readthedocs.io/napalm-star-wars', '@module': u'napalm-star-wars'}, u'EMPIRE': {'@namespace': u'https://napalm-yang.readthedocs.io/napalm-star-wars', '@module': u'napalm-star-wars'}, u'napalm-star-wars:REBEL_ALLIANCE': {'@namespace': u'https://napalm-yang.readthedocs.io/napalm-star-wars', '@module': u'napalm-star-wars'}, u'REBEL_ALLIANCE': {'@namespace': u'https://napalm-yang.readthedocs.io/napalm-star-wars', '@module': u'napalm-star-wars'}}), is_leaf=True, yang_name='affiliation', parent=self, path_helper=self._path_helper, extmethods=self._extmethods, register_paths=True, namespace='https://napalm-yang.readthedocs.io/napalm-star-wars', defining_module='napalm-star-wars', yang_type='identityref', is_config=True)
    except (TypeError, ValueError):
        raise ValueError({'error-string': 'affiliation must be of a type compatible with identityref', 'defined-type': 'napalm-star-wars:identityref', 'generated-type': 'YANGDynClass(base=RestrictedClassType(base_type=unicode, restriction_type=""dict_key"", restriction_arg={u\'napalm-star-wars:EMPIRE\': {\'@namespace\': u\'https://napalm-yang.readthedocs.io/napalm-star-wars\', \'@module\': u\'napalm-star-wars\'}, u\'EMPIRE\': {\'@namespace\': u\'https://napalm-yang.readthedocs.io/napalm-star-wars\', \'@module\': u\'napalm-star-wars\'}, u\'napalm-star-wars:REBEL_ALLIANCE\': {\'@namespace\': u\'https://napalm-yang.readthedocs.io/napalm-star-wars\', \'@module\': u\'napalm-star-wars\'}, u\'REBEL_ALLIANCE\': {\'@namespace\': u\'https://napalm-yang.readthedocs.io/napalm-star-wars\', \'@module\': u\'napalm-star-wars\'}},), is_leaf=True, yang_name=""affiliation"", parent=self, path_helper=self._path_helper, extmethods=self._extmethods, register_paths=True, namespace=\'https://napalm-yang.readthedocs.io/napalm-star-wars\', defining_module=\'napalm-star-wars\', yang_type=\'identityref\', is_config=True)'})
    self.__affiliation = t
    if '<MASK>':
        self._set()",False,"[""hasattr(v, '_utype')"", ""hasattr(self, '_set')""]",___________________,0.0
"def bitsBitOp(self, other, op, getVldFn, reduceCheckFn):
    """"""
    :attention: If other is Bool signal, convert this to bool
        (not ideal, due VHDL event operator)
    """"""
    other = toHVal(other)
    iamVal = isinstance(self, Value)
    otherIsVal = isinstance(other, Value)
    if '<MASK>':
        other = other._auto_cast(self._dtype)
        return bitsBitOp__val(self, other, op, getVldFn)
    else:
        if '<MASK>':
            self = self._auto_cast(BOOL)
            return op._evalFn(self, other)
        elif '<MASK>':
            pass
        else:
            raise TypeError('Can not apply operator %r (%r, %r)' % (op, self._dtype, other._dtype))
        if '<MASK>':
            r = reduceCheckFn(self, other)
            if '<MASK>':
                return r
        elif '<MASK>':
            r = reduceCheckFn(other, self)
            if '<MASK>':
                return r
        return Operator.withRes(op, [self, other], self._dtype)",False,"['iamVal and otherIsVal', 'other._dtype == BOOL', 'otherIsVal', 'self._dtype == other._dtype', 'r is not None', 'iamVal', 'r is not None']",___________________,0.0
"def expanded_sequence(self, node, include_additional_constant_sites=False):
    """"""
        Expand a nodes compressed sequence into the real sequence

        Parameters
        ----------
        node : PhyloTree.Clade
           Tree node

        Returns
        -------
        seq : np.array
           Sequence as np.array of chars
        """"""
    if '<MASK>':
        L = self.seq_len
    else:
        L = self.seq_len - self.additional_constant_sites
    return node.cseq[self.full_to_reduced_sequence_map[:L]]",False,['include_additional_constant_sites'],___________________,0.0
"def annotated(func, name=None):
    """"""Mark a function as callable from the command line.

    This function is meant to be called as decorator.  This function
    also initializes metadata about the function's arguments that is
    built up by the param decorator.

    Args:
        func (callable): The function that we wish to mark as callable
            from the command line.
        name (str): Optional string that will override the function's
            built-in name.
    """"""
    if '<MASK>':
        if '<MASK>':
            func.metadata = AnnotatedMetadata(func, name)
        return func
    func.metadata = AnnotatedMetadata(func, name)
    func.finalizer = False
    func.takes_cmdline = False
    func.decorated = False
    func.context = False
    return func",False,"[""hasattr(func, 'metadata')"", 'name is not None']",(((((((((((((((((((,0.0
"def read(self, amount):
    """"""Read a specified amount

        Reads will only be throttled if bandwidth limiting is enabled.
        """"""
    if '<MASK>':
        return self._fileobj.read(amount)
    self._bytes_seen += amount
    if '<MASK>':
        return self._fileobj.read(amount)
    self._consume_through_leaky_bucket()
    return self._fileobj.read(amount)",False,"['not self._bandwidth_limiting_enabled', 'self._bytes_seen < self._bytes_threshold']",___________________,0.0
"def register(self, desc, version=None, ignore_3D=False):
    """"""Register descriptors.

        Descriptor-like:
            * Descriptor instance: self
            * Descriptor class: use Descriptor.preset() method
            * module: use Descriptor-likes in module
            * Iterable: use Descriptor-likes in Iterable

        Parameters:
            desc(Descriptor-like): descriptors to register
            version(str): version
            ignore_3D(bool): ignore 3D descriptors

        """"""
    if '<MASK>':
        version = __version__
    version = StrictVersion(version)
    return self._register(desc, version, ignore_3D)",False,['version is None'],"_,,,,,,,,,,,,,,,,,,",0.0
"def predict_log_proba(self, X):
    """"""
        Return log-probability estimates for the RDD containing the
        test vector X.

        Parameters
        ----------
        X : RDD containing array-like items, shape = [m_samples, n_features]

        Returns
        -------
        C : RDD with array-like items, shape = [n_samples, n_classes]
            Returns the log-probability of the samples for each class in
            the model for each RDD block. The columns correspond to the classes
            in sorted order, as they appear in the attribute `classes_`.
        """"""
    if '<MASK>':
        return super(SparkBaseNB, self).predict_log_proba(X)
    check_rdd(X, (sp.spmatrix, np.ndarray))
    return X.map(lambda X: super(SparkBaseNB, self).predict_log_proba(X))",False,"['not isinstance(X, BlockRDD)']",___________________,0.0
"def is_confidential(client):
    if '<MASK>':
        return client.is_confidential
    client_type = getattr(client, 'client_type', None)
    if '<MASK>':
        return client_type == 'confidential'
    return True",False,"[""hasattr(client, 'is_confidential')"", 'client_type']",_type_type_type_type_type_type_type_type_type_,0.0
"def get(self, default=None):
    """"""Extract the smallest item from queue.
        Return default if queue is empty.""""""
    if '<MASK>':
        return default
    return heapq.heappop(self.__data)",False,['not self.__data'],"(self,,,,,,,,,,,,,,,,,",0.0
"def bayes_scale(s):
    """"""
    Remove mean and divide by standard deviation, using bayes_kvm statistics.
    """"""
    if '<MASK>':
        bm, bv, bs = bayes_mvs(s[~np.isnan(s)])
        return (s - bm.statistic) / bs.statistic
    else:
        return np.full(s.shape, np.nan)",False,['sum(~np.isnan(s)) > 1'],"(s,(s,(s,
    """"""
    """"""
    """"""
",0.0
"def serve(self):
    """"""
        Serve fuzzed JSON object
        """"""
    try:
        fuzzed = self.json.fuzzed
        if '<MASK>':
            self.client_queue.put((request.environ.get('REMOTE_ADDR'), fuzzed))
        response.headers.append('Access-Control-Allow-Origin', '*')
        response.headers.append('Accept-Encoding', 'identity')
        response.headers.append('Content-Type', self.config.content_type)
        if '<MASK>':
            PJFTestcaseServer.send_testcase(fuzzed, '127.0.0.1', self.config.ports['servers']['TCASE_PORT'])
        yield fuzzed
    except Exception as e:
        raise PJFBaseException(e.message if hasattr(e, 'message') else str(e))",False,"['self.config.fuzz_web', 'self.config.notify']",___________________,0.0
"def disambiguate_text(self, text, language=None, entities=None):
    """""" Call the disambiguation service in order to get meanings.

        Args:
            text (str): Text to be disambiguated.
            language (str): language of text (if known)
            entities (list): list of entities or mentions to be supplied by
                the user.

        Returns:
            dict, int: API response and API status.
        """"""
    body = {'text': text, 'entities': [], 'onlyNER': 'false', 'customisation': 'generic'}
    if '<MASK>':
        body['language'] = {'lang': language}
    if '<MASK>':
        body['entities'] = entities
    result, status_code = self._process_query(body)
    if '<MASK>':
        logger.debug('Disambiguation failed.')
    return (result, status_code)",False,"['language', 'entities', 'status_code != 200']",___________________,0.0
"def update_configuration(cfgfile=None):
    """"""Update configuration from file.

    :param cfgfile: Configuration file to load.
    """"""
    configobj.DEFAULT_INTERPOLATION = 'template'
    cfgfile = configuration_file(cfgfile)
    cfg = configobj.ConfigObj(cfgfile, configspec=cfgspec, encoding='utf-8')
    validator = Validator()
    val = cfg.validate(validator)
    if '<MASK>':
        raise ValueError('Invalid configuration: %s' % val)
    if '<MASK>':
        raise ValueError('List of files and flavors do not match')
    globals()['__config'] = cfg
    logger_init()
    if '<MASK>':
        logger.warning('Base URL ends with /. This is most likely a configuration error. The URL should contain nothing of the service paths.')
    logger.info('Configuration loaded from %s' % cfgfile)
    check()
    return cfg",False,"['val is not True', ""len(cfg['capture']['files']) != len(cfg['capture']['flavors'])"", ""cfg['server'].get('url', '').endswith('/')""]",___________________,0.0
"def iter_parse(fiql_str):
    """"""Iterate through the FIQL string. Yield a tuple containing the
    following FIQL components for each iteration:

      - preamble: Any operator or opening/closing paranthesis preceding a
        constraint or at the very end of the FIQL string.
      - selector: The selector portion of a FIQL constraint or ``None`` if
        yielding the last portion of the string.
      - comparison: The comparison portion of a FIQL constraint or ``None``
        if yielding the last portion of the string.
      - argument: The argument portion of a FIQL constraint or ``None`` if
        yielding the last portion of the string.

    For usage see :func:`parse_str_to_expression`.

    Args:
        fiql_str (string): The FIQL formatted string we want to parse.

    Yields:
        tuple: Preamble, selector, comparison, argument.
    """"""
    while len(fiql_str):
        constraint_match = CONSTRAINT_COMP.split(fiql_str, 1)
        if '<MASK>':
            yield (constraint_match[0], None, None, None)
            break
        yield (constraint_match[0], unquote_plus(constraint_match[1]), constraint_match[4], unquote_plus(constraint_match[6]) if constraint_match[6] else None)
        fiql_str = constraint_match[8]",False,['len(constraint_match) < 2'],___________________,0.0
"def _tree_load_nodes_dfs(self, parent_traj_node, load_data, with_links, recursive, max_depth, current_depth, trajectory, as_new, hdf5_group):
    """"""Loads a node from hdf5 file and if desired recursively everything below

        :param parent_traj_node: The parent node whose child should be loaded
        :param load_data: How to load the data
        :param with_links: If links should be loaded
        :param recursive: Whether loading recursively below hdf5_group
        :param max_depth: Maximum depth
        :param current_depth: Current depth
        :param trajectory: The trajectory object
        :param as_new: If trajectory is loaded as new
        :param hdf5_group: The hdf5 group containing the child to be loaded

        """"""
    if '<MASK>':
        max_depth = float('inf')
    loading_list = [(parent_traj_node, current_depth, hdf5_group)]
    while loading_list:
        parent_traj_node, current_depth, hdf5_group = loading_list.pop()
        if '<MASK>':
            if '<MASK>':
                self._tree_load_link(parent_traj_node, load_data=load_data, traj=trajectory, as_new=as_new, hdf5_soft_link=hdf5_group)
            continue
        name = hdf5_group._v_name
        is_leaf = self._all_get_from_attrs(hdf5_group, HDF5StorageService.LEAF)
        in_trajectory = name in parent_traj_node._children
        if '<MASK>':
            if '<MASK>':
                instance = parent_traj_node._children[name]
            else:
                instance = self._tree_create_leaf(name, trajectory, hdf5_group)
                parent_traj_node._add_leaf_from_storage(args=(instance,), kwargs={})
            self._prm_load_parameter_or_result(instance, load_data=load_data, _hdf5_group=hdf5_group)
            if '<MASK>':
                instance._stored = False
        else:
            if '<MASK>':
                traj_group = parent_traj_node._children[name]
                if '<MASK>':
                    traj_group.v_annotations.f_empty()
                    traj_group.v_comment = ''
            else:
                if '<MASK>':
                    class_name = self._all_get_from_attrs(hdf5_group, HDF5StorageService.CLASS_NAME)
                    class_constructor = trajectory._create_class(class_name)
                    instance = trajectory._construct_instance(class_constructor, name)
                    args = (instance,)
                else:
                    args = (name,)
                traj_group = parent_traj_node._add_group_from_storage(args=args, kwargs={})
            self._grp_load_group(traj_group, load_data=load_data, with_links=with_links, recursive=False, max_depth=max_depth, _traj=trajectory, _as_new=as_new, _hdf5_group=hdf5_group)
            if '<MASK>':
                new_depth = current_depth + 1
                for children in (hdf5_group._v_groups, hdf5_group._v_links):
                    for new_hdf5_group_name in children:
                        new_hdf5_group = children[new_hdf5_group_name]
                        loading_list.append((traj_group, new_depth, new_hdf5_group))",False,"['max_depth is None', 'isinstance(hdf5_group, pt.link.SoftLink)', 'is_leaf', 'with_links', 'in_trajectory', 'as_new', 'in_trajectory', 'recursive and current_depth < max_depth', 'load_data == pypetconstants.OVERWRITE_DATA', 'HDF5StorageService.CLASS_NAME in hdf5_group._v_attrs']",___________________,0.0
"def finalize(self):
    """"""Disables redirection""""""
    if '<MASK>':
        sys.stdout = self._original_steam
        print('Disabled redirection of `stdout`.')
        self._redirection = False
        self._original_steam = None",False,['self._original_steam is not None and self._redirection'],"_(self
        """""".............",0.0
"def sfen(self):
    """"""
        Gets an SFEN representation of the current position.
        """"""
    sfen = []
    empty = 0
    for square in SQUARES:
        piece = self.piece_at(square)
        if '<MASK>':
            empty += 1
        else:
            if '<MASK>':
                sfen.append(str(empty))
                empty = 0
            sfen.append(piece.symbol())
        if '<MASK>':
            if '<MASK>':
                sfen.append(str(empty))
                empty = 0
            if '<MASK>':
                sfen.append('/')
    sfen.append(' ')
    if '<MASK>':
        sfen.append('w')
    else:
        sfen.append('b')
    sfen.append(' ')
    pih_len = 0
    for color in COLORS:
        p = self.pieces_in_hand[color]
        pih_len += len(p)
        for piece_type in sorted(p.keys(), reverse=True):
            if '<MASK>':
                if '<MASK>':
                    sfen.append(str(p[piece_type]))
                piece = Piece(piece_type, color)
                sfen.append(piece.symbol())
    if '<MASK>':
        sfen.append('-')
    sfen.append(' ')
    sfen.append(str(self.move_number))
    return ''.join(sfen)",False,"['self.turn == WHITE', 'pih_len == 0', 'not piece', 'BB_SQUARES[square] & BB_FILE_1', 'empty', 'empty', 'square != I1', 'p[piece_type] >= 1', 'p[piece_type] > 1']",___________________,0.0
"def _get_samples(self, subset=None):
    """"""
        Helper function to get sample names from subset.

        Parameters
        ----------
        subset : str
            Subset name. If None, returns all samples.

        Returns
        -------
        List of sample names
        """"""
    if '<MASK>':
        samples = self.subsets['All_Samples']
    else:
        try:
            samples = self.subsets[subset]
        except KeyError:
            raise KeyError(""Subset '{:s}' does not "".format(subset) + ""exist.\nUse 'make_subset' to create a"" + 'subset.')
    return samples",False,['subset is None'],___________________,0.0
"def capacity_sp_meyerhof_and_hanna_1978(sp, fd, verbose=0):
    """"""
    Calculates the two-layered foundation capacity according Meyerhof and Hanna (1978)

    :param sp: Soil profile object
    :param fd: Foundation object
    :param wtl: water table level
    :param verbose: verbosity
    :return: ultimate bearing stress
    """"""
    assert isinstance(sp, sm.SoilProfile)
    sl_0 = sp.layer(1)
    sl_1 = sp.layer(2)
    h0 = sp.layer_depth(2)
    gwl = sp.gwl
    sl_0.nq_factor_0 = np.tan(np.pi / 4 + np.deg2rad(sl_0.phi / 2)) ** 2 * np.exp(np.pi * np.tan(np.deg2rad(sl_0.phi)))
    if '<MASK>':
        sl_0.nc_factor_0 = 5.14
    else:
        sl_0.nc_factor_0 = (sl_0.nq_factor_0 - 1) / np.tan(np.deg2rad(sl_0.phi))
    sl_0.ng_factor_0 = (sl_0.nq_factor_0 - 1) * np.tan(1.4 * np.deg2rad(sl_0.phi))
    sl_1.nq_factor_1 = np.tan(np.pi / 4 + np.deg2rad(sl_1.phi / 2)) ** 2 * np.exp(np.pi * np.tan(np.deg2rad(sl_1.phi)))
    if '<MASK>':
        sl_1.nc_factor_1 = 5.14
    else:
        sl_1.nc_factor_1 = (sl_1.nq_factor_1 - 1) / np.tan(np.deg2rad(sl_1.phi))
    sl_1.ng_factor_1 = (sl_1.nq_factor_1 - 1) * np.tan(1.4 * np.deg2rad(sl_1.phi))
    if '<MASK>':
        log('Nc: ', sl_1.nc_factor_1)
        log('Nq: ', sl_1.nq_factor_1)
        log('Ng: ', sl_1.ng_factor_1)
    sl_0.kp_0 = np.tan(np.pi / 4 + np.deg2rad(sl_0.phi / 2)) ** 2
    sl_1.kp_1 = np.tan(np.pi / 4 + np.deg2rad(sl_1.phi / 2)) ** 2
    if '<MASK>':
        sl_0.s_c_0 = 1 + 0.2 * sl_0.kp_0 * (fd.width / fd.length)
        sl_0.s_q_0 = 1.0 + 0.1 * sl_0.kp_0 * (fd.width / fd.length)
    else:
        sl_0.s_c_0 = 1 + 0.2 * (fd.width / fd.length)
        sl_0.s_q_0 = 1.0
    sl_0.s_g_0 = sl_0.s_q_0
    if '<MASK>':
        sl_1.s_c_1 = 1 + 0.2 * sl_1.kp_1 * (fd.width / fd.length)
        sl_1.s_q_1 = 1.0 + 0.1 * sl_1.kp_1 * (fd.width / fd.length)
    else:
        sl_1.s_c_1 = 1 + 0.2 * (fd.width / fd.length)
        sl_1.s_q_1 = 1.0
    sl_1.s_g_1 = sl_1.s_q_1
    a = 1
    s = 1
    r = 1 + fd.width / fd.length
    if '<MASK>':
        q_at_interface = sl_0.unit_bouy_weight * h0
        unit_eff_weight_0_at_fd_depth = sl_0.unit_bouy_weight
        unit_eff_weight_0_at_interface = sl_0.unit_bouy_weight
        unit_eff_weight_1_below_foundation = sl_1.unit_bouy_weight
    elif '<MASK>':
        q_at_interface = sl_0.unit_dry_weight * gwl + sl_0.unit_bouy_weight * (h0 - gwl)
        q_d = sl_0.unit_dry_weight * gwl + sl_0.unit_bouy_weight * (fd.depth - gwl)
        unit_eff_weight_0_at_fd_depth = q_d / fd.depth
        unit_eff_weight_0_at_interface = sl_0.unit_bouy_weight
        unit_eff_weight_1_below_foundation = sl_1.unit_bouy_weight
    elif '<MASK>':
        if '<MASK>':
            average_unit_bouy_weight = sl_0.unit_bouy_weight + (gwl - fd.depth) / fd.width * (sl_0.unit_dry_weight - sl_0.unit_bouy_weight)
            q_at_interface = sl_0.unit_dry_weight * gwl + sl_0.unit_bouy_weight * (h0 - gwl)
            unit_eff_weight_0_at_fd_depth = sl_0.unit_dry_weight
            unit_eff_weight_0_at_interface = average_unit_bouy_weight
            unit_eff_weight_1_below_foundation = sl_1.unit_bouy_weight
        else:
            average_unit_bouy_weight = sl_1.unit_bouy_weight + (gwl - h0) / fd.width * (sl_1.unit_dry_weight - sl_1.unit_bouy_weight)
            q_at_interface = sl_0.unit_dry_weight * h0
            unit_eff_weight_0_at_fd_depth = sl_0.unit_dry_weight
            unit_eff_weight_0_at_interface = sl_0.unit_dry_weight
            unit_eff_weight_1_below_foundation = average_unit_bouy_weight
    elif '<MASK>':
        q_at_interface = sl_0.unit_dry_weight * h0
        unit_eff_weight_0_at_fd_depth = sl_0.unit_dry_weight
        unit_eff_weight_0_at_interface = sl_0.unit_dry_weight
        unit_eff_weight_1_below_foundation = sl_1.unit_dry_weight
    else:
        raise ValueError('Could not interpret inputs')
    q_ult6 = q_at_interface - unit_eff_weight_0_at_fd_depth * fd.depth
    q_0 = sl_0.cohesion * sl_0.nc_factor_0 + 0.5 * unit_eff_weight_0_at_interface * fd.width * sl_0.ng_factor_0
    q_b2 = q_at_interface * sl_1.nq_factor_1 * sl_1.s_q_1
    q_1 = sl_1.cohesion * sl_1.nc_factor_1 + 0.5 * unit_eff_weight_1_below_foundation * fd.width * sl_1.ng_factor_1
    q_b3 = unit_eff_weight_1_below_foundation * fd.width * sl_1.ng_factor_1 * sl_1.s_g_1 / 2
    q_ult5 = r * (unit_eff_weight_0_at_interface * (h0 - fd.depth) ** 2) * (1 + 2 * fd.depth / (h0 - fd.depth)) * (np.tan(np.deg2rad(sl_0.phi)) / fd.width) * s
    q_t2 = unit_eff_weight_0_at_fd_depth * fd.depth * sl_0.nq_factor_0 * sl_0.s_q_0
    q_t3 = unit_eff_weight_0_at_interface * fd.width * sl_0.ng_factor_0 * sl_0.s_g_0 / 2
    q_b1 = sl_1.cohesion * sl_1.nc_factor_1 * sl_1.s_c_1
    q_b = q_b1 + q_b2 + q_b3
    q1_q0 = q_1 / q_0
    x = np.array([0.0, 0.082, 0.206, 0.298, 0.404, 0.509, 0.598, 0.685, 0.772])
    y = np.array([0.627, 0.7, 0.794, 0.855, 0.912, 0.948, 0.968, 0.983, 0.997])
    ca_c0 = np.interp(q1_q0, x, y)
    ca = ca_c0 * sl_0.cohesion
    x_0 = np.array([0, 20.08, 22.42, 25.08, 27.58, 30.08, 32.58, 34.92, 37.83, 40.0, 42.67, 45.0, 47.0, 49.75])
    y_0 = np.array([0.93, 0.93, 0.93, 0.93, 1.01, 1.17, 1.32, 1.56, 1.87, 2.26, 2.72, 3.35, 3.81, 4.82])
    x_2 = np.array([0, 20.08, 22.5, 25.08, 27.58, 30.08, 32.5, 35.0, 37.67, 40.17, 42.67, 45.0, 47.5, 50.0])
    y_2 = np.array([1.55, 1.55, 1.71, 1.86, 2.1, 2.33, 2.72, 3.11, 3.81, 4.43, 5.28, 6.14, 7.46, 9.24])
    x_4 = np.array([0, 20.0, 22.51, 25.1, 27.69, 30.11, 32.45, 35.04, 37.88, 40.14, 42.65, 45.07, 47.33, 50.08])
    y_4 = np.array([2.49, 2.49, 2.64, 2.87, 3.34, 3.81, 4.43, 5.2, 6.29, 7.38, 9.01, 11.11, 14.29, 19.34])
    x_10 = np.array([0, 20.0, 22.5, 25.08, 28.0, 30.0, 32.5, 34.92, 37.5, 40.17, 42.42, 45.0, 47.17, 50.08])
    y_10 = np.array([3.27, 3.27, 3.74, 4.44, 5.37, 6.07, 7.16, 8.33, 10.04, 12.3, 15.95, 21.17, 27.47, 40.0])
    x_int = sl_0.phi
    if '<MASK>':
        fd.ks = 0
    elif '<MASK>':
        fd.ks = np.interp(x_int, x_0, y_0)
    elif '<MASK>':
        fd.ks = np.interp(x_int, x_2, y_2)
    elif '<MASK>':
        fd.ks = np.interp(x_int, x_4, y_4)
    elif '<MASK>':
        fd.ks = np.interp(x_int, x_10, y_10)
    elif '<MASK>':
        ks_1 = np.interp(x_int, x_0, y_0)
        ks_2 = np.interp(x_int, x_2, y_2)
        fd.ks = (ks_2 - ks_1) * q1_q0 / 0.2 + ks_1
    elif '<MASK>':
        ks_1 = np.interp(x_int, x_2, y_2)
        ks_2 = np.interp(x_int, x_4, y_4)
        fd.ks = (ks_2 - ks_1) * (q1_q0 - 0.2) / 0.2 + ks_1
    elif '<MASK>':
        ks_1 = np.interp(x_int, x_4, y_4)
        ks_2 = np.interp(x_int, x_10, y_10)
        fd.ks = (ks_2 - ks_1) * (q1_q0 - 0.4) / 0.6 + ks_1
    else:
        raise DesignError(""Cannot compute 'ks', bearing ratio out-of-range (q1_q0 = %.3f) required: 0-1."" % q1_q0)
    q_ult4 = r * (2 * ca * (h0 - fd.depth) / fd.width) * a
    q_ult5_ks = q_ult5 * fd.ks
    q_ult = q_b + q_ult4 + q_ult5_ks - q_ult6
    q_t1 = sl_0.cohesion * sl_0.nc_factor_0 * sl_0.s_c_0
    q_t = q_t1 + q_t2 + q_t3
    if '<MASK>':
        if '<MASK>':
            fd.q_ult = q_t
        else:
            vert_eff_stress_interface = sp.vertical_effective_stress(h0)
            vert_eff_stress_lowest = sp.vertical_effective_stress(fd.width + fd.depth)
            average_eff_stress = (vert_eff_stress_interface + vert_eff_stress_lowest) / 2
            c_2_eff = sl_1.cohesion + average_eff_stress * np.tan(np.radians(sl_1.phi))
            if '<MASK>':
                fd.q_ult = q_t
            else:
                h_over_b = (h0 - fd.depth) / fd.width
                c1_over_c2 = sl_0.cohesion / c_2_eff
                c_1_over_c_2 = [0.1, 0.2, 0.25, 0.333, 0.5, 0.667, 1.0]
                m_1 = [1.584, 1.444, 1.389, 1.311, 1.193, 1.109, 1.0]
                m_125 = [1.446, 1.342, 1.302, 1.241, 1.152, 1.088, 1.0]
                m_167 = [1.302, 1.235, 1.208, 1.167, 1.107, 1.064, 1.0]
                m_25 = [1.154, 1.121, 1.107, 1.088, 1.056, 1.033, 1.0]
                m_5 = [1, 1, 1, 1, 1, 1, 1]
                if '<MASK>':
                    m = np.interp(c1_over_c2, c_1_over_c_2, m_1)
                elif '<MASK>':
                    m = np.interp(c1_over_c2, c_1_over_c_2, m_125)
                elif '<MASK>':
                    m = np.interp(c1_over_c2, c_1_over_c_2, m_167)
                elif '<MASK>':
                    m = np.interp(c1_over_c2, c_1_over_c_2, m_25)
                elif '<MASK>':
                    m = np.interp(c1_over_c2, c_1_over_c_2, m_5)
                elif '<MASK>':
                    m_a = np.interp(c1_over_c2, c_1_over_c_2, m_1)
                    m_b = np.interp(c1_over_c2, c_1_over_c_2, m_125)
                    m = np.interp(h_over_b, [0.1, 0.125], [m_a, m_b])
                elif '<MASK>':
                    m_a = np.interp(c1_over_c2, c_1_over_c_2, m_125)
                    m_b = np.interp(c1_over_c2, c_1_over_c_2, m_167)
                    m = np.interp(h_over_b, [0.125, 0.167], [m_a, m_b])
                elif '<MASK>':
                    m_a = np.interp(c1_over_c2, c_1_over_c_2, m_167)
                    m_b = np.interp(c1_over_c2, c_1_over_c_2, m_25)
                    m = np.interp(h_over_b, [0.167, 0.25], [m_a, m_b])
                elif '<MASK>':
                    m_a = np.interp(c1_over_c2, c_1_over_c_2, m_25)
                    m_b = np.interp(c1_over_c2, c_1_over_c_2, m_5)
                    m = np.interp(h_over_b, [0.25, 0.5], [m_a, m_b])
                fd.q_ult = sl_0.cohesion * m * sl_0.nc_factor_0 + unit_eff_weight_0_at_fd_depth * fd.depth
    else:
        fd.q_ult = q_ult
    return fd.q_ult",False,"['sl_0.phi == 0', 'sl_1.phi == 0', 'verbose', 'sl_0.phi >= 10', 'sl_1.phi >= 10', 'gwl == 0', 'sl_0.phi < 1', 'q_ult > q_t', '0 < gwl <= fd.depth', 'q1_q0 == 0', 'h0 > fd.width / 2', 'fd.depth < gwl <= fd.width + fd.depth', 'q1_q0 == 0.2', 'sl_0.cohesion > c_2_eff', 'gwl < h0', 'gwl > fd.depth + fd.width', 'q1_q0 == 0.4', 'h_over_b == 0.1', 'q1_q0 == 1.0', 'h_over_b == 0.125', '0 < q1_q0 < 0.2', 'h_over_b == 0.167', '0.2 < q1_q0 < 0.4', 'h_over_b == 0.25', '0.4 < q1_q0 < 1.0', 'h_over_b >= 0.5', '0.1 < h_over_b < 0.125', '0.125 < h_over_b < 0.167', '0.167 < h_over_b < 0.25', '0.25 < h_over_b < 0.5']",___________________,0.0
"def create(self, roomId=None, toPersonId=None, toPersonEmail=None, text=None, markdown=None, files=None, **request_parameters):
    """"""Post a message, and optionally a attachment, to a room.

        The files parameter is a list, which accepts multiple values to allow
        for future expansion, but currently only one file may be included with
        the message.

        Args:
            roomId(basestring): The room ID.
            toPersonId(basestring): The ID of the recipient when sending a
                private 1:1 message.
            toPersonEmail(basestring): The email address of the recipient when
                sending a private 1:1 message.
            text(basestring): The message, in plain text. If `markdown` is
                specified this parameter may be optionally used to provide
                alternate text for UI clients that do not support rich text.
            markdown(basestring): The message, in markdown format.
            files(`list`): A list of public URL(s) or local path(s) to files to
                be posted into the room. Only one file is allowed per message.
            **request_parameters: Additional request parameters (provides
                support for parameters that may be added in the future).

        Returns:
            Message: A Message object with the details of the created message.

        Raises:
            TypeError: If the parameter types are incorrect.
            ApiError: If the Webex Teams cloud returns an error.
            ValueError: If the files parameter is a list of length > 1, or if
                the string in the list (the only element in the list) does not
                contain a valid URL or path to a local file.

        """"""
    check_type(roomId, basestring)
    check_type(toPersonId, basestring)
    check_type(toPersonEmail, basestring)
    check_type(text, basestring)
    check_type(markdown, basestring)
    check_type(files, list)
    if '<MASK>':
        if '<MASK>':
            raise ValueError('The length of the `files` list is greater than one (1). The files parameter is a list, which accepts multiple values to allow for future expansion, but currently only one file may be included with the message.')
        check_type(files[0], basestring)
    post_data = dict_from_items_with_values(request_parameters, roomId=roomId, toPersonId=toPersonId, toPersonEmail=toPersonEmail, text=text, markdown=markdown, files=files)
    if '<MASK>':
        json_data = self._session.post(API_ENDPOINT, json=post_data)
    elif '<MASK>':
        try:
            post_data['files'] = open_local_file(files[0])
            multipart_data = MultipartEncoder(post_data)
            headers = {'Content-type': multipart_data.content_type}
            json_data = self._session.post(API_ENDPOINT, headers=headers, data=multipart_data)
        finally:
            post_data['files'].file_object.close()
    else:
        raise ValueError('The `files` parameter does not contain a vaild URL or path to a local file.')
    return self._object_factory(OBJECT_TYPE, json_data)",False,"['files', 'not files or is_web_url(files[0])', 'len(files) != 1', 'is_local_file(files[0])']",___________________,0.0
"def get_from_thread_name_or_id(self, name_or_id, report_error=True):
    """"""See if *name_or_id* is either a thread name or a thread id.
        The frame of that id/name is returned, or None if name_or_id is
        invalid.""""""
    thread_id = self.proc.get_int_noerr(name_or_id)
    if '<MASK>':
        name2id = Mthread.map_thread_names()
        if '<MASK>':
            name_or_id = Mthread.current_thread_name()
            pass
        thread_id = name2id.get(name_or_id)
        if '<MASK>':
            self.errmsg(""I don't know about thread name %s."" % name_or_id)
            return (None, None)
        pass
    threads = sys._current_frames()
    frame = threads.get(thread_id)
    if '<MASK>':
        self.errmsg(""I don't know about thread number %s (%d)."" % name_or_id, thread_id)
        return (None, None)
    return (frame, thread_id)",False,"['thread_id is None', 'frame is None and report_error', ""name_or_id == '.'"", 'thread_id is None']",___________________,0.0
"def prepare(data):
    """"""
    Try to get current process ready to unpickle process object
    """"""
    if '<MASK>':
        process.current_process().name = data['name']
    if '<MASK>':
        process.current_process().authkey = data['authkey']
    if '<MASK>':
        util.log_to_stderr()
    if '<MASK>':
        util.get_logger().setLevel(data['log_level'])
    if '<MASK>':
        import logging
        util.get_logger().handlers[0].setFormatter(logging.Formatter(data['log_fmt']))
    if '<MASK>':
        sys.path = data['sys_path']
    if '<MASK>':
        sys.argv = data['sys_argv']
    if '<MASK>':
        os.chdir(data['dir'])
    if '<MASK>':
        process.ORIGINAL_DIR = data['orig_dir']
    if '<MASK>':
        from . import semaphore_tracker
        semaphore_tracker._semaphore_tracker._pid = data['tracker_pid']
    if '<MASK>':
        _fixup_main_from_name(data['init_main_from_name'])
    elif '<MASK>':
        _fixup_main_from_path(data['init_main_from_path'])",False,"[""'name' in data"", ""'authkey' in data"", ""'log_to_stderr' in data and data['log_to_stderr']"", ""'log_level' in data"", ""'log_fmt' in data"", ""'sys_path' in data"", ""'sys_argv' in data"", ""'dir' in data"", ""'orig_dir' in data"", ""'tracker_pid' in data"", ""'init_main_from_name' in data"", ""'init_main_from_path' in data""]",___________________,0.0
"def decode(self, packet):
    """"""
        Decode a PUBLISH control packet. 
        """"""
    self.encoded = packet
    lenLen = 1
    while packet[lenLen] & 128:
        lenLen += 1
    packet_remaining = packet[lenLen + 1:]
    self.dup = packet[0] & 8 == 8
    self.qos = (packet[0] & 6) >> 1
    self.retain = packet[0] & 1 == 1
    self.topic, _ = decodeString(packet_remaining)
    topicLen = decode16Int(packet_remaining)
    if '<MASK>':
        self.msgId = decode16Int(packet_remaining[topicLen + 2:topicLen + 4])
        self.payload = packet_remaining[topicLen + 4:]
    else:
        self.msgId = None
        self.payload = packet_remaining[topicLen + 2:]",False,['self.qos'],___________________,0.0
"def attach_to_tangle(self, trunk_transaction, branch_transaction, trytes, min_weight_magnitude=None):
    """"""
        Attaches the specified transactions (trytes) to the Tangle by
        doing Proof of Work. You need to supply branchTransaction as
        well as trunkTransaction (basically the tips which you're going
        to validate and reference with this transaction) - both of which
        you'll get through the getTransactionsToApprove API call.

        The returned value is a different set of tryte values which you
        can input into :py:meth:`broadcast_transactions` and
        :py:meth:`store_transactions`.

        References:

        - https://iota.readme.io/docs/attachtotangle
        """"""
    if '<MASK>':
        min_weight_magnitude = self.default_min_weight_magnitude
    return core.AttachToTangleCommand(self.adapter)(trunkTransaction=trunk_transaction, branchTransaction=branch_transaction, minWeightMagnitude=min_weight_magnitude, trytes=trytes)",False,['min_weight_magnitude is None'],___________________,0.0
"def prompt(message='', title='', default='', multiline=False, password=None, parent=None):
    """"""Modal dialog asking for an input, returns string or None if cancelled""""""
    if '<MASK>':
        style = wx.TE_PASSWORD | wx.OK | wx.CANCEL
        result = dialogs.textEntryDialog(parent, message, title, default, style)
    elif '<MASK>':
        style = wx.TE_MULTILINE | wx.OK | wx.CANCEL
        result = dialogs.textEntryDialog(parent, message, title, default, style)
        result.text = '\n'.join(result.text.splitlines())
    else:
        result = dialogs.textEntryDialog(parent, message, title, default)
    if '<MASK>':
        return result.text",False,"['password', 'result.accepted', 'multiline']","_,,,,,,,,,,,,,,,,,,",0.0
"def uuids(self):
    """""" Extract uuid from each item of specified ``seq``.
        """"""
    for f in self._seq:
        if '<MASK>':
            yield f.uuid
        elif '<MASK>':
            yield f
        else:
            raise ValueError('Invalid type for sequence item: {0}'.format(type(f)))",False,"['isinstance(f, File)', 'isinstance(f, six.string_types)']",(((((((((((((((((((,0.0
"def isSame(self, other: HdlStatement) -> bool:
    """"""
        Doc on parent class :meth:`HdlStatement.isSame`
        """"""
    if '<MASK>':
        return True
    if '<MASK>':
        return False
    if '<MASK>':
        for (ac, astm), (bc, bstm) in zip(self.cases, other.cases):
            if '<MASK>':
                return False
        return True
    return False",False,"['self is other', 'self.rank != other.rank', 'isinstance(other, SwitchContainer) and isSameHVal(self.switchOn, other.switchOn) and (len(self.cases) == len(other.cases)) and isSameStatementList(self.default, other.default)', 'not isSameHVal(ac, bc) or not isSameStatementList(astm, bstm)']","_,,,,,,,,,,,,,,,,,,",0.0
"def _get_entry(self, entry, entry_tree):
    """"""Helper function for retrieving a particular entry from the prefix trees""""""
    for e in entry_tree[entry.filename]:
        if '<MASK>':
            return e",False,['entry == e'],_tree[_tree[_tree[_tree[_tree[_tree[_,0.0
"def reader_acquire(self):
    """"""Acquire the lock to read""""""
    self._order_mutex.acquire()
    self._readers_mutex.acquire()
    if '<MASK>':
        self._access_mutex.acquire()
    self._readers += 1
    self._order_mutex.release()
    self._readers_mutex.release()",False,['self._readers == 0'],"_(self
    """""" """"""
    """""" """"""
    """"""
    """"""
   ",0.0
"def cumsum(df, new_column: str, column: str, index: list, date_column: str, date_format: str):
    """"""
    DEPRECATED - please use `compute_cumsum` instead
    """"""
    logging.getLogger(__name__).warning(f'DEPRECATED: use compute_cumsum')
    date_temp = '__date_temp__'
    if '<MASK>':
        index = [index]
    levels = list(range(0, len(index)))
    df[date_temp] = pd.to_datetime(df[date_column], format=date_format)
    reference_cols = [date_temp, date_column]
    df = df.groupby(index + reference_cols).sum()
    df[new_column] = df.groupby(level=levels)[column].cumsum()
    df.reset_index(inplace=True)
    del df[date_temp]
    return df",False,"['isinstance(index, str)']","______))
    """"""
    """"""
    """"""
   ",0.0
"def generate_action(args):
    """"""Generate action.""""""
    controller = args.get('<controller>')
    action = args.get('<action>')
    with_template = args.get('-t')
    current_path = os.getcwd()
    logger.info('Start generating action.')
    controller_file_path = os.path.join(current_path, 'application/controllers', controller + '.py')
    if '<MASK>':
        logger.warning(""The controller %s does't exist."" % controller)
        return
    if '<MASK>':
        action_source_path = os.path.join(dirname(abspath(__file__)), 'templates/action.py')
    else:
        action_source_path = os.path.join(dirname(abspath(__file__)), 'templates/action_without_template.py')
    with open(action_source_path, 'r') as action_source_file:
        with open(controller_file_path, 'a') as controller_file:
            for action_line in action_source_file:
                new_line = action_line.replace('#{controller}', controller).replace('#{action}', action)
                controller_file.write(new_line)
    logger.info('Updated: %s' % _relative_path(controller_file_path))
    if '<MASK>':
        assets_dir_path = os.path.join(current_path, 'application/pages/%s/%s' % (controller, action))
        _mkdir_p(assets_dir_path)
        action_html_template_path = os.path.join(dirname(abspath(__file__)), 'templates/action.html')
        action_html_path = os.path.join(assets_dir_path, '%s.html' % action)
        with open(action_html_template_path, 'r') as action_html_template_file:
            with open(action_html_path, 'w') as action_html_file:
                for line in action_html_template_file:
                    new_line = line.replace('#{action}', action).replace('#{action|title}', action.title()).replace('#{controller}', controller)
                    action_html_file.write(new_line)
        logger.info('New: %s' % _relative_path(action_html_path))
        action_js_template_path = os.path.join(dirname(abspath(__file__)), 'templates/action.js')
        action_js_path = os.path.join(assets_dir_path, '%s.js' % action)
        shutil.copy(action_js_template_path, action_js_path)
        logger.info('New: %s' % _relative_path(action_js_path))
        action_less_template_path = os.path.join(dirname(abspath(__file__)), 'templates/action.less')
        action_less_path = os.path.join(assets_dir_path, '%s.less' % action)
        shutil.copy(action_less_template_path, action_less_path)
        logger.info('New: %s' % _relative_path(action_less_path))
    logger.info('Finish generating action.')",False,"['not os.path.exists(controller_file_path)', 'with_template', 'with_template']",___________________,0.0
"def parse_aggregate_report_xml(xml, nameservers=None, timeout=2.0, parallel=False):
    """"""Parses a DMARC XML report string and returns a consistent OrderedDict

    Args:
        xml (str): A string of DMARC aggregate report XML
        nameservers (list): A list of one or more nameservers to use
        (Cloudflare's public DNS resolvers by default)
        timeout (float): Sets the DNS timeout in seconds
        parallel (bool): Parallel processing

    Returns:
        OrderedDict: The parsed aggregate DMARC report
    """"""
    errors = []
    try:
        xmltodict.parse(xml)['feedback']
    except Exception as e:
        errors.append(e.__str__())
    try:
        xml = xml_header_regex.sub('<?xml version=""1.0""?>', xml)
        xml = xml_schema_regex.sub('', xml)
        report = xmltodict.parse(xml)['feedback']
        report_metadata = report['report_metadata']
        schema = 'draft'
        if '<MASK>':
            schema = report['version']
        new_report = OrderedDict([('xml_schema', schema)])
        new_report_metadata = OrderedDict()
        if '<MASK>':
            if '<MASK>':
                report_metadata['org_name'] = report_metadata['email'].split('@')[-1]
        org_name = report_metadata['org_name']
        if '<MASK>':
            org_name = get_base_domain(org_name)
        new_report_metadata['org_name'] = org_name
        new_report_metadata['org_email'] = report_metadata['email']
        extra = None
        if '<MASK>':
            extra = report_metadata['extra_contact_info']
        new_report_metadata['org_extra_contact_info'] = extra
        new_report_metadata['report_id'] = report_metadata['report_id']
        report_id = new_report_metadata['report_id']
        report_id = report_id.replace('<', '').replace('>', '').split('@')[0]
        new_report_metadata['report_id'] = report_id
        date_range = report['report_metadata']['date_range']
        date_range['begin'] = timestamp_to_human(date_range['begin'])
        date_range['end'] = timestamp_to_human(date_range['end'])
        new_report_metadata['begin_date'] = date_range['begin']
        new_report_metadata['end_date'] = date_range['end']
        if '<MASK>':
            if '<MASK>':
                errors = [report['report_metadata']['error']]
            else:
                errors = report['report_metadata']['error']
        new_report_metadata['errors'] = errors
        new_report['report_metadata'] = new_report_metadata
        records = []
        policy_published = report['policy_published']
        new_policy_published = OrderedDict()
        new_policy_published['domain'] = policy_published['domain']
        adkim = 'r'
        if '<MASK>':
            if '<MASK>':
                adkim = policy_published['adkim']
        new_policy_published['adkim'] = adkim
        aspf = 'r'
        if '<MASK>':
            if '<MASK>':
                aspf = policy_published['aspf']
        new_policy_published['aspf'] = aspf
        new_policy_published['p'] = policy_published['p']
        sp = new_policy_published['p']
        if '<MASK>':
            if '<MASK>':
                sp = report['policy_published']['sp']
        new_policy_published['sp'] = sp
        pct = '100'
        if '<MASK>':
            if '<MASK>':
                pct = report['policy_published']['pct']
        new_policy_published['pct'] = pct
        fo = '0'
        if '<MASK>':
            if '<MASK>':
                fo = report['policy_published']['fo']
        new_policy_published['fo'] = fo
        new_report['policy_published'] = new_policy_published
        if '<MASK>':
            for record in report['record']:
                report_record = _parse_report_record(record, nameservers=nameservers, dns_timeout=timeout, parallel=parallel)
                records.append(report_record)
        else:
            report_record = _parse_report_record(report['record'], nameservers=nameservers, dns_timeout=timeout, parallel=parallel)
            records.append(report_record)
        new_report['records'] = records
        return new_report
    except expat.ExpatError as error:
        raise InvalidAggregateReport('Invalid XML: {0}'.format(error.__str__()))
    except KeyError as error:
        raise InvalidAggregateReport('Missing field: {0}'.format(error.__str__()))
    except AttributeError:
        raise InvalidAggregateReport('Report missing required section')
    except Exception as error:
        raise InvalidAggregateReport('Unexpected error: {0}'.format(error.__str__()))",False,"[""'version' in report"", ""report_metadata['org_name'] is None"", 'org_name is not None', ""'extra_contact_info' in report_metadata"", ""'error' in report['report_metadata']"", ""'adkim' in policy_published"", ""'aspf' in policy_published"", ""'sp' in policy_published"", ""'pct' in policy_published"", ""'fo' in policy_published"", ""type(report['record']) == list"", ""report_metadata['email'] is not None"", ""type(report['report_metadata']['error']) != list"", ""policy_published['adkim'] is not None"", ""policy_published['aspf'] is not None"", ""policy_published['sp'] is not None"", ""policy_published['pct'] is not None"", ""policy_published['fo'] is not None""]",___________________,0.0
"def calculate_top_margin(self):
    """"""
		Calculate the margin in pixels above the plot area, setting
		border_top.
		""""""
    self.border_top = 5
    if '<MASK>':
        self.border_top += self.title_font_size
    self.border_top += 5
    if '<MASK>':
        self.border_top += self.subtitle_font_size",False,"['self.show_graph_title', 'self.show_graph_subtitle']","_(self):
    """"""
    """"""
    """"""
    """"""
    """"""",0.0
"def vcdTypeInfoForHType(t) -> Tuple[str, int, Callable[[RtlSignalBase, Value], str]]:
    """"""
    :return: (vcd type name, vcd width)
    """"""
    if '<MASK>':
        return (VCD_SIG_TYPE.WIRE, t.bit_length(), vcdBitsFormatter)
    elif '<MASK>':
        return (VCD_SIG_TYPE.REAL, 1, vcdEnumFormatter)
    else:
        raise ValueError(t)",False,"['isinstance(t, (SimBitsT, Bits, HBool))', 'isinstance(t, HEnum)']",___________________,0.0
"def server(self):
    """"""
        All in one endpoints. This property is created automaticly
        if you have implemented all the getters and setters.
        """"""
    if '<MASK>':
        return Server(self._validator)
    if '<MASK>':
        validator = OAuth1RequestValidator(clientgetter=self._clientgetter, tokengetter=self._tokengetter, tokensetter=self._tokensetter, grantgetter=self._grantgetter, grantsetter=self._grantsetter, noncegetter=self._noncegetter, noncesetter=self._noncesetter, verifiergetter=self._verifiergetter, verifiersetter=self._verifiersetter, config=self.app.config)
        self._validator = validator
        server = Server(validator)
        if '<MASK>':
            server._check_signature = lambda *args, **kwargs: True
        return server
    raise RuntimeError('application not bound to required getters and setters')",False,"[""hasattr(self, '_validator')"", ""hasattr(self, '_clientgetter') and hasattr(self, '_tokengetter') and hasattr(self, '_tokensetter') and hasattr(self, '_noncegetter') and hasattr(self, '_noncesetter') and hasattr(self, '_grantgetter') and hasattr(self, '_grantsetter') and hasattr(self, '_verifiergetter') and hasattr(self, '_verifiersetter')"", 'self.app.testing']",___________________,0.0
"def encode(self, x, layer=None, sample=False, **kwargs):
    """"""Encode a dataset using the hidden layer activations of our network.

        Parameters
        ----------
        x : ndarray
            A dataset to encode. Rows of this dataset capture individual data
            points, while columns represent the variables in each data point.

        layer : str, optional
            The name of the hidden layer output to use. By default, we use
            the ""middle"" hidden layer---for example, for a 4,2,4 or 4,3,2,3,4
            autoencoder, we use the layer with size 2.

        sample : bool, optional
            If True, then draw a sample using the hidden activations as
            independent Bernoulli probabilities for the encoded data. This
            assumes the hidden layer has a logistic sigmoid activation function.

        Returns
        -------
        ndarray :
            The given dataset, encoded by the appropriate hidden layer
            activation.
        """"""
    enc = self.feed_forward(x, **kwargs)[self._find_output(layer)]
    if '<MASK>':
        return np.random.binomial(n=1, p=enc).astype(np.uint8)
    return enc",False,['sample'],",,,,,,,,,,,,,,,,,,,",0.0
"def save_log(self, directory=None, logname=None, header=None):
    """"""
        Save analysis.lalog in specified location
        """"""
    if '<MASK>':
        directory = self.export_dir
    if '<MASK>':
        directory = os.path.dirname(directory)
    if '<MASK>':
        logname = 'analysis.lalog'
    if '<MASK>':
        header = self._log_header()
    loc = logging.write_logfile(self.log, header, os.path.join(directory, logname))
    return loc",False,"['directory is None', 'not os.path.isdir(directory)', 'logname is None', 'header is None']",___________________,0.0
"def signature(self, name=None):
    """"""Return our function signature as a string.

        By default this function uses the annotated name of the function
        however if you need to override that with a custom name you can
        pass name=<custom name>

        Args:
            name (str): Optional name to override the default name given
                in the function signature.

        Returns:
            str: The formatted function signature
        """"""
    self._ensure_loaded()
    if '<MASK>':
        name = self.name
    num_args = len(self.arg_names)
    num_def = 0
    if '<MASK>':
        num_def = len(self.arg_defaults)
    num_no_def = num_args - num_def
    args = []
    for i in range(0, len(self.arg_names)):
        typestr = ''
        if '<MASK>':
            typestr = '{} '.format(self.annotated_params[self.arg_names[i]].type_name)
        if '<MASK>':
            default = str(self.arg_defaults[i - num_no_def])
            if '<MASK>':
                default = ""''""
            args.append('{}{}={}'.format(typestr, str(self.arg_names[i]), default))
        else:
            args.append(typestr + str(self.arg_names[i]))
    return '{}({})'.format(name, ', '.join(args))",False,"['name is None', 'self.arg_defaults is not None', 'self.arg_names[i] in self.annotated_params', 'i >= num_no_def', 'len(default) == 0']","__,,_,,_,,_,,,,,,,,",0.0
"def assert_no_title(self, title, **kwargs):
    """"""
        Asserts that the page doesn't have the given title.

        Args:
            title (str | RegexObject): The string that the title should include.
            **kwargs: Arbitrary keyword arguments for :class:`TitleQuery`.

        Returns:
            True

        Raises:
            ExpectationNotMet: If the assertion hasn't succeeded during the wait time.
        """"""
    query = TitleQuery(title, **kwargs)

    @self.synchronize(wait=query.wait)
    def assert_no_title():
        if '<MASK>':
            raise ExpectationNotMet(query.negative_failure_message)
        return True
    return assert_no_title()",False,['query.resolves_for(self)'],"_____title,
    """"""
    """"""
    """"""
    """"""",0.0
"def show(self, commits=None, encoding='utf-8'):
    """"""Show the data of a set of commits.

        The method returns the output of Git show command for a
        set of commits using the following options:

            git show --raw --numstat --pretty=fuller --decorate=full
                --parents -M -C -c [<commit>...<commit>]

        When the list of commits is empty, the command will return
        data about the last commit, like the default behaviour of
        `git show`.

        :param commits: list of commits to show data
        :param encoding: encode the output using this format

        :returns: a generator where each item is a line from the show output

        :raises EmptyRepositoryError: when the repository is empty and
            the action cannot be performed
        :raises RepositoryError: when an error occurs fetching the show output
        """"""
    if '<MASK>':
        logger.warning('Git %s repository is empty; unable to run show', self.uri)
        raise EmptyRepositoryError(repository=self.uri)
    if '<MASK>':
        commits = []
    cmd_show = ['git', 'show']
    cmd_show.extend(self.GIT_PRETTY_OUTPUT_OPTS)
    cmd_show.extend(commits)
    for line in self._exec_nb(cmd_show, cwd=self.dirpath, env=self.gitenv):
        yield line
    logger.debug('Git show fetched from %s repository (%s)', self.uri, self.dirpath)",False,"['self.is_empty()', 'commits is None']",___________________,0.0
"def delete(self, **kwargs):
    """"""
        Deletes the entity immediately. Also performs any on_delete operations
        specified as part of column definitions.
        """"""
    if '<MASK>':
        self._before_delete()
        _on_delete(self)
    session.forget(self)
    self._apply_changes(self._last, {}, delete=True, _conn=kwargs.get('_conn'))
    self._modified = True
    self._deleted = True
    if '<MASK>':
        self._after_delete()",False,"[""kwargs.get('skip_on_delete_i_really_mean_it') is not SKIP_ON_DELETE"", ""kwargs.get('skip_on_delete_i_really_mean_it') is not SKIP_ON_DELETE""]","_,,,,,,,,,,,,,,,,,,",0.0
"def create_dashboard_from_file(self, dashboard_name, filename, filter, shared=False, public=False):
    """"""
        **Description**
            Create a new dasboard using a dashboard template saved to disk. See :func:`~SdcClient.save_dashboard_to_file` to use the file to create a dashboard (usefl to create and restore backups).

            The file can contain the following JSON formats:
            1. dashboard object in the format of an array element returned by :func:`~SdcClient.get_dashboards`
            2. JSON object with the following properties:
                * version: dashboards API version (e.g. 'v2')
                * dashboard: dashboard object in the format of an array element returned by :func:`~SdcClient.get_dashboards`

        **Arguments**
            - **dashboard_name**: the name of the dashboard that will be created.
            - **filename**: name of a file containing a JSON object
            - **filter**: a boolean expression combining Sysdig Monitor segmentation criteria defines what the new dasboard will be applied to. For example: *kubernetes.namespace.name='production' and container.image='nginx'*.
            - **shared**: if set to True, the new dashboard will be a shared one.
            - **public**: if set to True, the new dashboard will be shared with public token.

        **Success Return Value**
            A dictionary showing the details of the new dashboard.

        **Example**
            `examples/dashboard_save_load.py <https://github.com/draios/python-sdc-client/blob/master/examples/dashboard_save_load.py>`_
        """"""
    with open(filename) as data_file:
        loaded_object = json.load(data_file)
    if '<MASK>':
        loaded_object = {'version': 'v1', 'dashboard': loaded_object}
    dashboard = loaded_object['dashboard']
    if '<MASK>':
        conversion_result, dashboard = self._convert_dashboard_to_current_version(dashboard, loaded_object['version'])
        if '<MASK>':
            return (conversion_result, dashboard)
    return self.create_dashboard_from_template(dashboard_name, dashboard, filter, shared, public)",False,"[""'dashboard' not in loaded_object"", ""loaded_object['version'] != self._dashboards_api_version"", 'conversion_result == False']",___________________,0.0
"def write(self, output_buffer, kmip_version=enums.KMIPVersion.KMIP_1_0):
    """"""
        Write the data encoding the Create request payload to a buffer.

        Args:
            output_buffer (stream): A data buffer in which to encode object
                data, supporting a write method.
            kmip_version (KMIPVersion): An enumeration defining the KMIP
                version with which the object will be encoded. Optional,
                defaults to KMIP 1.0.

        Raises:
            InvalidField: Raised if the object type attribute or template
                attribute is not defined.
        """"""
    local_buffer = utils.BytearrayStream()
    if '<MASK>':
        self._object_type.write(local_buffer, kmip_version=kmip_version)
    else:
        raise exceptions.InvalidField('The Create request payload is missing the object type field.')
    if '<MASK>':
        if '<MASK>':
            self._template_attribute.write(local_buffer, kmip_version=kmip_version)
        else:
            raise exceptions.InvalidField('The Create request payload is missing the template attribute field.')
    elif '<MASK>':
        attributes = objects.convert_template_attribute_to_attributes(self._template_attribute)
        attributes.write(local_buffer, kmip_version=kmip_version)
    else:
        raise exceptions.InvalidField('The Create request payload is missing the template attribute field.')
    self.length = local_buffer.length()
    super(CreateRequestPayload, self).write(output_buffer, kmip_version=kmip_version)
    output_buffer.write(local_buffer.buffer)",False,"['self._object_type', 'kmip_version < enums.KMIPVersion.KMIP_2_0', 'self._template_attribute', 'self._template_attribute']",___________________,0.0
"def read(self, input_buffer, kmip_version=enums.KMIPVersion.KMIP_1_0):
    """"""
        Read the data encoding the CreateKeyPair response payload and decode it
        into its constituent parts.

        Args:
            input_buffer (stream): A data buffer containing encoded object
                data, supporting a read method.
            kmip_version (KMIPVersion): An enumeration defining the KMIP
                version with which the object will be decoded. Optional,
                defaults to KMIP 1.0.

        Raises:
            InvalidKmipEncoding: Raised if the private key unique identifier or
                the public key unique identifier is missing from the encoded
                payload.
        """"""
    super(CreateKeyPairResponsePayload, self).read(input_buffer, kmip_version=kmip_version)
    local_buffer = utils.BytearrayStream(input_buffer.read(self.length))
    if '<MASK>':
        self._private_key_unique_identifier = primitives.TextString(tag=enums.Tags.PRIVATE_KEY_UNIQUE_IDENTIFIER)
        self._private_key_unique_identifier.read(local_buffer, kmip_version=kmip_version)
    else:
        raise exceptions.InvalidKmipEncoding('The CreateKeyPair response payload encoding is missing the private key unique identifier.')
    if '<MASK>':
        self._public_key_unique_identifier = primitives.TextString(tag=enums.Tags.PUBLIC_KEY_UNIQUE_IDENTIFIER)
        self._public_key_unique_identifier.read(local_buffer, kmip_version=kmip_version)
    else:
        raise exceptions.InvalidKmipEncoding('The CreateKeyPair response payload encoding is missing the public key unique identifier.')
    if '<MASK>':
        if '<MASK>':
            self._private_key_template_attribute = objects.TemplateAttribute(tag=enums.Tags.PRIVATE_KEY_TEMPLATE_ATTRIBUTE)
            self._private_key_template_attribute.read(local_buffer, kmip_version=kmip_version)
        if '<MASK>':
            self._public_key_template_attribute = objects.TemplateAttribute(tag=enums.Tags.PUBLIC_KEY_TEMPLATE_ATTRIBUTE)
            self._public_key_template_attribute.read(local_buffer, kmip_version=kmip_version)
    self.is_oversized(local_buffer)",False,"['self.is_tag_next(enums.Tags.PRIVATE_KEY_UNIQUE_IDENTIFIER, local_buffer)', 'self.is_tag_next(enums.Tags.PUBLIC_KEY_UNIQUE_IDENTIFIER, local_buffer)', 'kmip_version < enums.KMIPVersion.KMIP_2_0', 'self.is_tag_next(enums.Tags.PRIVATE_KEY_TEMPLATE_ATTRIBUTE, local_buffer)', 'self.is_tag_next(enums.Tags.PUBLIC_KEY_TEMPLATE_ATTRIBUTE, local_buffer)']",___________________,0.0
"def find(self, path, all=False):
    """"""
        Only allow lookups for jspm_packages.

        # TODO: figure out the 'jspm_packages' dir from packag.json.
        """"""
    bits = path.split('/')
    dirs_to_serve = ['jspm_packages', settings.SYSTEMJS_OUTPUT_DIR]
    if '<MASK>':
        return []
    return super(SystemFinder, self).find(path, all=all)",False,['not bits or bits[0] not in dirs_to_serve'],___________________,0.0
"def _is_node_return_ended(self, node):
    """"""Check if the node ends with an explicit return statement.

        Args:
            node (astroid.NodeNG): node to be checked.

        Returns:
            bool: True if the node ends with an explicit statement, False otherwise.

        """"""
    if '<MASK>':
        return True
    if '<MASK>':
        try:
            funcdef_node = node.func.inferred()[0]
            if '<MASK>':
                return True
        except astroid.InferenceError:
            pass
    if '<MASK>':
        return True
    if '<MASK>':
        if '<MASK>':
            return True
        if '<MASK>':
            return True
        exc = utils.safe_infer(node.exc)
        if '<MASK>':
            return False
        exc_name = exc.pytype().split('.')[-1]
        handlers = utils.get_exception_handlers(node, exc_name)
        handlers = list(handlers) if handlers is not None else []
        if '<MASK>':
            return any((self._is_node_return_ended(_handler) for _handler in handlers))
        return True
    if '<MASK>':
        is_orelse_returning = any((self._is_node_return_ended(_ore) for _ore in node.orelse if not isinstance(_ore, astroid.FunctionDef)))
        is_if_returning = any((self._is_node_return_ended(_ifn) for _ifn in node.body if not isinstance(_ifn, astroid.FunctionDef)))
        return is_if_returning and is_orelse_returning
    return any((self._is_node_return_ended(_child) for _child in node.get_children() if not isinstance(_child, astroid.ExceptHandler)))",False,"['isinstance(node, astroid.Return)', 'isinstance(node, astroid.Call)', 'isinstance(node, astroid.While)', 'isinstance(node, astroid.Raise)', 'isinstance(node, astroid.If)', 'not node.exc', 'not utils.is_node_inside_try_except(node)', 'exc is None or exc is astroid.Uninferable', 'handlers', 'self._is_function_def_never_returning(funcdef_node)']",___________________,0.0
"def _parse_report_record(record, nameservers=None, dns_timeout=2.0, parallel=False):
    """"""
    Converts a record from a DMARC aggregate report into a more consistent
    format

    Args:
        record (OrderedDict): The record to convert
        nameservers (list): A list of one or more nameservers to use
        (Cloudflare's public DNS resolvers by default)
        dns_timeout (float): Sets the DNS timeout in seconds

    Returns:
        OrderedDict: The converted record
    """"""
    if '<MASK>':
        nameservers = ['1.1.1.1', '1.0.0.1', '2606:4700:4700::1111', '2606:4700:4700::1001']
    record = record.copy()
    new_record = OrderedDict()
    new_record_source = get_ip_address_info(record['row']['source_ip'], cache=IP_ADDRESS_CACHE, nameservers=nameservers, timeout=dns_timeout, parallel=parallel)
    new_record['source'] = new_record_source
    new_record['count'] = int(record['row']['count'])
    policy_evaluated = record['row']['policy_evaluated'].copy()
    new_policy_evaluated = OrderedDict([('disposition', 'none'), ('dkim', 'fail'), ('spf', 'fail'), ('policy_override_reasons', [])])
    if '<MASK>':
        new_policy_evaluated['disposition'] = policy_evaluated['disposition']
        if '<MASK>':
            new_policy_evaluated['disposition'] = 'none'
    if '<MASK>':
        new_policy_evaluated['dkim'] = policy_evaluated['dkim']
    if '<MASK>':
        new_policy_evaluated['spf'] = policy_evaluated['spf']
    reasons = []
    spf_aligned = policy_evaluated['spf'] == 'pass'
    dkim_aligned = policy_evaluated['dkim'] == 'pass'
    dmarc_aligned = spf_aligned or dkim_aligned
    new_record['alignment'] = dict()
    new_record['alignment']['spf'] = spf_aligned
    new_record['alignment']['dkim'] = dkim_aligned
    new_record['alignment']['dmarc'] = dmarc_aligned
    if '<MASK>':
        if '<MASK>':
            reasons = policy_evaluated['reason']
        else:
            reasons = [policy_evaluated['reason']]
    for reason in reasons:
        if '<MASK>':
            reason['comment'] = None
    new_policy_evaluated['policy_override_reasons'] = reasons
    new_record['policy_evaluated'] = new_policy_evaluated
    new_record['identifiers'] = record['identifiers'].copy()
    new_record['auth_results'] = OrderedDict([('dkim', []), ('spf', [])])
    if '<MASK>':
        auth_results = record['auth_results'].copy()
        if '<MASK>':
            auth_results['spf'] = []
        if '<MASK>':
            auth_results['dkim'] = []
    else:
        auth_results = new_record['auth_results'].copy()
    if '<MASK>':
        auth_results['dkim'] = [auth_results['dkim']]
    for result in auth_results['dkim']:
        if '<MASK>':
            new_result = OrderedDict([('domain', result['domain'])])
            if '<MASK>':
                new_result['selector'] = result['selector']
            else:
                new_result['selector'] = 'none'
            if '<MASK>':
                new_result['result'] = result['result']
            else:
                new_result['result'] = 'none'
            new_record['auth_results']['dkim'].append(new_result)
    if '<MASK>':
        auth_results['spf'] = [auth_results['spf']]
    for result in auth_results['spf']:
        new_result = OrderedDict([('domain', result['domain'])])
        if '<MASK>':
            new_result['scope'] = result['scope']
        else:
            new_result['scope'] = 'mfrom'
        if '<MASK>':
            new_result['result'] = result['result']
        else:
            new_result['result'] = 'none'
        new_record['auth_results']['spf'].append(new_result)
    if '<MASK>':
        envelope_from = None
        if '<MASK>':
            envelope_from = new_record['auth_results']['spf'][-1]['domain']
        if '<MASK>':
            envelope_from = str(envelope_from).lower()
        new_record['identifiers']['envelope_from'] = envelope_from
    elif '<MASK>':
        if '<MASK>':
            envelope_from = new_record['auth_results']['spf'][-1]['domain']
            if '<MASK>':
                envelope_from = str(envelope_from).lower()
            new_record['identifiers']['envelope_from'] = envelope_from
    envelope_to = None
    if '<MASK>':
        envelope_to = new_record['identifiers']['envelope_to']
        del new_record['identifiers']['envelope_to']
    new_record['identifiers']['envelope_to'] = envelope_to
    return new_record",False,"['nameservers is None', ""'disposition' in policy_evaluated"", ""'dkim' in policy_evaluated"", ""'spf' in policy_evaluated"", ""'reason' in policy_evaluated"", ""record['auth_results'] is not None"", ""type(auth_results['dkim']) != list"", ""type(auth_results['spf']) != list"", ""'envelope_from' not in new_record['identifiers']"", ""'envelope_to' in new_record['identifiers']"", ""new_policy_evaluated['disposition'].strip().lower() == 'pass'"", ""type(policy_evaluated['reason']) == list"", ""'comment' not in reason"", ""'spf' not in auth_results"", ""'dkim' not in auth_results"", ""'domain' in result and result['domain'] is not None"", ""'scope' in result and result['scope'] is not None"", ""'result' in result and result['result'] is not None"", ""len(auth_results['spf']) > 0"", 'envelope_from is not None', ""new_record['identifiers']['envelope_from'] is None"", ""'selector' in result and result['selector'] is not None"", ""'result' in result and result['result'] is not None"", ""len(auth_results['spf']) > 0"", 'envelope_from is not None']",___________________,0.0
"def check_list(var, num_terms):
    """""" Check if a variable is a list and is the correct length.

    If variable is not a list it will make it a list of the correct length with
    all terms identical.
    """"""
    if '<MASK>':
        if '<MASK>':
            var = list(var)
        else:
            var = [var]
        for _ in range(1, num_terms):
            var.append(var[0])
    if '<MASK>':
        print('""%s"" has the wrong number of terms; it needs %s. Exiting ...' % (var, num_terms))
        sys.exit(1)
    return var",False,"['not isinstance(var, list)', 'len(var) != num_terms', 'isinstance(var, tuple)']","____terms):
    """""" """"""
    """"""
    """"""
   ",0.0
"def cycle(self):
    """"""
        Request one batch of events from Skype, calling :meth:`onEvent` with each event in turn.

        Subclasses may override this method to alter loop functionality.
        """"""
    try:
        events = self.getEvents()
    except requests.ConnectionError:
        return
    for event in events:
        self.onEvent(event)
        if '<MASK>':
            event.ack()",False,['self.autoAck'],___________________,0.0
"def p_closed_proposition_list(self, p):
    """""" closed_proposition_list :  closed_proposition_list SLASH SLASH closed_proposition
                                    | closed_proposition""""""
    if '<MASK>':
        p[0] = [p[1]]
    else:
        p[0] = p[1] + [p[4]]",False,['len(p) == 2'],_((((((((((((((((((,0.0
"def decorator(f):

    @wraps(f)
    def decorated_function(*args, **kwargs):
        template_name = template
        if '<MASK>':
            template_name = request.endpoint.replace('.', '/') + '.html'
        context = f(*args, **kwargs)
        if '<MASK>':
            context = {}
        elif '<MASK>':
            return context
        return render_template(template_name, **context)
    return decorated_function",False,"['template_name is None', 'context is None', 'not isinstance(context, dict)']","__name,_name, **,,,,,,,,,,,",0.0
"def _values_of_same_type(self, val1, val2):
    """"""Checks if two values agree in type.

        The sparse parameter is less restrictive than the parameter. If both values
        are sparse matrices they are considered to be of same type
        regardless of their size and values they contain.

        """"""
    if '<MASK>':
        return True
    else:
        return super(SparseParameter, self)._values_of_same_type(val1, val2)",False,['self._is_supported_matrix(val1) and self._is_supported_matrix(val2)'],___________________,0.0
"def auth_user_get_url(self, scope=None):
    """"""Build authorization URL for User Agent.""""""
    if '<MASK>':
        raise AuthMissingError('No client_id specified')
    return '{}?{}'.format(self.auth_url_user, urllib.urlencode(dict(client_id=self.client_id, scope=' '.join(scope or self.auth_scope), response_type='code', redirect_uri=self.auth_redirect_uri)))",False,['not self.client_id'],___________________,0.0
"def main():
    """"""Main function of the module.""""""
    parser = argparse.ArgumentParser(prog=_PROGRAN_NAME, description=_MODULE_DESC, formatter_class=argparse.RawTextHelpFormatter)
    launch_modes = parser.add_mutually_exclusive_group(required=True)
    launch_modes.add_argument('-r', '--remote', dest='remote', action='store_true', default=False, help='launch in remote mode')
    launch_modes.add_argument('-i', '--input-file', dest='input_file', type=str, default='', help='render UI from file')
    launch_modes.add_argument('-c', '--config', nargs=2, dest='config', help=_CONFIG_DESC, metavar=('CONFIG', 'SRC'))
    parser.add_argument('-H', '--host', dest='host', default=_HOST, type=str, help='set internal webserver host')
    parser.add_argument('-p', '--port', dest='port', default=_PORT, type=int, help='set internal webserver port')
    parser.add_argument('-n', '--no-browser', dest='dont_start_browser', action='store_true', default=False, help=""don't start browser automatically"")
    parser.add_argument('-o', '--output-file', dest='output_file', type=str, default='', help='save profile to file')
    parser.add_argument('--debug', dest='debug_mode', action='store_true', default=False, help=""don't suppress error messages"")
    parser.add_argument('--version', action='version', version='vprof %s' % __version__)
    args = parser.parse_args()
    if '<MASK>':
        with open(args.input_file) as ifile:
            saved_stats = json.loads(ifile.read())
            if '<MASK>':
                print('Incorrect profiler version - %s. %s is required.' % (saved_stats['version'], __version__))
                sys.exit(_ERR_CODES['input_file_error'])
            stats_server.start(args.host, args.port, saved_stats, args.dont_start_browser, args.debug_mode)
    elif '<MASK>':
        stats_server.start(args.host, args.port, {}, args.dont_start_browser, args.debug_mode)
    else:
        config, source = args.config
        try:
            program_stats = runner.run_profilers(source, config, verbose=True)
        except runner.AmbiguousConfigurationError:
            print('Profiler configuration %s is ambiguous. Please, remove duplicates.' % config)
            sys.exit(_ERR_CODES['ambiguous_configuration'])
        except runner.BadOptionError as exc:
            print(exc)
            sys.exit(_ERR_CODES['bad_option'])
        if '<MASK>':
            with open(args.output_file, 'w') as outfile:
                program_stats['version'] = __version__
                outfile.write(json.dumps(program_stats, indent=2))
        else:
            stats_server.start(args.host, args.port, program_stats, args.dont_start_browser, args.debug_mode)",False,"['args.input_file', 'args.remote', ""saved_stats['version'] != __version__"", 'args.output_file']",___________________,0.0
"def handle(self, *args, **options):
    """""" command execution """"""
    assume_yes = options.get('assume_yes', False)
    default_language = options.get('default_language', None)
    transaction.commit_unless_managed()
    transaction.enter_transaction_management()
    transaction.managed(True)
    self.cursor = connection.cursor()
    self.introspection = connection.introspection
    self.default_lang = default_language or mandatory_language()
    all_models = get_models()
    found_db_change_fields = False
    for model in all_models:
        if '<MASK>':
            model_full_name = '%s.%s' % (model._meta.app_label, model._meta.module_name)
            translatable_fields = get_all_translatable_fields(model, column_in_current_table=True)
            db_table = model._meta.db_table
            for field_name in translatable_fields:
                db_table_fields = self.get_table_fields(db_table)
                db_change_langs = list(set(list(self.get_db_change_languages(field_name, db_table_fields)) + [self.default_lang]))
                if '<MASK>':
                    sql_sentences = self.get_sync_sql(field_name, db_change_langs, model, db_table_fields)
                    if '<MASK>':
                        found_db_change_fields = True
                        print_db_change_langs(db_change_langs, field_name, model_full_name)
                        execute_sql = ask_for_confirmation(sql_sentences, model_full_name, assume_yes)
                        if '<MASK>':
                            print('Executing SQL...')
                            for sentence in sql_sentences:
                                self.cursor.execute(sentence)
                                transaction.commit()
                            print('Done')
                        else:
                            print('SQL not executed')
    if '<MASK>':
        transaction.commit()
    transaction.leave_transaction_management()
    if '<MASK>':
        print('\nNo new translatable fields detected')
    if '<MASK>':
        variable = 'TRANSMETA_DEFAULT_LANGUAGE'
        has_transmeta_default_language = getattr(settings, variable, False)
        if '<MASK>':
            variable = 'LANGUAGE_CODE'
        if '<MASK>':
            print('\n\nYou should change in your settings the %s variable to ""%s""' % (variable, default_language))",False,"['transaction.is_dirty()', 'not found_db_change_fields', 'default_language', ""hasattr(model._meta, 'translatable_fields')"", 'not has_transmeta_default_language', 'getattr(settings, variable) != default_language', 'db_change_langs', 'sql_sentences', 'execute_sql']",___________________,0.0
"def segment(self, text):
    """""" Call the segmenter in order to split text in sentences.

        Args:
            text (str): Text to be segmented.

        Returns:
            dict, int: A dict containing a list of dicts with the offsets of
                each sentence; an integer representing the response code.
        """"""
    files = {'text': text}
    res, status_code = self.post(self.segmentation_service, files=files)
    if '<MASK>':
        logger.debug('Segmentation failed.')
    return (self.decode(res), status_code)",False,['status_code != 200'],___________________,0.0
"def compressed_file_type(content):
    magic_dict = {b'\x1f\x8b\x08': 'gz', b'BZh': 'bz2', b'PK\x03\x04': 'zip'}
    for magic, filetype in magic_dict.items():
        if '<MASK>':
            return filetype
    return None",False,['content.startswith(magic)'],(((((((((((((((((((,0.0
"def eligible_node(self, id, eligible=None, ineligible=None):
    """""" Toggle the eligibility of the node.

           https://www.nomadproject.io/docs/http/node.html

            arguments:
              - id (str uuid): node id
              - eligible (bool): Set to True to mark node eligible
              - ineligible (bool): Set to True to mark node ineligible
            returns: dict
            raises:
              - nomad.api.exceptions.BaseNomadException
              - nomad.api.exceptions.URLNotFoundNomadException
        """"""
    payload = {}
    if '<MASK>':
        raise nomad.api.exceptions.InvalidParameters
    if '<MASK>':
        raise nomad.api.exceptions.InvalidParameters
    if '<MASK>':
        payload = {'Eligibility': 'eligible', 'NodeID': id}
    elif '<MASK>':
        payload = {'Eligibility': 'ineligible', 'NodeID': id}
    elif '<MASK>':
        payload = {'Eligibility': 'ineligible', 'NodeID': id}
    elif '<MASK>':
        payload = {'Eligibility': 'eligible', 'NodeID': id}
    return self.request(id, 'eligibility', json=payload, method='post').json()",False,"['eligible is not None and ineligible is not None', 'eligible is None and ineligible is None', 'eligible is not None and eligible', 'eligible is not None and (not eligible)', 'ineligible is not None', 'ineligible is not None and (not ineligible)']","_,,,,,,,,,,,,,,,,,,",0.0
"def store(self, *args, **kwargs):
    """"""Acquires a lock before storage and releases it afterwards.""""""
    try:
        self.acquire_lock()
        return self._storage_service.store(*args, **kwargs)
    finally:
        if '<MASK>':
            try:
                self.release_lock()
            except RuntimeError:
                self._logger.error('Could not release lock `%s`!' % str(self.lock))",False,['self.lock is not None'],_service_service_service_service_service_service_service_service_service_,0.0
"def download(self, source, target, mpi=None, pos=0, chunk=0, part=0):
    """"""Thread worker for download operation.""""""
    s3url = S3URL(source)
    obj = self.lookup(s3url)
    if '<MASK>':
        raise Failure('The obj ""%s"" does not exists.' % (s3url.path,))
    if '<MASK>':
        if '<MASK>':
            message('%s => %s', source, target)
            return
        elif '<MASK>':
            message('%s => %s (synced)', source, target)
            return
        elif '<MASK>':
            raise Failure('File already exists: %s' % target)
        fsize = int(obj['ContentLength'])
        if '<MASK>':
            mpi = ThreadUtil.MultipartItem(tempfile_get(target))
            mpi.total = 1
            pos = 0
            chunk = fsize
        else:
            for args in self.get_file_splits(tempfile_get(target), source, target, fsize, self.opt.multipart_split_size):
                self.pool.download(*args)
            return
    tempfile = mpi.id
    if '<MASK>':
        self.mkdirs(tempfile)
    response = self.s3.get_object(Bucket=s3url.bucket, Key=s3url.path, Range='bytes=%d-%d' % (pos, pos + chunk - 1))
    self.write_file_chunk(tempfile, pos, chunk, response['Body'])
    if '<MASK>':
        try:
            self.update_privilege(obj, tempfile)
            self._verify_file_size(obj, tempfile)
            tempfile_set(tempfile, target)
            message('%s => %s', source, target)
        except Exception as e:
            tempfile_set(tempfile, None)
            raise Failure('Download Failure: %s, Source: %s.' % (e.message, source))",False,"['obj is None', 'not mpi', 'self.opt.recursive', ""mpi.complete({'PartNumber': part})"", 'self.opt.dry_run', 'fsize < self.opt.max_singlepart_download_size', 'self.opt.sync_check and self.sync_check(LocalMD5Cache(target), obj)', 'not self.opt.force and os.path.exists(target)']",___________________,0.0
"def start(self):
    """"""Starts redirection of `stdout`""""""
    if '<MASK>':
        self._original_steam = sys.stdout
        sys.stdout = self
        self._redirection = True
    if '<MASK>':
        print('Established redirection of `stdout`.')",False,"['sys.stdout is not self', 'self._redirection']",___________________,0.0
"def delete_file_if_needed(instance, filefield_name):
    """"""Delete file from database only if needed.

    When editing and the filefield is a new file,
    deletes the previous file (if any) from the database.
    Call this function immediately BEFORE saving the instance.
    """"""
    if '<MASK>':
        model_class = type(instance)
        if '<MASK>':
            old_file = getattr(model_class.objects.only(filefield_name).get(pk=instance.pk), filefield_name)
        else:
            old_file = None
        if '<MASK>':
            if '<MASK>':
                DatabaseFileStorage().delete(old_file.name)",False,"['instance.pk', ""model_class.objects.filter(pk=instance.pk).exclude(**{'%s__isnull' % filefield_name: True}).exclude(**{'%s__exact' % filefield_name: ''}).exists()"", 'old_file', '(old_file.name == getattr(instance, filefield_name)) is False']","_):
    """"""..............",0.0
"def despike(self, expdecay_despiker=False, exponent=None, noise_despiker=True, win=3, nlim=12.0, exponentplot=False, maxiter=4, autorange_kwargs={}, focus_stage='rawdata'):
    """"""
        Despikes data with exponential decay and noise filters.

        Parameters
        ----------
        expdecay_despiker : bool
            Whether or not to apply the exponential decay filter.
        exponent : None or float
            The exponent for the exponential decay filter. If None,
            it is determined automatically using `find_expocoef`.
        tstep : None or float
            The timeinterval between measurements. If None, it is
            determined automatically from the Time variable.
        noise_despiker : bool
            Whether or not to apply the standard deviation spike filter.
        win : int
            The rolling window over which the spike filter calculates
            the trace statistics.
        nlim : float
            The number of standard deviations above the rolling mean
            that data are excluded.
        exponentplot : bool
            Whether or not to show a plot of the automatically determined
            exponential decay exponent.
        maxiter : int
            The max number of times that the fitler is applied.
        focus_stage : str
            Which stage of analysis to apply processing to. 
            Defaults to 'rawdata'. Can be one of:
            * 'rawdata': raw data, loaded from csv file.
            * 'despiked': despiked data.
            * 'signal'/'background': isolated signal and background data.
              Created by self.separate, after signal and background
              regions have been identified by self.autorange.
            * 'bkgsub': background subtracted data, created by 
              self.bkg_correct
            * 'ratios': element ratio data, created by self.ratio.
            * 'calibrated': ratio data calibrated to standards, created by self.calibrate.

        Returns
        -------
        None
        """"""
    if '<MASK>':
        self.set_focus(focus_stage)
    if '<MASK>':
        if '<MASK>':
            self.find_expcoef(plot=exponentplot, autorange_kwargs=autorange_kwargs)
        exponent = self.expdecay_coef
        time.sleep(0.1)
    with self.pbar.set(total=len(self.data), desc='Despiking') as prog:
        for d in self.data.values():
            d.despike(expdecay_despiker, exponent, noise_despiker, win, nlim, maxiter)
            prog.update()
    self.stages_complete.update(['despiked'])
    self.focus_stage = 'despiked'
    return",False,"['focus_stage != self.focus_stage', 'expdecay_despiker and exponent is None', ""not hasattr(self, 'expdecay_coef')""]",___________________,0.0
"def client_for_path(self, path):
    """"""
        Returns a new client with the same root URL and authentication, but
        a different specific URL.  For instance, if you have a client pointed
        at https://analytics.luminoso.com/api/v5/, and you want new ones for
        Project A and Project B, you would call:

            client_a = client.client_for_path('projects/<project_id_a>')
            client_b = client.client_for_path('projects/<project_id_b>')

        and your base client would remian unchanged.

        Paths with leading slashes are appended to the root url; otherwise,
        paths are set relative to the current path.
        """"""
    if '<MASK>':
        url = self.root_url + path
    else:
        url = self.url + path
    return self.__class__(self.session, url)",False,"[""path.startswith('/')""]",___________________,0.0
"def color_values(color):
    """"""Read color_names.txt and find the red, green, and blue values
        for a named color.
    """"""
    this_dir = os.path.dirname(os.path.realpath(inspect.getsourcefile(lambda: 0)))
    color_name_file = os.path.join(this_dir, 'color_names.txt')
    found = False
    for line in open(color_name_file, 'r'):
        line = line.rstrip()
        if '<MASK>':
            red = line.split()[2]
            green = line.split()[3]
            blue = line.split()[4]
            found = True
            break
    if '<MASK>':
        print('Color name ""%s"" not found, using default (white)' % color)
        red = 255
        green = 255
        blue = 255
    return (red, green, blue)",False,"['not found', 'color.lower() == line.split()[0]']",___________________,0.0
"def getstats(self, save=True, filename=None, samples=None, subset=None, ablation_time=False):
    """"""
        Return pandas dataframe of all sample statistics.
        """"""
    slst = []
    if '<MASK>':
        subset = self.make_subset(samples)
    samples = self._get_samples(subset)
    for s in self.stats_calced:
        for nm in [n for n in samples if self.srm_identifier not in n]:
            if '<MASK>':
                reps = np.arange(self.stats[nm][s].shape[-1])
                ss = np.array([s] * reps.size)
                nms = np.array([nm] * reps.size)
                stdf = pd.DataFrame(self.stats[nm][s].T, columns=self.stats[nm]['analytes'], index=[ss, nms, reps])
                stdf.index.set_names(['statistic', 'sample', 'rep'], inplace=True)
            else:
                stdf = pd.DataFrame(self.stats[nm][s], index=self.stats[nm]['analytes'], columns=[[s], [nm]]).T
                stdf.index.set_names(['statistic', 'sample'], inplace=True)
            slst.append(stdf)
    out = pd.concat(slst)
    if '<MASK>':
        ats = self.ablation_times(samples=samples, subset=subset)
        ats['statistic'] = 'nanmean'
        ats.set_index('statistic', append=True, inplace=True)
        ats = ats.reorder_levels(['statistic', 'sample', 'rep'])
        out = out.join(ats)
    out.drop(self.internal_standard, 1, inplace=True)
    if '<MASK>':
        if '<MASK>':
            filename = 'stat_export.csv'
        out.to_csv(self.export_dir + '/' + filename)
    self.stats_df = out
    return out",False,"['samples is not None', 'ablation_time', 'save', 'filename is None', 'self.stats[nm][s].ndim == 2']",___________________,0.0
"def get_image(self, image, show_history=False):
    """"""**Description**
            Find the image with the tag <image> and return its json description

        **Arguments**
            - image: Input image can be in the following formats: registry/repo:tag

        **Success Return Value**
            A JSON object representing the image.
        """"""
    itype = self._discover_inputimage_format(image)
    if '<MASK>':
        return [False, 'cannot use input image string: no discovered imageDigest']
    params = {}
    params['history'] = str(show_history and itype not in ['imageid', 'imageDigest']).lower()
    if '<MASK>':
        params['fulltag'] = image
    url = self.url + '/api/scanning/v1/anchore/images'
    url += {'imageid': '/by_id/{}'.format(image), 'imageDigest': '/{}'.format(image)}.get(itype, '')
    res = requests.get(url, params=params, headers=self.hdrs, verify=self.ssl_verify)
    if '<MASK>':
        return [False, self.lasterr]
    return [True, res.json()]",False,"[""itype not in ['tag', 'imageid', 'imageDigest']"", ""itype == 'tag'"", 'not self._checkResponse(res)']",___________________,0.0
"def to_python(self):
    """"""Deconstruct the ``Expression`` instance to a list or tuple
        (If ``Expression`` contains only one ``Constraint``).

        Returns:
            list or tuple: The deconstructed ``Expression``.
        """"""
    if '<MASK>':
        return None
    if '<MASK>':
        return self.elements[0].to_python()
    operator = self.operator or Operator(';')
    return [operator.to_python()] + [elem.to_python() for elem in self.elements]",False,"['len(self.elements) == 0', 'len(self.elements) == 1']","_python,,,,,,,,,,,,,,,,,",0.0
"def server(self):
    """"""
        All in one endpoints. This property is created automaticly
        if you have implemented all the getters and setters.

        However, if you are not satisfied with the getter and setter,
        you can create a validator with :class:`OAuth2RequestValidator`::

            class MyValidator(OAuth2RequestValidator):
                def validate_client_id(self, client_id):
                    # do something
                    return True

        And assign the validator for the provider::

            oauth._validator = MyValidator()
        """"""
    expires_in = self.app.config.get('OAUTH2_PROVIDER_TOKEN_EXPIRES_IN')
    token_generator = self.app.config.get('OAUTH2_PROVIDER_TOKEN_GENERATOR', None)
    if '<MASK>':
        token_generator = import_string(token_generator)
    refresh_token_generator = self.app.config.get('OAUTH2_PROVIDER_REFRESH_TOKEN_GENERATOR', None)
    if '<MASK>':
        refresh_token_generator = import_string(refresh_token_generator)
    if '<MASK>':
        return Server(self._validator, token_expires_in=expires_in, token_generator=token_generator, refresh_token_generator=refresh_token_generator)
    if '<MASK>':
        usergetter = None
        if '<MASK>':
            usergetter = self._usergetter
        validator_class = self._validator_class
        if '<MASK>':
            validator_class = OAuth2RequestValidator
        validator = validator_class(clientgetter=self._clientgetter, tokengetter=self._tokengetter, grantgetter=self._grantgetter, usergetter=usergetter, tokensetter=self._tokensetter, grantsetter=self._grantsetter)
        self._validator = validator
        return Server(validator, token_expires_in=expires_in, token_generator=token_generator, refresh_token_generator=refresh_token_generator)
    raise RuntimeError('application not bound to required getters')",False,"['token_generator and (not callable(token_generator))', 'refresh_token_generator and (not callable(refresh_token_generator))', ""hasattr(self, '_validator')"", ""hasattr(self, '_clientgetter') and hasattr(self, '_tokengetter') and hasattr(self, '_tokensetter') and hasattr(self, '_grantgetter') and hasattr(self, '_grantsetter')"", ""hasattr(self, '_usergetter')"", 'validator_class is None']",___________________,0.0
"def get_report_data(self, report):
    """"""
        Returns a completed report as a list of csv strings.
        """"""
    if '<MASK>':
        raise ReportFailureException(report)
    interval = getattr(settings, 'CANVAS_REPORT_POLLING_INTERVAL', 5)
    while report.status != 'complete':
        if '<MASK>':
            raise ReportFailureException(report)
        sleep(interval)
        report = self.get_report_status(report)
    if '<MASK>':
        return
    data = self._get_report_file(report.attachment.url)
    return data.split('\n')",False,"['report.report_id is None or report.status is None', 'report.attachment is None or report.attachment.url is None', ""report.status == 'error'""]",(((((((((((((((((((,0.0
"def pformat_dict_summary_html(dict):
    """"""
    Briefly print the dictionary keys.
    """"""
    if '<MASK>':
        return '   {}'
    html = []
    for key, value in sorted(six.iteritems(dict)):
        if '<MASK>':
            value = '...'
        html.append(_format_dict_item(key, value))
    return mark_safe(u'<br/>'.join(html))",False,"['not dict', 'not isinstance(value, DICT_EXPANDED_TYPES)']",(((((((((((((((((((,0.0
"def switch_to_window(self, window, wait=None):
    """"""
        If ``window`` is a lambda, it switches to the first window for which ``window`` returns a
        value other than False or None. If a window that matches can't be found, the window will be
        switched back and :exc:`WindowError` will be raised.

        Args:
            window (Window | lambda): The window that should be switched to, or a filtering lambda.
            wait (int | float, optional): The number of seconds to wait to find the window.

        Returns:
            Window: The new current window.

        Raises:
            ScopeError: If this method is invoked inside :meth:`scope, :meth:`frame`, or
                :meth:`window`.
            WindowError: If no window matches the given lambda.
        """"""
    if '<MASK>':
        raise ScopeError('`switch_to_window` is not supposed to be invoked from within `scope`s, `frame`s, or other `window`s.')
    if '<MASK>':
        self.driver.switch_to_window(window.handle)
        return window
    else:

        @self.document.synchronize(errors=(WindowError,), wait=wait)
        def switch_and_get_matching_window():
            original_window_handle = self.driver.current_window_handle
            try:
                for handle in self.driver.window_handles:
                    self.driver.switch_to_window(handle)
                    result = window()
                    if '<MASK>':
                        return Window(self, handle)
            except Exception:
                self.driver.switch_to_window(original_window_handle)
                raise
            self.driver.switch_to_window(original_window_handle)
            raise WindowError('Could not find a window matching lambda')
        return switch_and_get_matching_window()",False,"['len(self._scopes) > 1', 'isinstance(window, Window)', 'result']",___________________,0.0
"def _delete_old_scoop_rev_data(old_scoop_rev):
    if '<MASK>':
        try:
            elements = shared.elements
            for key in elements:
                var_dict = elements[key]
                if '<MASK>':
                    del var_dict[old_scoop_rev]
            logging.getLogger('pypet.scoop').debug('Deleted old SCOOP data from revolution `%s`.' % old_scoop_rev)
        except AttributeError:
            logging.getLogger('pypet.scoop').error('Could not delete old SCOOP data from revolution `%s`.' % old_scoop_rev)",False,"['old_scoop_rev is not None', 'old_scoop_rev in var_dict']",_s_s_s_s_s_s_s_s_s_,0.0
"def repair(self, data_to_repair):
    num_cols = len(data_to_repair[0])
    col_ids = range(num_cols)
    col_types = ['Y'] * len(col_ids)
    for i, col in enumerate(col_ids):
        if '<MASK>':
            col_types[i] = 'I'
        elif '<MASK>':
            col_types[i] = 'X'
    col_type_dict = {col_id: col_type for col_id, col_type in zip(col_ids, col_types)}
    not_I_col_ids = filter(lambda x: col_type_dict[x] != 'I', col_ids)
    if '<MASK>':
        cols_to_repair = filter(lambda x: col_type_dict[x] == 'Y', col_ids)
    else:
        cols_to_repair = filter(lambda x: col_type_dict[x] in 'YX', col_ids)
    safe_stratify_cols = [self.feature_to_repair]
    data_dict = {col_id: [] for col_id in col_ids}
    for row in data_to_repair:
        for i in col_ids:
            data_dict[i].append(row[i])
    repair_types = {}
    for col_id, values in data_dict.items():
        if '<MASK>':
            repair_types[col_id] = float
        elif '<MASK>':
            repair_types[col_id] = int
        else:
            repair_types[col_id] = str
    ""\n     Create unique value structures: When performing repairs, we choose median values. If repair is partial, then values will be modified to some intermediate value between the original and the median value. However, the partially repaired value will only be chosen out of values that exist in the data set.  This prevents choosing values that might not make any sense in the data's context.  To do this, for each column, we need to sort all unique values and create two data structures: a list of values, and a dict mapping values to their positions in that list. Example: There are unique_col_vals[col] = [1, 2, 5, 7, 10, 14, 20] in the column. A value 2 must be repaired to 14, but the user requests that data only be repaired by 50%. We do this by finding the value at the right index:\n       index_lookup[col][2] = 1\n       index_lookup[col][14] = 5\n       this tells us that unique_col_vals[col][3] = 7 is 50% of the way from 2 to 14.\n    ""
    unique_col_vals = {}
    index_lookup = {}
    for col_id in not_I_col_ids:
        col_values = data_dict[col_id]
        col_values = sorted(list(set(col_values)))
        unique_col_vals[col_id] = col_values
        index_lookup[col_id] = {col_values[i]: i for i in range(len(col_values))}
    '\n     Make a list of unique values per each stratified column.  Then make a list of combinations of stratified groups. Example: race and gender cols are stratified: [(white, female), (white, male), (black, female), (black, male)] The combinations are tuples because they can be hashed and used as dictionary keys.  From these, find the sizes of these groups.\n    '
    unique_stratify_values = [unique_col_vals[i] for i in safe_stratify_cols]
    all_stratified_groups = list(product(*unique_stratify_values))
    stratified_group_indices = defaultdict(list)
    val_sets = {group: {col_id: set() for col_id in cols_to_repair} for group in all_stratified_groups}
    for i, row in enumerate(data_to_repair):
        group = tuple((row[col] for col in safe_stratify_cols))
        for col_id in cols_to_repair:
            val_sets[group][col_id].add(row[col_id])
        stratified_group_indices[group].append(i)
    '\n     Separate data by stratified group to perform repair on each Y column\'s values given that their corresponding protected attribute is a particular stratified group. We need to keep track of each Y column\'s values corresponding to each particular stratified group, as well as each value\'s index, so that when we repair the data, we can modify the correct value in the original data. Example: Supposing there is a Y column, ""Score1"", in which the 3rd and 5th scores, 70 and 90 respectively, belonged to black women, the data structure would look like: {(""Black"", ""Woman""): {Score1: [(70,2),(90,4)]}}\n    '
    stratified_group_data = {group: {} for group in all_stratified_groups}
    for group in all_stratified_groups:
        for col_id, col_dict in data_dict.items():
            indices = {}
            for i in stratified_group_indices[group]:
                value = col_dict[i]
                if '<MASK>':
                    indices[value] = []
                indices[value].append(i)
            stratified_col_values = [(occurs, val) for val, occurs in indices.items()]
            stratified_col_values.sort(key=lambda tup: tup[1])
            stratified_group_data[group][col_id] = stratified_col_values
    mode_feature_to_repair = get_mode(data_dict[self.feature_to_repair])
    for col_id in cols_to_repair:
        group_offsets = {group: 0 for group in all_stratified_groups}
        col = data_dict[col_id]
        num_quantiles = min((len(val_sets[group][col_id]) for group in all_stratified_groups))
        quantile_unit = 1.0 / num_quantiles
        if '<MASK>':
            for quantile in range(num_quantiles):
                median_at_quantiles = []
                indices_per_group = {}
                for group in all_stratified_groups:
                    group_data_at_col = stratified_group_data[group][col_id]
                    num_vals = len(group_data_at_col)
                    offset = int(round(group_offsets[group] * num_vals))
                    number_to_get = int(round((group_offsets[group] + quantile_unit) * num_vals) - offset)
                    group_offsets[group] += quantile_unit
                    if '<MASK>':
                        offset_data = group_data_at_col[offset:offset + number_to_get]
                        indices_per_group[group] = [i for val_indices, _ in offset_data for i in val_indices]
                        values = sorted([float(val) for _, val in offset_data])
                        median_at_quantiles.append(get_median(values, self.kdd))
                median = get_median(median_at_quantiles, self.kdd)
                median_val_pos = index_lookup[col_id][median]
                for group in all_stratified_groups:
                    for index in indices_per_group[group]:
                        original_value = col[index]
                        current_val_pos = index_lookup[col_id][original_value]
                        distance = median_val_pos - current_val_pos
                        distance_to_repair = int(round(distance * self.repair_level))
                        index_of_repair_value = current_val_pos + distance_to_repair
                        repaired_value = unique_col_vals[col_id][index_of_repair_value]
                        data_dict[col_id][index] = repaired_value
        elif '<MASK>':
            feature = CategoricalFeature(col)
            categories = feature.bin_index_dict.keys()
            group_features = get_group_data(all_stratified_groups, stratified_group_data, col_id)
            categories_count = get_categories_count(categories, all_stratified_groups, group_features)
            categories_count_norm = get_categories_count_norm(categories, all_stratified_groups, categories_count, group_features)
            median = get_median_per_category(categories, categories_count_norm)
            dist_generator = lambda group_index, category: gen_desired_dist(group_index, category, col_id, median, self.repair_level, categories_count_norm, self.feature_to_repair, mode_feature_to_repair)
            count_generator = lambda group_index, group, category: gen_desired_count(group_index, group, category, median, group_features, self.repair_level, categories_count)
            group_features, overflow = flow_on_group_features(all_stratified_groups, group_features, count_generator)
            group_features, assigned_overflow, distribution = assign_overflow(all_stratified_groups, categories, overflow, group_features, dist_generator)
            for group in all_stratified_groups:
                indices = stratified_group_indices[group]
                for i, index in enumerate(indices):
                    repaired_value = group_features[group].data[i]
                    data_dict[col_id][index] = repaired_value
    repaired_data = []
    for i, orig_row in enumerate(data_to_repair):
        new_row = [orig_row[j] if j not in cols_to_repair else data_dict[j][i] for j in col_ids]
        repaired_data.append(new_row)
    return repaired_data",False,"['self.kdd', 'i in self.features_to_ignore', 'all((isinstance(value, float) for value in values))', 'repair_types[col_id] in {int, float}', 'i == self.feature_to_repair', 'all((isinstance(value, int) for value in values))', 'repair_types[col_id] in {str}', 'value not in indices', 'number_to_get > 0']",___________________,0.0
"def find_recipes(folders, pattern=None, base=None):
    """"""find recipes will use a list of base folders, files,
       or patterns over a subset of content to find recipe files
       (indicated by Starting with Singularity
    
       Parameters
       ==========
        base: if defined, consider folders recursively below this level.

    """"""
    if '<MASK>':
        folders = os.getcwd()
    if '<MASK>':
        folders = [folders]
    manifest = dict()
    for base_folder in folders:
        custom_pattern = None
        if '<MASK>':
            manifest = find_single_recipe(filename=base_folder, pattern=pattern, manifest=manifest)
            continue
        elif '<MASK>':
            custom_pattern = base_folder.split('/')[-1:][0]
            base_folder = '/'.join(base_folder.split('/')[0:-1])
        manifest = find_folder_recipes(base_folder=base_folder, pattern=custom_pattern or pattern, manifest=manifest, base=base)
    return manifest",False,"['folders is None', 'not isinstance(folders, list)', 'os.path.isfile(base_folder)', 'not os.path.isdir(base_folder)']",___________________,0.0
"def add(self, image_path=None, image_uri=None, image_name=None, url=None, metadata=None, save=True, copy=False):
    """"""get or create a container, including the collection to add it to.
    This function can be used from a file on the local system, or via a URL
    that has been downloaded. Either way, if one of url, version, or image_file
    is not provided, the model is created without it. If a version is not
    provided but a file path is, then the file hash is used.

    Parameters
    ==========
    image_path: full path to image file
    image_name: if defined, the user wants a custom name (and not based on uri)
    metadata: any extra metadata to keep for the image (dict)
    save: if True, move the image to the cache if it's not there
    copy: If True, copy the image instead of moving it.

    image_name: a uri that gets parsed into a names object that looks like:

    {'collection': 'vsoch',
     'image': 'hello-world',
     'storage': 'vsoch/hello-world-latest.img',
     'tag': 'latest',
     'version': '12345'
     'uri': 'vsoch/hello-world:latest@12345'}

    After running add, the user will take some image in a working
    directory, add it to the database, and have it available for search
    and use under SREGISTRY_STORAGE/<collection>/<container>

    If the container was retrieved from a webby place, it should have version
    If no version is found, the file hash is used.
    """"""
    from sregistry.database.models import Container, Collection
    if '<MASK>':
        if '<MASK>':
            bot.error('Cannot find %s' % image_path)
            sys.exit(1)
    if '<MASK>':
        bot.error('You must provide an image uri <collection>/<namespace>')
        sys.exit(1)
    names = parse_image_name(remove_uri(image_uri))
    bot.debug('Adding %s to registry' % names['uri'])
    metadata = self.get_metadata(image_path, names=names)
    collection = self.get_or_create_collection(names['collection'])
    version = names.get('version')
    if '<MASK>':
        if '<MASK>':
            version = get_image_hash(image_path)
        else:
            version = ''
        names = parse_image_name(remove_uri(image_uri), version=version)
    if '<MASK>':
        if '<MASK>':
            image_name = self._get_storage_name(names)
        if '<MASK>':
            copyfile(image_path, image_name)
        else:
            shutil.move(image_path, image_name)
        image_path = image_name
    if '<MASK>':
        url = metadata['url']
    container = self.get_container(name=names['image'], collection_id=collection.id, tag=names['tag'], version=version)
    if '<MASK>':
        action = 'new'
        container = Container(metrics=json.dumps(metadata), name=names['image'], image=image_path, client=self.client_name, tag=names['tag'], version=version, url=url, uri=names['uri'], collection_id=collection.id)
        self.session.add(container)
        collection.containers.append(container)
    else:
        action = 'update'
        metrics = json.loads(container.metrics)
        metrics.update(metadata)
        container.url = url
        container.client = self.client_name
        if '<MASK>':
            container.image = image_path
        container.metrics = json.dumps(metrics)
    self.session.commit()
    bot.info('[container][%s] %s' % (action, names['uri']))
    return container",False,"['image_path is not None', 'image_uri is None', 'version == None', 'save is True and image_path is not None', ""url is None and 'url' in metadata"", 'container is None', 'not os.path.exists(image_path) and save is True', 'image_path != None', 'image_name is None', 'copy is True', 'image_path is not None']",___________________,0.0
"def write_configs(self, project_root):
    """"""Wrapper method that writes all configuration files to the pipeline
        directory
        """"""
    with open(join(project_root, 'resources.config'), 'w') as fh:
        fh.write(self.resources)
    with open(join(project_root, 'containers.config'), 'w') as fh:
        fh.write(self.containers)
    with open(join(project_root, 'params.config'), 'w') as fh:
        fh.write(self.params)
    with open(join(project_root, 'manifest.config'), 'w') as fh:
        fh.write(self.manifest)
    if '<MASK>':
        with open(join(project_root, 'user.config'), 'w') as fh:
            fh.write(self.user_config)
    lib_dir = join(project_root, 'lib')
    if '<MASK>':
        os.makedirs(lib_dir)
    with open(join(lib_dir, 'Helper.groovy'), 'w') as fh:
        fh.write(self.help)
    pipeline_to_json = self.render_pipeline()
    with open(splitext(self.nf_file)[0] + '.html', 'w') as fh:
        fh.write(pipeline_to_json)",False,"[""not exists(join(project_root, 'user.config'))"", 'not exists(lib_dir)']","__)
    """"""._)
    """"""._)
    """""".",0.0
"def get_configuration(feature, annot_beats, framesync, boundaries_id, labels_id):
    """"""Gets the configuration dictionary from the current parameters of the
    algorithms to be evaluated.""""""
    config = {}
    config['annot_beats'] = annot_beats
    config['feature'] = feature
    config['framesync'] = framesync
    bound_config = {}
    if '<MASK>':
        bound_config = eval(msaf.algorithms.__name__ + '.' + boundaries_id).config
        config.update(bound_config)
    if '<MASK>':
        label_config = eval(msaf.algorithms.__name__ + '.' + labels_id).config
        if '<MASK>':
            overlap = set(bound_config.keys()).intersection(set(label_config.keys()))
            assert len(overlap) == 0, 'Parameter %s must not exist both in %s and %s algorithms' % (overlap, boundaries_id, labels_id)
        config.update(label_config)
    return config",False,"[""boundaries_id != 'gt'"", 'labels_id is not None', 'labels_id != boundaries_id']",___________________,0.0
"def request(self, method, path, contents, headers, decode_json=False, stream=False, query=None, cdn=False):
    """"""
        See :py:func:`swiftly.client.client.Client.request`
        """"""
    if '<MASK>':
        raise Exception('CDN not yet supported with LocalClient')
    if '<MASK>':
        contents = StringIO(contents)
    if '<MASK>':
        headers = {}
    if '<MASK>':
        query = {}
    rpath = path.lstrip('/')
    if '<MASK>':
        container_name, object_name = rpath.split('/', 1)
    else:
        container_name = rpath
        object_name = ''
    if '<MASK>':
        status, reason, hdrs, body = self._account(method, contents, headers, stream, query, cdn)
    elif '<MASK>':
        status, reason, hdrs, body = self._container(method, container_name, contents, headers, stream, query, cdn)
    else:
        status, reason, hdrs, body = self._object(method, container_name, object_name, contents, headers, stream, query, cdn)
    if '<MASK>':
        if '<MASK>':
            if '<MASK>':
                body = loads(body)
            else:
                body = None
        return (status, reason, hdrs, body)
    raise Exception('%s %s failed: %s %s' % (method, path, status, reason))",False,"['cdn', 'isinstance(contents, six.string_types)', 'not headers', 'not query', ""'/' in rpath"", 'not container_name', 'status and status // 100 != 5', 'not object_name', 'not stream and decode_json and (status // 100 == 2)', 'body']",",,,,,,,,,,,,,,,,,,,",0.0
"def events(institute_id, case_name, event_id=None):
    """"""Handle events.""""""
    institute_obj, case_obj = institute_and_case(store, institute_id, case_name)
    link = request.form.get('link')
    content = request.form.get('content')
    variant_id = request.args.get('variant_id')
    user_obj = store.user(current_user.email)
    if '<MASK>':
        store.delete_event(event_id)
    elif '<MASK>':
        variant_obj = store.variant(variant_id)
        level = request.form.get('level', 'specific')
        store.comment(institute_obj, case_obj, user_obj, link, variant=variant_obj, content=content, comment_level=level)
    else:
        store.comment(institute_obj, case_obj, user_obj, link, content=content)
    return redirect(request.referrer)",False,"['event_id', 'variant_id']",___________________,0.0
"def gravatar(user_or_email, size=GRAVATAR_DEFAULT_SIZE, alt_text='', css_class='gravatar'):
    """""" Builds an gravatar <img> tag from an user or email """"""
    if '<MASK>':
        email = user_or_email.email
    else:
        email = user_or_email
    try:
        url = escape(get_gravatar_url(email=email, size=size))
    except:
        return ''
    return mark_safe('<img class=""{css_class}"" src=""{src}"" width=""{width}"" height=""{height}"" alt=""{alt}"" />'.format(css_class=css_class, src=url, width=size, height=size, alt=alt_text))",False,"[""hasattr(user_or_email, 'email')""]",___________________,0.0
"def send(self, message):
    """"""Verifies and sends message.

        :param message: Message instance.
        :param envelope_from: Email address to be used in MAIL FROM command.
        """"""
    assert message.send_to, 'No recipients have been added'
    if '<MASK>':
        raise BadHeaderError
    if '<MASK>':
        message.date = time.time()
    sender = message.sender or self.mail.default_sender
    if '<MASK>':
        self.host.sendmail(sanitize_address(sender) if sender is not None else None, message.send_to, message.as_string(self.mail.default_sender), message.mail_options, message.rcpt_options)
    email_dispatched.send(message, mail=self.mail)
    self.num_emails += 1
    if '<MASK>':
        self.num_emails = 0
        if '<MASK>':
            self.host.quit()
            self.host = self.configure_host()",False,"['message.has_bad_headers(self.mail.default_sender)', 'message.date is None', 'self.host', 'self.num_emails == self.mail.max_emails', 'self.host']",___________________,0.0
"def _const_val_to_py_ast(ctx: GeneratorContext, form: LispForm) -> GeneratedPyAST:
    """"""Generate Python AST nodes for constant Lisp forms.

    Nested values in collections for :const nodes are not parsed, so recursive
    structures need to call into this function to generate Python AST nodes for
    nested elements. For top-level :const Lisp AST nodes, see
    `_const_node_to_py_ast`.""""""
    handle_value = _CONST_VALUE_HANDLERS.get(type(form))
    if '<MASK>':
        handle_value = _const_seq_to_py_ast
    assert handle_value is not None, 'A type handler must be defined for constants'
    return handle_value(ctx, form)",False,"['handle_value is None and isinstance(form, ISeq)']",___________________,0.0
"def search(self, query=None, args=None):
    """"""query a Singularity registry for a list of images. 
     If query is None, collections are listed. 

    EXAMPLE QUERIES:

    [empty]             list all collections in registry
    vsoch               do a general search for the expression ""vsoch""
    vsoch/              list all containers in collection vsoch
    /dinosaur           list containers across collections called ""dinosaur""
    vsoch/dinosaur      list details of container vsoch/dinosaur
                          tag ""latest"" is used by default, and then the most recent
    vsoch/dinosaur:tag  list details for specific container
    
    """"""
    if '<MASK>':
        if '<MASK>':
            return self._collection_search(query)
        elif '<MASK>':
            return self._container_search(query, across_collections=True)
        elif '<MASK>':
            return self._container_search(query)
        return self._collection_search(query=query)
    return self._search_all()",False,"['query is not None', ""query.endswith('/')"", ""query.startswith('/')"", ""'/' in query or ':' in query""]",(((((((((((((((((((,0.0
"def _check_bases_classes(self, node):
    """"""check that the given class node implements abstract methods from
        base classes
        """"""

    def is_abstract(method):
        return method.is_abstract(pass_is_abstract=False)
    if '<MASK>':
        return
    methods = sorted(unimplemented_abstract_methods(node, is_abstract).items(), key=lambda item: item[0])
    for name, method in methods:
        owner = method.parent.frame()
        if '<MASK>':
            continue
        if '<MASK>':
            continue
        self.add_message('abstract-method', node=node, args=(name, owner.name))",False,"['class_is_abstract(node)', 'owner is node', 'name in node.locals']","_,,,,,,,,,,,,,,,,,,",0.0
"def report_raw_stats(sect, stats, _):
    """"""calculate percentage of code / doc / comment / empty
    """"""
    total_lines = stats['total_lines']
    if '<MASK>':
        raise EmptyReportError()
    sect.description = '%s lines have been analyzed' % total_lines
    lines = ('type', 'number', '%', 'previous', 'difference')
    for node_type in ('code', 'docstring', 'comment', 'empty'):
        key = node_type + '_lines'
        total = stats[key]
        percent = float(total * 100) / total_lines
        lines += (node_type, str(total), '%.2f' % percent, 'NC', 'NC')
    sect.append(Table(children=lines, cols=5, rheaders=1))",False,['not total_lines'],___________________,0.0
"def get_session(self):
    """"""
        Returns the Session currently used.

        :return: An instance of :class:`OpenSSL.SSL.Session` or
            :obj:`None` if no session exists.

        .. versionadded:: 0.14
        """"""
    session = _lib.SSL_get1_session(self._ssl)
    if '<MASK>':
        return None
    pysession = Session.__new__(Session)
    pysession._session = _ffi.gc(session, _lib.SSL_SESSION_free)
    return pysession",False,['session == _ffi.NULL'],"__)
    """"""
    """"""
    """"""
    """"""
    """"""
",0.0
"def _set_items(self, a_iter):
    """"""Clear and set the strings (and data if any) in the control from a list""""""
    self._items_dict = {}
    if '<MASK>':
        string_list = []
        data_list = []
    elif '<MASK>':
        raise ValueError('items must be an iterable')
    elif '<MASK>':
        self._items_dict = a_iter
        string_list = a_iter.values()
        data_list = a_iter.keys()
    elif '<MASK>':
        self._items_dict = dict(a_iter)
        data_list, string_list = zip(*a_iter)
    else:
        string_list = a_iter
        data_list = a_iter
    self.wx_obj.SetItems(string_list)
    for i, data in enumerate(data_list):
        self.set_data(i, data)",False,"['not a_iter', 'not isinstance(a_iter, (tuple, list, dict))', 'isinstance(a_iter, dict)', 'isinstance(a_iter[0], (tuple, list)) and len(a_iter[0]) == 2']",___________________,0.0
"def kill(args):
    """"""kill is a helper function to call the ""kill"" function of the client,
       meaning we bring down an instance.
    """"""
    from sregistry.main import Client as cli
    if '<MASK>':
        for name in args.commands:
            cli.destroy(name)
    sys.exit(0)",False,['len(args.commands) > 0'],(((((((((((((((((((,0.0
"def detect_voice(self, prob_detect_voice=0.5):
    """"""
        Returns self as a list of tuples:
        [('v', voiced segment), ('u', unvoiced segment), (etc.)]

        The overall order of the AudioSegment is preserved.

        :param prob_detect_voice: The raw probability that any random 20ms window of the audio file
                                  contains voice.
        :returns: The described list.
        """"""
    assert self.frame_rate in (48000, 32000, 16000, 8000), 'Try resampling to one of the allowed frame rates.'
    assert self.sample_width == 2, 'Try resampling to 16 bit.'
    assert self.channels == 1, 'Try resampling to one channel.'

    class model_class:

        def __init__(self, aggressiveness):
            self.v = webrtcvad.Vad(int(aggressiveness))

        def predict(self, vector):
            if '<MASK>':
                return 1
            else:
                return 0
    model = model_class(aggressiveness=2)
    pyesno = 0.3
    pnoyes = 0.2
    p_realyes_outputyes = 0.4
    p_realyes_outputno = 0.05
    p_yes_raw = prob_detect_voice
    filtered = self.detect_event(model=model, ms_per_input=20, transition_matrix=(pyesno, pnoyes), model_stats=(p_realyes_outputyes, p_realyes_outputno), event_length_s=0.25, prob_raw_yes=p_yes_raw)
    ret = []
    for tup in filtered:
        t = ('v', tup[1]) if tup[0] == 'y' else ('u', tup[1])
        ret.append(t)
    return ret",False,"['self.v.is_speech(vector.raw_data, vector.frame_rate)']",___________________,0.0
"def encode_output(value, output_file):
    """"""
    Encodes given value so it can be written to given file object.

    Value may be Unicode, binary string or any other data type.

    The exact behaviour depends on the Python version:

    Python 3.x

        `sys.stdout` is a `_io.TextIOWrapper` instance that accepts `str`
        (unicode) and breaks on `bytes`.

        It is OK to simply assume that everything is Unicode unless special
        handling is introduced in the client code.

        Thus, no additional processing is performed.

    Python 2.x

        `sys.stdout` is a file-like object that accepts `str` (bytes)
        and breaks when `unicode` is passed to `sys.stdout.write()`.

        We can expect both Unicode and bytes. They need to be encoded so as
        to match the file object encoding.

        The output is binary if the object doesn't explicitly require Unicode.

    """"""
    if '<MASK>':
        return compat.text_type(value)
    else:
        stream_encoding = getattr(output_file, 'encoding', None)
        if '<MASK>':
            if '<MASK>':
                return compat.text_type(value)
            else:
                return value.encode(stream_encoding, 'ignore')
        elif '<MASK>':
            return value.encode('utf-8')
        else:
            return str(value)",False,"['sys.version_info > (3, 0)', 'stream_encoding', ""stream_encoding.upper() == 'UTF-8'"", 'isinstance(value, compat.text_type)']",___________________,0.0
"def __write_to_hdf5_heavy(self, filename_out, *args, **kwargs):
    """""" Write data to HDF5 file.

        Args:
            filename_out (str): Name of output file
        """"""
    block_size = 0
    chunk_dim = self.__get_chunk_dimensions()
    blob_dim = self.__get_blob_dimensions(chunk_dim)
    n_blobs = self.container.calc_n_blobs(blob_dim)
    with h5py.File(filename_out, 'w') as h5:
        h5.attrs[b'CLASS'] = b'FILTERBANK'
        h5.attrs[b'VERSION'] = b'1.0'
        if '<MASK>':
            bs_compression = bitshuffle.h5.H5FILTER
            bs_compression_opts = (block_size, bitshuffle.h5.H5_COMPRESS_LZ4)
        else:
            bs_compression = None
            bs_compression_opts = None
            logger.warning('Warning: bitshuffle not found. No compression applied.')
        dset = h5.create_dataset('data', shape=self.selection_shape, chunks=chunk_dim, compression=bs_compression, compression_opts=bs_compression_opts, dtype=self.data.dtype)
        dset_mask = h5.create_dataset('mask', shape=self.selection_shape, chunks=chunk_dim, compression=bs_compression, compression_opts=bs_compression_opts, dtype='uint8')
        dset.dims[0].label = b'frequency'
        dset.dims[1].label = b'feed_id'
        dset.dims[2].label = b'time'
        dset_mask.dims[0].label = b'frequency'
        dset_mask.dims[1].label = b'feed_id'
        dset_mask.dims[2].label = b'time'
        for key, value in self.header.items():
            dset.attrs[key] = value
        if '<MASK>':
            logger.info('Using %i n_blobs to write the data.' % n_blobs)
            for ii in range(0, n_blobs):
                logger.info('Reading %i of %i' % (ii + 1, n_blobs))
                bob = self.container.read_blob(blob_dim, n_blob=ii)
                c_start = self.container.chan_start_idx + ii * blob_dim[self.freq_axis]
                t_start = self.container.t_start + c_start / self.selection_shape[self.freq_axis] * blob_dim[self.time_axis]
                t_stop = t_start + blob_dim[self.time_axis]
                c_start = c_start % self.selection_shape[self.freq_axis]
                c_stop = c_start + blob_dim[self.freq_axis]
                logger.debug(t_start, t_stop, c_start, c_stop)
                dset[t_start:t_stop, 0, c_start:c_stop] = bob[:]
        else:
            logger.info('Using %i n_blobs to write the data.' % n_blobs)
            for ii in range(0, n_blobs):
                logger.info('Reading %i of %i' % (ii + 1, n_blobs))
                bob = self.container.read_blob(blob_dim, n_blob=ii)
                t_start = self.container.t_start + ii * blob_dim[self.time_axis]
                if '<MASK>':
                    t_stop = self.n_ints_in_file
                else:
                    t_stop = (ii + 1) * blob_dim[self.time_axis]
                dset[t_start:t_stop] = bob[:]",False,"['HAS_BITSHUFFLE', 'blob_dim[self.freq_axis] < self.selection_shape[self.freq_axis]', '(ii + 1) * blob_dim[self.time_axis] > self.n_ints_in_file']",___________________,0.0
"def _sendPendingMessages(self):
    """"""Method sleeps, if nothing to do""""""
    if '<MASK>':
        time.sleep(0.1)
        return
    msg = self._queue.pop(0)
    if '<MASK>':
        self._sendMsg(msg)
        msg.refresh()
        if '<MASK>':
            self._queue.append(msg)
    else:
        self._queue.append(msg)
        time.sleep(0.01)",False,"['len(self._queue) == 0', 'msg.canSend()', 'not msg.isFinished()']",(((((((((((((((((((,0.0
"def get_token(self, grant_type, client_id, client_secret, redirect_uri, code, **params):
    """"""Generate access token HTTP response.

        :param grant_type: Desired grant type. Must be ""authorization_code"".
        :type grant_type: str
        :param client_id: Client ID.
        :type client_id: str
        :param client_secret: Client secret.
        :type client_secret: str
        :param redirect_uri: Client redirect URI.
        :type redirect_uri: str
        :param code: Authorization code.
        :type code: str
        :rtype: requests.Response
        """"""
    if '<MASK>':
        return self._make_json_error_response('unsupported_grant_type')
    is_valid_client_id = self.validate_client_id(client_id)
    is_valid_client_secret = self.validate_client_secret(client_id, client_secret)
    is_valid_redirect_uri = self.validate_redirect_uri(client_id, redirect_uri)
    scope = params.get('scope', '')
    is_valid_scope = self.validate_scope(client_id, scope)
    data = self.from_authorization_code(client_id, code, scope)
    is_valid_grant = data is not None
    if '<MASK>':
        return self._make_json_error_response('invalid_client')
    if '<MASK>':
        return self._make_json_error_response('invalid_grant')
    if '<MASK>':
        return self._make_json_error_response('invalid_scope')
    self.discard_authorization_code(client_id, code)
    access_token = self.generate_access_token()
    token_type = self.token_type
    expires_in = self.token_expires_in
    refresh_token = self.generate_refresh_token()
    self.persist_token_information(client_id=client_id, scope=scope, access_token=access_token, token_type=token_type, expires_in=expires_in, refresh_token=refresh_token, data=data)
    return self._make_json_response({'access_token': access_token, 'token_type': token_type, 'expires_in': expires_in, 'refresh_token': refresh_token})",False,"[""grant_type != 'authorization_code'"", 'not (is_valid_client_id and is_valid_client_secret)', 'not is_valid_grant or not is_valid_redirect_uri', 'not is_valid_scope']",___________________,0.0
"def load_certificate(type, buffer):
    """"""
    Load a certificate (X509) from the string *buffer* encoded with the
    type *type*.

    :param type: The file type (one of FILETYPE_PEM, FILETYPE_ASN1)

    :param bytes buffer: The buffer the certificate is stored in

    :return: The X509 object
    """"""
    if '<MASK>':
        buffer = buffer.encode('ascii')
    bio = _new_mem_buf(buffer)
    if '<MASK>':
        x509 = _lib.PEM_read_bio_X509(bio, _ffi.NULL, _ffi.NULL, _ffi.NULL)
    elif '<MASK>':
        x509 = _lib.d2i_X509_bio(bio, _ffi.NULL)
    else:
        raise ValueError('type argument must be FILETYPE_PEM or FILETYPE_ASN1')
    if '<MASK>':
        _raise_current_error()
    return X509._from_raw_x509_ptr(x509)",False,"['isinstance(buffer, _text_type)', 'type == FILETYPE_PEM', 'x509 == _ffi.NULL', 'type == FILETYPE_ASN1']",___________________,0.0
"def check():
    """"""Check configuration for sanity.
    """"""
    if '<MASK>':
        logger.warning('HTTPS CHECKS ARE TURNED OFF. A SECURE CONNECTION IS NOT GUARANTEED')
    if '<MASK>':
        open(config('server')['certificate'], 'rb').close()
    if '<MASK>':
        logger.info('Agent runs in backup mode. No data will be sent to Opencast')",False,"[""config('server')['insecure']"", ""config('server')['certificate']"", ""config('agent')['backup_mode']""]",___________________,0.0
"def getvolumeinfo(path):
    """"""
    Return information for the volume containing the given path. This is going
    to be a pair containing (file system, file system flags).
    """"""
    volpath = ctypes.create_unicode_buffer(len(path) + 2)
    rv = GetVolumePathName(path, volpath, len(volpath))
    if '<MASK>':
        raise WinError()
    fsnamebuf = ctypes.create_unicode_buffer(MAX_PATH + 1)
    fsflags = DWORD(0)
    rv = GetVolumeInformation(volpath, None, 0, None, None, byref(fsflags), fsnamebuf, len(fsnamebuf))
    if '<MASK>':
        raise WinError()
    return (fsnamebuf.value, fsflags.value)",False,"['rv == 0', 'rv == 0']","(,,,,,,,,,,,,,,,,,,",0.0
"def _score(estimator, Z_test, scorer):
    """"""Compute the score of an estimator on a given test set.""""""
    score = scorer(estimator, Z_test)
    if '<MASK>':
        raise ValueError('scoring must return a number, got %s (%s) instead.' % (str(score), type(score)))
    return score",False,"['not isinstance(score, numbers.Number)']",_((((((((((((((((((,0.0
"def set_pubkey(self, pkey):
    """"""
        Set the public key of the certificate.

        :param pkey: The public key.
        :type pkey: :py:class:`PKey`

        :return: :py:data:`None`
        """"""
    if '<MASK>':
        raise TypeError('pkey must be a PKey instance')
    set_result = _lib.X509_set_pubkey(self._x509, pkey._pkey)
    _openssl_assert(set_result == 1)",False,"['not isinstance(pkey, PKey)']",",,,,,,,,,,,,,,,,,,,",0.0
"def deleteRuleOnLogicalInterface(self, logicalInterfaceId, ruleId):
    """"""
        Deletes a rule from a logical interface
        Parameters:
          - logicalInterfaceId (string),
          - ruleId (string)
        Returns: response (object)
        Throws APIException on failure
        """"""
    req = ApiClient.oneRuleForLogicalInterfaceUrl % (self.host, '/draft', logicalInterfaceId, ruleId)
    resp = requests.delete(req, auth=self.credentials, verify=self.verify)
    if '<MASK>':
        self.logger.debug('Logical interface rule deleted')
    else:
        raise ibmiotf.APIException(resp.status_code, 'HTTP error deleting logical interface rule', resp)
    return resp",False,['resp.status_code == 204'],___________________,0.0
"def decode(self):
    """"""
        Decode/extract the domains to test from the adblock formated file.

        :return: The list of domains to test.
        :rtype: list
        """"""
    result = []
    regex = '^(?:.*\\|\\|)([^\\/\\$\\^]{1,}).*$'
    regex_v3 = '(?:#+(?:[a-z]+?)?\\[[a-z]+(?:\\^|\\*)\\=(?:\\\'|\\""))(.*\\..*)(?:(?:\\\'|\\"")\\])'
    regex_v4 = '^\\|(.*\\..*)\\|$'
    for line in self.to_format:
        rematch = rematch_v3 = rematch_v4 = None
        rematch = Regex(line, regex, return_data=True, rematch=True, group=0).match()
        rematch_v4 = Regex(line, regex_v4, return_data=True, rematch=True, group=0).match()
        rematch_v3 = Regex(line, regex_v3, return_data=True, rematch=True, group=0).match()
        if '<MASK>':
            if '<MASK>':
                options = line.split(self.options_separator)[-1].split(self.option_separator)
                if '<MASK>':
                    result.extend(self._extract_base(rematch))
                extra = self._handle_options(options)
                if '<MASK>':
                    extra.extend(self._extract_base(rematch))
                    result.extend(self._extract_base(extra))
                elif '<MASK>':
                    result.extend(self._extract_base(rematch))
            else:
                result.extend(self._extract_base(rematch))
        if '<MASK>':
            result.extend(List(self._format_decoded(rematch_v4)).format())
        if '<MASK>':
            result.extend(List(self._format_decoded(rematch_v3)).format())
    return List(result).format()",False,"['rematch', 'rematch_v4', 'rematch_v3', 'self.options_separator in line', ""not options[-1] or 'third-party' in options or 'script' in options or ('popup' in options) or ('xmlhttprequest' in options)"", 'extra and isinstance(extra, list)', 'extra']",___________________,0.0
"def record_line(self, frame, event, arg):
    """"""Records line execution time.""""""
    if '<MASK>':
        if '<MASK>':
            runtime = time.time() - self.prev_timestamp
            self.lines.append([self.prev_path, self.prev_lineno, runtime])
        self.prev_lineno = frame.f_lineno
        self.prev_path = frame.f_code.co_filename
        self.prev_timestamp = time.time()
    return self.record_line",False,"[""event == 'line'"", 'self.prev_timestamp']",___________________,0.0
"@coroutine
@functools.wraps(fn)
def wrapper(*args, **kw):
    _engine.activate()
    try:
        if '<MASK>':
            yield from fn(*args, **kw)
        else:
            fn(*args, **kw)
    finally:
        _engine.disable()",False,['iscoroutinefunction(fn)'],(((((((((((((((((((,0.0
"@app.before_request
def check_user():
    if '<MASK>':
        static_endpoint = 'static' in request.endpoint or 'report' in request.endpoint
        public_endpoint = getattr(app.view_functions[request.endpoint], 'is_public', False)
        relevant_endpoint = not (static_endpoint or public_endpoint)
        if '<MASK>':
            next_url = '{}?{}'.format(request.path, request.query_string.decode())
            login_url = url_for('login.login', next=next_url)
            return redirect(login_url)",False,"[""not app.config.get('LOGIN_DISABLED') and request.endpoint"", 'relevant_endpoint and (not current_user.is_authenticated)']",___________________,0.0
"def send_offer_assignment_email(self, user_email, offer_assignment_id, subject, email_body, site_code=None):
    """""" Sends the offer assignment email.
    Args:
        self: Ignore.
        user_email (str): Recipient's email address.
        offer_assignment_id (str): Key of the entry in the offer_assignment model.
        subject (str): Email subject.
        email_body (str): The body of the email.
        site_code (str): Identifier of the site sending the email.
    """"""
    config = get_sailthru_configuration(site_code)
    response = _send_offer_assignment_notification_email(config, user_email, subject, email_body, site_code, self)
    if '<MASK>':
        send_id = response.get_body().get('send_id')
        if '<MASK>':
            logger.info('[Offer Assignment] Offer assignment notification sent with message --- {message}'.format(message=email_body))
        else:
            logger.exception('[Offer Assignment] An error occurred while updating email status data for offer {token_offer} and email {token_email} via the ecommerce API.'.format(token_offer=offer_assignment_id, token_email=user_email))",False,"['response and response.is_ok()', ""_update_assignment_email_status(offer_assignment_id, send_id, 'success')""]",___________________,0.0
"def has_import(state, name, same_as=False, not_imported_msg='Did you import `{{pkg}}`?', incorrect_as_msg='Did you import `{{pkg}}` as `{{alias}}`?'):
    """"""Checks whether student imported a package or function correctly.

    Python features many ways to import packages.
    All of these different methods revolve around the ``import``, ``from`` and ``as`` keywords.
    ``has_import()`` provides a robust way to check whether a student correctly imported a certain package.

    By default, ``has_import()`` allows for different ways of aliasing the imported package or function.
    If you want to make sure the correct alias was used to refer to the package or function that was imported,
    set ``same_as=True``.

    Args:
        name (str): the name of the package that has to be checked.
        same_as (bool): if True, the alias of the package or function has to be the same. Defaults to False.
        not_imported_msg (str): feedback message when the package is not imported.
        incorrect_as_msg (str): feedback message if the alias is wrong.

    :Example:

        Example 1, where aliases don't matter (defaut): ::

            # solution
            import matplotlib.pyplot as plt

            # sct
            Ex().has_import(""matplotlib.pyplot"")

            # passing submissions
            import matplotlib.pyplot as plt
            from matplotlib import pyplot as plt
            import matplotlib.pyplot as pltttt

            # failing submissions
            import matplotlib as mpl

        Example 2, where the SCT is coded so aliases do matter: ::

            # solution
            import matplotlib.pyplot as plt

            # sct
            Ex().has_import(""matplotlib.pyplot"", same_as=True)

            # passing submissions
            import matplotlib.pyplot as plt
            from matplotlib import pyplot as plt

            # failing submissions
            import matplotlib.pyplot as pltttt

    """"""
    student_imports = state.ast_dispatcher('imports', state.student_ast)
    solution_imports = state.ast_dispatcher('imports', state.solution_ast)
    if '<MASK>':
        raise InstructorError(""`has_import()` couldn't find an import of the package %s in your solution code."" % name)
    fmt_kwargs = {'pkg': name, 'alias': solution_imports[name]}
    _msg = state.build_message(not_imported_msg, fmt_kwargs)
    state.do_test(DefinedCollTest(name, student_imports, _msg))
    if '<MASK>':
        _msg = state.build_message(incorrect_as_msg, fmt_kwargs)
        state.do_test(EqualTest(solution_imports[name], student_imports[name], _msg))
    return state",False,"['name not in solution_imports', 'same_as']",___________________,0.0
"def patch_agencies(agencies):
    """"""Fill the fields that are necessary for passing transitfeed checks.""""""
    yield Agency(-1, 'http://hiposfer.com', 'Unknown agency', 'Europe/Berlin')
    for agency_id, agency_url, agency_name, agency_timezone in agencies:
        if '<MASK>':
            agency_url = 'http://hiposfer.com'
        if '<MASK>':
            agency_timezone = 'Europe/Berlin'
        yield Agency(agency_id, agency_url, agency_name, agency_timezone)",False,"['not agency_url', 'not agency_timezone']",___________________,0.0
"def genes_by_alias(self, build='37', genes=None):
    """"""Return a dictionary with hgnc symbols as keys and a list of hgnc ids
             as value.

        If a gene symbol is listed as primary the list of ids will only consist
        of that entry if not the gene can not be determined so the result is a list
        of hgnc_ids

        Args:
            build(str)
            genes(iterable(scout.models.HgncGene)):

        Returns:
            alias_genes(dict): {<hgnc_alias>: {'true': <hgnc_id>, 'ids': {<hgnc_id_1>, <hgnc_id_2>, ...}}}
        """"""
    LOG.info('Fetching all genes by alias')
    alias_genes = {}
    if '<MASK>':
        genes = self.hgnc_collection.find({'build': build})
    for gene in genes:
        hgnc_id = gene['hgnc_id']
        hgnc_symbol = gene['hgnc_symbol']
        for alias in gene['aliases']:
            true_id = None
            if '<MASK>':
                true_id = hgnc_id
            if '<MASK>':
                alias_genes[alias]['ids'].add(hgnc_id)
                if '<MASK>':
                    alias_genes[alias]['true'] = hgnc_id
            else:
                alias_genes[alias] = {'true': hgnc_id, 'ids': set([hgnc_id])}
    return alias_genes",False,"['not genes', 'alias == hgnc_symbol', 'alias in alias_genes', 'true_id']",___________________,0.0
"def request(self, url, method='get', data=None, files=None, raw=False, raw_all=False, headers=dict(), raise_for=dict(), session=None):
    """"""Make synchronous HTTP request.
			Can be overidden to use different http module (e.g. urllib2, twisted, etc).""""""
    try:
        import requests
    except ImportError as exc:
        exc.args = ('Unable to find/import ""requests"" module. Please make sure that it is installed, e.g. by running ""pip install requests"" command.\nFor more info, visit: http://docs.python-requests.org/en/latest/user/install/',)
        raise exc
    if '<MASK>':
        patched_session = self._requests_setup(requests, **self.request_adapter_settings or dict())
        if '<MASK>':
            self._requests_session = patched_session
    if '<MASK>':
        session = getattr(self, '_requests_session', None)
        if '<MASK>':
            session = self._requests_session = requests.session()
    elif '<MASK>':
        session = requests
    method = method.lower()
    kwz = (self._requests_base_keywords or dict()).copy()
    kwz.update(self.request_extra_keywords or dict())
    kwz, func = (dict(), ft.partial(session.request, method.upper(), **kwz))
    kwz_headers = (self.request_base_headers or dict()).copy()
    kwz_headers.update(headers)
    if '<MASK>':
        if '<MASK>':
            if '<MASK>':
                data.seek(0)
                kwz['data'] = iter(ft.partial(data.read, 200 * 2 ** 10), b'')
            else:
                kwz['data'] = data
        else:
            kwz['data'] = json.dumps(data)
            kwz_headers.setdefault('Content-Type', 'application/json')
    if '<MASK>':
        for k, file_tuple in files.iteritems():
            if '<MASK>':
                files[k] = tuple(file_tuple) + ('application/octet-stream',)
            file_tuple[1].seek(0)
        kwz['files'] = files
    if '<MASK>':
        kwz['headers'] = kwz_headers
    code = res = None
    try:
        res = func(url, **kwz)
        code = res.status_code
        if '<MASK>':
            return
        if '<MASK>':
            res.raise_for_status()
    except requests.RequestException as err:
        message = b'{0} [type: {1}, repr: {0!r}]'.format(err, type(err))
        if '<MASK>':
            message = res.text
            try:
                message = json.loads(message)
            except:
                message = '{}: {!r}'.format(str(err), message)[:300]
            else:
                msg_err, msg_data = (message.pop('error', None), message)
                if '<MASK>':
                    message = '{}: {}'.format(msg_err.get('code', err), msg_err.get('message', msg_err))
                    if '<MASK>':
                        message = '{} (data: {})'.format(message, msg_data)
        raise raise_for.get(code, ProtocolError)(code, message)
    if '<MASK>':
        res = res.content
    elif '<MASK>':
        res = (code, dict(res.headers.items()), res.content)
    else:
        res = json.loads(res.text)
    return res",False,"['not self._requests_setup_done', 'session is None', 'data is not None', 'files is not None', 'kwz_headers', 'raw', 'patched_session is not None', 'not session', 'not session', ""method in ['post', 'put']"", 'code == requests.codes.no_content', 'code != requests.codes.ok', 'raw_all', ""all((hasattr(data, k) for k in ['seek', 'read']))"", 'len(file_tuple) == 2', ""(res and getattr(res, 'text', None)) is not None"", 'msg_err', 'msg_data']",___________________,0.0
"def openSession(self, slot, flags=0):
    """"""
        C_OpenSession

        :param slot: slot number returned by :func:`getSlotList`
        :type slot: integer
        :param flags: 0 (default), `CKF_RW_SESSION` for RW session
        :type flags: integer
        :return: a :class:`Session` object
        """"""
    se = PyKCS11.LowLevel.CK_SESSION_HANDLE()
    flags |= CKF_SERIAL_SESSION
    rv = self.lib.C_OpenSession(slot, flags, se)
    if '<MASK>':
        raise PyKCS11Error(rv)
    return Session(self, se)",False,['rv != CKR_OK'],___________________,0.0
"def get(self, uid=None, key_wrapping_specification=None):
    """"""
        Get a managed object from a KMIP appliance.

        Args:
            uid (string): The unique ID of the managed object to retrieve.
            key_wrapping_specification (dict): A dictionary containing various
                settings to be used when wrapping the key during retrieval.
                See Note below. Optional, defaults to None.

        Returns:
            ManagedObject: The retrieved managed object object.

        Raises:
            ClientConnectionNotOpen: if the client connection is unusable
            KmipOperationFailure: if the operation result is a failure
            TypeError: if the input argument is invalid

        Notes:
            The derivation_parameters argument is a dictionary that can
            contain the following key/value pairs:

            Key                             | Value
            --------------------------------|---------------------------------
            'wrapping_method'               | A WrappingMethod enumeration
                                            | that specifies how the object
                                            | should be wrapped.
            'encryption_key_information'    | A dictionary containing the ID
                                            | of the wrapping key and
                                            | associated cryptographic
                                            | parameters.
            'mac_signature_key_information' | A dictionary containing the ID
                                            | of the wrapping key and
                                            | associated cryptographic
                                            | parameters.
            'attribute_names'               | A list of strings representing
                                            | the names of attributes that
                                            | should be included with the
                                            | wrapped object.
            'encoding_option'               | An EncodingOption enumeration
                                            | that specifies the encoding of
                                            | the object before it is wrapped.
        """"""
    if '<MASK>':
        if '<MASK>':
            raise TypeError('uid must be a string')
    if '<MASK>':
        if '<MASK>':
            raise TypeError('Key wrapping specification must be a dictionary.')
    spec = self._build_key_wrapping_specification(key_wrapping_specification)
    result = self.proxy.get(uid, key_wrapping_specification=spec)
    status = result.result_status.value
    if '<MASK>':
        managed_object = self.object_factory.convert(result.secret)
        return managed_object
    else:
        reason = result.result_reason.value
        message = result.result_message.value
        raise exceptions.KmipOperationFailure(status, reason, message)",False,"['uid is not None', 'key_wrapping_specification is not None', 'status == enums.ResultStatus.SUCCESS', 'not isinstance(uid, six.string_types)', 'not isinstance(key_wrapping_specification, dict)']","_,,,,,,,,,,,,,,,,,,",0.0
"def invoke(self, line):
    """"""Invoke a one or more function given a list of arguments.

        The functions are searched for using the current context on the context stack
        and its annotated type information is used to convert all of the string parameters
        passed in line to appropriate python types.

        Args:
            line (list): The list of command line arguments.

        Returns:
            bool: A boolean specifying if the last function created a new context
                (False if a new context was created) and a list with the remainder of the
                command line if this function did not consume all arguments.)
        """"""
    finished = True
    while len(line) > 0:
        val, line, finished = self.invoke_one(line)
        if '<MASK>':
            iprint(val)
    return finished",False,['val is not None'],___________________,0.0
"def _get_consecutive_and_overlapping_fronts(onset_fronts, offset_fronts, onset_front_id, offset_front_id):
    """"""
    Gets an onset_front and an offset_front such that they both occupy at least some of the same
    frequency channels, then returns the portion of each that overlaps with the other.
    """"""
    onset_front = _get_front_idxs_from_id(onset_fronts, onset_front_id)
    offset_front = _get_front_idxs_from_id(offset_fronts, offset_front_id)
    consecutive_portions_of_onset_front = [c for c in _get_consecutive_portions_of_front(onset_front)]
    for consecutive_portion_of_onset_front in consecutive_portions_of_onset_front:
        onset_front_frequency_indexes = [f for f, _ in consecutive_portion_of_onset_front]
        overlapping_offset_front = [(f, s) for f, s in offset_front if f in onset_front_frequency_indexes]
        for consecutive_portion_of_offset_front in _get_consecutive_portions_of_front(overlapping_offset_front):
            if '<MASK>':
                return (consecutive_portion_of_onset_front, consecutive_portion_of_offset_front)
    return ([], [])",False,['consecutive_portion_of_offset_front'],___________________,0.0
"def invoke_string(self, line):
    """"""Parse and invoke a string line.

        Args:
            line (str): The line that we want to parse and invoke.

        Returns:
            bool: A boolean specifying if the last function created a new context
                (False if a new context was created) and a list with the remainder of the
                command line if this function did not consume all arguments.)
        """"""
    line = str(line)
    if '<MASK>':
        return True
    if '<MASK>':
        return True
    args = self._split_line(line)
    return self.invoke(args)",False,"['len(line) == 0', ""line[0] == u'#'""]",___________________,0.0
"def draw_x_labels(self):
    """"""Draw the X axis labels""""""
    if '<MASK>':
        labels = self.get_x_labels()
        count = len(labels)
        labels = enumerate(iter(labels))
        start = int(not self.step_include_first_x_label)
        labels = itertools.islice(labels, start, None, self.step_x_labels)
        list(map(self.draw_x_label, labels))
        self.draw_x_guidelines(self.field_width(), count)",False,['self.show_x_labels'],_x_x_x_x_x_x_x_x_x_,0.0
"def kernel_matrix_xX(svm_model, original_x, original_X):
    if '<MASK>':
        K = (svm_model.zeta + svm_model.gamma * np.dot(original_x, original_X.T)) ** svm_model.Q
    elif '<MASK>':
        K = np.exp(-svm_model.gamma * cdist(original_X, np.atleast_2d(original_x), 'euclidean').T ** 2).ravel()
    ""\n        K = np.zeros((svm_model.data_num, svm_model.data_num))\n\n        for i in range(svm_model.data_num):\n            for j in range(svm_model.data_num):\n                if (svm_model.svm_kernel == 'polynomial_kernel' or svm_model.svm_kernel == 'soft_polynomial_kernel'):\n                    K[i, j] = Kernel.polynomial_kernel(svm_model, original_x, original_X[j])\n                elif (svm_model.svm_kernel == 'gaussian_kernel' or svm_model.svm_kernel == 'soft_gaussian_kernel'):\n                    K[i, j] = Kernel.gaussian_kernel(svm_model, original_x, original_X[j])\n        ""
    return K",False,"[""svm_model.svm_kernel == 'polynomial_kernel' or svm_model.svm_kernel == 'soft_polynomial_kernel'"", ""svm_model.svm_kernel == 'gaussian_kernel' or svm_model.svm_kernel == 'soft_gaussian_kernel'""]",___________________,0.0
"def decrement(self):
    """"""Decrement the count by one""""""
    with self._lock:
        if '<MASK>':
            raise RuntimeError('Counter is at zero. It cannot dip below zero')
        self._count -= 1
        if '<MASK>':
            self._callback()",False,"['self._count == 0', 'self._is_finalized and self._count == 0']","(self):
        """""" """"""
        """"""
        """"""
        """"""
        """"""",0.0
"def init(name, *args):
    """"""
    Initializes a matcher instance passing variadic arguments to
    its constructor. Acts as a delegator proxy.

    Arguments:
        name (str): matcher class name or alias to execute.
        *args (mixed): variadic argument

    Returns:
        matcher: matcher instance.

    Raises:
        ValueError: if matcher was not found.
    """"""
    matcher = get(name)
    if '<MASK>':
        raise ValueError('Cannot find matcher: {}'.format(name))
    return matcher(*args)",False,['not matcher'],"(name,
    """"""
    """"""
    """"""
    """"""
    """"""
",0.0
"def finalize(self, remove_all_handlers=True):
    """"""Finalizes the manager, closes and removes all handlers if desired.""""""
    for tool in self._tools:
        tool.finalize()
    self._tools = []
    self._stdout_to_logger = None
    for config in (self._sp_config, self._mp_config):
        if '<MASK>':
            config.close()
    self._sp_config = None
    self._mp_config = None
    if '<MASK>':
        self.tabula_rasa()",False,"['remove_all_handlers', ""hasattr(config, 'close')""]",___________________,0.0
"def shutdown(self, *args):
    """"""
        Shutdown the running process and the monitor
        """"""
    try:
        self._shutdown()
        if '<MASK>':
            self.process.wait()
            self.process.stdout.close()
            self.process.stdin.close()
            self.process.stderr.close()
        self.finished = True
        self.send_testcase('', '127.0.0.1', self.config.ports['servers']['TCASE_PORT'])
        self.logger.debug('[{0}] - PJFProcessMonitor successfully completed'.format(time.strftime('%H:%M:%S')))
    except Exception as e:
        raise PJFBaseException(e.message if hasattr(e, 'message') else str(e))",False,['self.process'],___________________,0.0
"def run_script(self, log=None, ml_log=None, mlp_out=None, overwrite=False, file_out=None, output_mask=None, script_file=None, print_meshlabserver_output=True):
    """""" Run the script
        """"""
    temp_script = False
    temp_ml_log = False
    if '<MASK>':
        temp_file_in_file = tempfile.NamedTemporaryFile(delete=False, suffix='.xyz', dir=os.getcwd())
        temp_file_in_file.write(b'0 0 0')
        temp_file_in_file.close()
        self.file_in = [temp_file_in_file.name]
    if '<MASK>':
        script_file = None
    elif '<MASK>':
        temp_script = True
        temp_script_file = tempfile.NamedTemporaryFile(delete=False, suffix='.mlx')
        temp_script_file.close()
        self.save_to_file(temp_script_file.name)
        script_file = temp_script_file.name
    if '<MASK>':
        temp_ml_log = True
        ml_log_file = tempfile.NamedTemporaryFile(delete=False, suffix='.txt')
        ml_log_file.close()
        ml_log = ml_log_file.name
    if '<MASK>':
        file_out = self.file_out
    run(script=script_file, log=log, ml_log=ml_log, mlp_in=self.mlp_in, mlp_out=mlp_out, overwrite=overwrite, file_in=self.file_in, file_out=file_out, output_mask=output_mask, ml_version=self.ml_version, print_meshlabserver_output=print_meshlabserver_output)
    if '<MASK>':
        self.geometry = compute.parse_geometry(ml_log, log, print_output=print_meshlabserver_output)
    if '<MASK>':
        self.topology = compute.parse_topology(ml_log, log, print_output=print_meshlabserver_output)
    if '<MASK>':
        self.hausdorff_distance = compute.parse_hausdorff(ml_log, log, print_output=print_meshlabserver_output)
    if '<MASK>':
        os.remove(temp_file_in_file.name)
    if '<MASK>':
        os.remove(temp_script_file.name)
    if '<MASK>':
        os.remove(ml_log_file.name)",False,"['self.__no_file_in', 'not self.filters', '(self.parse_geometry or self.parse_topology or self.parse_hausdorff) and ml_log is None', 'file_out is None', 'self.parse_geometry', 'self.parse_topology', 'self.parse_hausdorff', 'self.__no_file_in', 'temp_script', 'temp_ml_log', 'script_file is None']",___________________,0.0
"def check_weekday(year, month, day, reverse=False):
    """"""
    Make sure any event day we send back for weekday repeating
    events is not a weekend.
    """"""
    d = date(year, month, day)
    while d.weekday() in (5, 6):
        if '<MASK>':
            d -= timedelta(days=1)
        else:
            d += timedelta(days=1)
    return (d.year, d.month, d.day)",False,['reverse'],",,,,,,,,,,,,,,,,,,,",0.0
"def preview(self, state):
    """"""Previews a potentially non-existant focus within
        `state`. Returns `Just(focus)` if it exists, Nothing otherwise.

        Requires kind Fold.
        """"""
    if '<MASK>':
        raise TypeError('Must be an instance of Fold to .preview()')
    pure = lambda a: Const(Nothing())
    func = lambda a: Const(Just(a))
    return self.apply(func, pure, state).unwrap()",False,['not self._is_kind(Fold)'],___________________,0.0
"def f_delete_items(self, iterator, *args, **kwargs):
    """"""Deletes items from storage on disk.

        Per default the item is NOT removed from the trajectory.

        Links are NOT deleted on the hard disk, please delete links manually before deleting
        data!

        :param iterator:

            A sequence of items you want to remove. Either the instances themselves
            or strings with the names of the items.

        :param remove_from_trajectory:

            If items should also be removed from trajectory. Default is `False`.


        :param args: Additional arguments passed to the storage service

        :param kwargs: Additional keyword arguments passed to the storage service

            If you use the standard hdf5 storage service, you can pass the following additional
            keyword argument:

            :param delete_only:

                You can partially delete leaf nodes. Specify a list of parts of the result node
                that should be deleted like `delete_only=['mystuff','otherstuff']`.
                This wil only delete the hdf5 sub parts `mystuff` and `otherstuff` from disk.
                BE CAREFUL,
                erasing data partly happens at your own risk. Depending on how complex the
                loading process of your result node is, you might not be able to reconstruct
                any data due to partially deleting some of it.

                Be aware that you need to specify the names of parts as they were stored
                to HDF5. Depending on how your leaf construction works, this may differ
                from the names the data might have in your leaf in the trajectory container.

                If the hdf5 nodes you specified in `delete_only` cannot be found a warning
                is issued.

                Note that massive deletion will fragment your HDF5 file.
                Try to avoid changing data on disk whenever you can.

                If you want to erase a full node, simply ignore this argument or set to `None`.

            :param remove_from_item:

                If data that you want to delete from storage should also be removed from
                the items in `iterator` if they contain these. Default is `False`.

            :param recursive:

                If you want to delete a group node and it has children you need to
                set `recursive` to `True. Default is `False`.

        """"""
    remove_from_trajectory = kwargs.pop('remove_from_trajectory', False)
    recursive = kwargs.get('recursive', False)
    fetched_items = self._nn_interface._fetch_items(REMOVE, iterator, args, kwargs)
    if '<MASK>':
        try:
            self._storage_service.store(pypetconstants.LIST, fetched_items, trajectory_name=self.v_name)
        except:
            self._logger.error('Could not remove `%s` from the trajectory. Maybe the item(s) was/were never stored to disk.' % str(fetched_items))
            raise
        for _, item, dummy1, dummy2 in fetched_items:
            if '<MASK>':
                self._nn_interface._remove_node_or_leaf(item, recursive=recursive)
            else:
                item._stored = False
    else:
        self._logger.warning('Your removal was not successful, could not find a single item to remove.')",False,"['fetched_items', 'remove_from_trajectory']",___________________,0.0
"def _trace_memory_usage(self, frame, event, arg):
    """"""Checks memory usage when 'line' event occur.""""""
    if '<MASK>':
        self._events_list.append((frame.f_lineno, self._process.memory_info().rss, frame.f_code.co_name, frame.f_code.co_filename))
    return self._trace_memory_usage",False,"[""event == 'line' and frame.f_code.co_filename in self.target_modules""]",___________________,0.0
"def _get_content(cls, file):
    """"""
        Get and return the content of the given log file.

        :param file: The file we have to get the content from.
        :type file: str

        :return The content of the given file.
        :rtype: dict
        """"""
    if '<MASK>':
        return Dict().from_json(File(file).read())
    return {}",False,['PyFunceble.path.isfile(file)'],"_,,,,,,,,,,,,,,,,,,",0.0
"def to_unicode(string):
    """"""Convert a string (bytes, str or unicode) to unicode.""""""
    assert isinstance(string, basestring)
    if '<MASK>':
        if '<MASK>':
            return string.decode('utf-8')
        else:
            return string
    elif '<MASK>':
        return string.decode('utf-8')
    else:
        return string",False,"['sys.version_info[0] >= 3', 'isinstance(string, bytes)', 'isinstance(string, str)']",___________________,0.0
"def add_patches(self, patches, after=None):
    """""" Add a list of patches to the patches list """"""
    if '<MASK>':
        self.insert_patches(patches)
    else:
        self._check_patch(after)
        patchlines = self._patchlines_before(after)
        patchlines.append(self.patch2line[after])
        for patch in patches:
            patchline = PatchLine(patch)
            patchlines.append(patchline)
            self.patch2line[patchline.get_patch()] = patchline
        patchlines.extend(self._patchlines_after(after))
        self.patchlines = patchlines",False,['after is None'],___________________,0.0
"def _determine_function_name_type(node, config=None):
    """"""Determine the name type whose regex the a function's name should match.

    :param node: A function node.
    :type node: astroid.node_classes.NodeNG
    :param config: Configuration from which to pull additional property classes.
    :type config: :class:`optparse.Values`

    :returns: One of ('function', 'method', 'attr')
    :rtype: str
    """"""
    property_classes, property_names = _get_properties(config)
    if '<MASK>':
        return 'function'
    if '<MASK>':
        decorators = node.decorators.nodes
    else:
        decorators = []
    for decorator in decorators:
        if '<MASK>':
            infered = utils.safe_infer(decorator)
            if '<MASK>':
                return 'attr'
        elif '<MASK>':
            return 'attr'
    return 'method'",False,"['not node.is_method()', 'node.decorators', 'isinstance(decorator, astroid.Name) or (isinstance(decorator, astroid.Attribute) and decorator.attrname in property_names)', 'infered and infered.qname() in property_classes', ""isinstance(decorator, astroid.Attribute) and decorator.attrname in ('setter', 'deleter')""]","_____________)
    """""" """"""
",0.0
"def select_option(self):
    """""" Select this node if it is an option element inside a select tag. """"""
    if '<MASK>':
        warn('Attempt to select disabled option: {}'.format(self.value or self.text))
    self.base.select_option()",False,['self.disabled'],"__self):
    """"""............",0.0
"def check_install(software=None, quiet=True):
    """"""check_install will attempt to run the singularity command, and
       return True if installed. The command line utils will not run 
       without this check.

       Parameters
       ==========
       software: the software to check if installed
       quiet: should we be quiet? (default True)
    """"""
    if '<MASK>':
        software = 'singularity'
    cmd = [software, '--version']
    try:
        version = run_command(cmd, software)
    except:
        return False
    if '<MASK>':
        if '<MASK>':
            version = version['message']
            bot.info('Found %s version %s' % (software.upper(), version))
        return True
    return False",False,"['software is None', 'version is not None', ""quiet is False and version['return_code'] == 0""]",___________________,0.0
"def is_starved(self):
    """"""
        Used to identify when buffered messages should be processed and responded to.

        When max_in_flight > 1 and you're batching messages together to perform work
        is isn't possible to just compare the len of your list of buffered messages against
        your configured max_in_flight (because max_in_flight may not be evenly divisible
        by the number of producers you're connected to, ie. you might never get that many
        messages... it's a *max*).

        Example::

            def message_handler(self, nsq_msg, reader):
                # buffer messages
                if reader.is_starved():
                    # perform work

            reader = nsq.Reader(...)
            reader.set_message_handler(functools.partial(message_handler, reader=reader))
            nsq.run()
        """"""
    for conn in itervalues(self.conns):
        if '<MASK>':
            return True
    return False",False,['conn.in_flight > 0 and conn.in_flight >= conn.last_rdy * 0.85'],___________________,0.0
"def thickest(self, n=1, index=False):
    """"""
        Returns the thickest interval(s) as a striplog.

        Args:
            n (int): The number of thickest intervals to return. Default: 1.
            index (bool): If True, only the indices of the intervals are
                returned. You can use this to index into the striplog.

        Returns:
            Interval. The thickest interval. Or, if ``index`` was ``True``,
            the index of the thickest interval.
        """"""
    s = sorted(range(len(self)), key=lambda k: self[k].thickness)
    indices = s[-n:]
    if '<MASK>':
        return indices
    elif '<MASK>':
        i = indices[0]
        return self[i]
    else:
        return self[indices]",False,"['index', 'n == 1']",(((((((((((((((((((,0.0
"def write(self, output_stream, kmip_version=enums.KMIPVersion.KMIP_1_0):
    """"""
        Write the data encoding the Sign response to a stream.

        Args:
            output_stream (stream): A data stream in which to encode object
                data, supporting a write method; usually a BytearrayStream
                object.

        Raises:
            ValueError: Raised if the unique_identifier or signature
                attributes are not defined.
        """"""
    local_stream = utils.BytearrayStream()
    if '<MASK>':
        self._unique_identifier.write(local_stream, kmip_version=kmip_version)
    else:
        raise ValueError('invalid payload missing the unique identifier attribute')
    if '<MASK>':
        self._signature_data.write(local_stream, kmip_version=kmip_version)
    else:
        raise ValueError('invalid payload missing the signature attribute')
    self.length = local_stream.length()
    super(SignResponsePayload, self).write(output_stream, kmip_version=kmip_version)
    output_stream.write(local_stream.buffer)",False,"['self._unique_identifier', 'self._signature_data']","_,_,_,_,_,_,_,_,_,_",0.0
"def spawn(self, ident, func, *args, **kwargs):
    """"""
        Returns immediately to the caller and begins executing the
        func in the background. Use get_results and the ident given
        to retrieve the results of the func. If the func causes an
        exception, this exception will be caught and the
        sys.exc_info() will be returned via get_results.

        :param ident: An identifier to find the results of the func
            from get_results. This identifier can be anything unique
            to the Concurrency instance.
        :param func: The function to execute concurrently.
        :param args: The args to give the func.
        :param kwargs: The keyword args to the give the func.
        :returns: None
        """"""
    if '<MASK>':
        self._pool.spawn_n(self._spawner, ident, func, *args, **kwargs)
        sleep()
    else:
        self._spawner(ident, func, *args, **kwargs)",False,['self._pool'],___________________,0.0
"def _file_in_next_patches(self, filename, patch):
    """""" Checks if a backup file of the filename in the applied patches after
        patch exists """"""
    if '<MASK>':
        return
    patches = self.db.patches_after(patch)
    for patch in patches:
        file = self.quilt_pc + File(os.path.join(patch.get_name(), filename))
        if '<MASK>':
            raise QuiltError('File %s is modified by patch %s' % (filename, patch.get_name()))",False,"['not self.db.is_patch(patch)', 'file.exists()']","____)
        """""".)
        """""".)
        """""".",0.0
"def run(self):
    """"""Runs server""""""
    try:
        self._start()
        running = True
        while running:
            msg = ''
            name = ''
            client_id = ''
            request_id = ''
            request = self._socket.recv_string()
            self._logger.log(1, 'Recevied REQ `%s`', request)
            split_msg = request.split(self.DELIMITER)
            if '<MASK>':
                msg, name, client_id, request_id = split_msg
            if '<MASK>':
                response = self._lock(name, client_id, request_id)
            elif '<MASK>':
                response = self._unlock(name, client_id, request_id)
            elif '<MASK>':
                response = self.PONG
            elif '<MASK>':
                response = self.CLOSED
                running = False
            else:
                response = self.MSG_ERROR + self.DELIMITER + 'Request `%s` not understood (or wrong number of delimiters)' % request
                self._logger.error(response)
            respond = self._pre_respond_hook(response)
            if '<MASK>':
                self._logger.log(1, 'Sending REP `%s` to `%s` (request id `%s`)', response, client_id, request_id)
                self._socket.send_string(response)
        self._close()
    except Exception:
        self._logger.exception('Crashed Lock Server!')
        raise",False,"['len(split_msg) == 4', 'msg == self.LOCK', 'respond', 'msg == self.UNLOCK', 'msg == self.PING', 'msg == self.DONE']",___________________,0.0
"def validateDeviceTypeConfiguration(self, typeId):
    """"""
        Validate the device type configuration.
        Parameters:
            - typeId (string) - the platform device type
        Throws APIException on failure.
        """"""
    req = ApiClient.draftDeviceTypeUrl % (self.host, typeId)
    body = {'operation': 'validate-configuration'}
    resp = requests.patch(req, auth=self.credentials, headers={'Content-Type': 'application/json'}, data=json.dumps(body), verify=self.verify)
    if '<MASK>':
        self.logger.debug('Validation for device type configuration succeeded')
    else:
        raise ibmiotf.APIException(resp.status_code, 'Validation for device type configuration failed', resp)
    return resp.json()",False,['resp.status_code == 200'],",,,,,,,,,,,,,,,,,,,",0.0
"def parse_bool(value):
    """"""
    Parse string to bool.

    :param str value: String value to parse as bool
    :return bool:
    """"""
    boolean = parse_str(value).capitalize()
    if '<MASK>':
        return True
    elif '<MASK>':
        return False
    else:
        raise ValueError('Unable to parse boolean value ""{}""'.format(value))",False,"[""boolean in ('True', 'Yes', 'On', '1')"", ""boolean in ('False', 'No', 'Off', '0')""]","_(value):
    """"""
    """"""
    """"""
    """"""
    """"""",0.0
"def create_app(config_file=None, config=None):
    """"""Flask app factory function.""""""
    app = Flask(__name__)
    app.config.from_pyfile('config.py')
    app.jinja_env.add_extension('jinja2.ext.do')
    if '<MASK>':
        app.config.update(config)
    if '<MASK>':
        app.config.from_pyfile(config_file)
    app.mme_nodes = mme_nodes(app.config.get('MME_URL'), app.config.get('MME_TOKEN'))
    app.config['JSON_SORT_KEYS'] = False
    current_log_level = logger.getEffectiveLevel()
    coloredlogs.install(level='DEBUG' if app.debug else current_log_level)
    configure_extensions(app)
    register_blueprints(app)
    register_filters(app)
    if '<MASK>':
        configure_email_logging(app)

    @app.before_request
    def check_user():
        if '<MASK>':
            static_endpoint = 'static' in request.endpoint or 'report' in request.endpoint
            public_endpoint = getattr(app.view_functions[request.endpoint], 'is_public', False)
            relevant_endpoint = not (static_endpoint or public_endpoint)
            if '<MASK>':
                next_url = '{}?{}'.format(request.path, request.query_string.decode())
                login_url = url_for('login.login', next=next_url)
                return redirect(login_url)
    return app",False,"['config', 'config_file', ""not (app.debug or app.testing) and app.config.get('MAIL_USERNAME')"", ""not app.config.get('LOGIN_DISABLED') and request.endpoint"", 'relevant_endpoint and (not current_user.is_authenticated)']",___________________,0.0
"def expire_in(self, value):
    """"""
        Computes :attr:`.expiration_time` when the value is set.
        """"""
    if '<MASK>':
        self._expiration_time = int(time.time()) + int(value)
        self._expire_in = value",False,['value'],_time_time_time_time_time_time_time_time_time_,0.0
"def map(self, mols, nproc=None, nmols=None, quiet=False, ipynb=False, id=-1):
    """"""Calculate descriptors over mols.

        Parameters:
            mols(Iterable[rdkit.Mol]): moleculars

            nproc(int): number of process to use. default: multiprocessing.cpu_count()

            nmols(int): number of all mols to use in progress-bar. default: mols.__len__()

            quiet(bool): don't show progress bar. default: False

            ipynb(bool): use ipython notebook progress bar. default: False

            id(int): conformer id to use. default: -1.

        Returns:
            Iterator[Result[scalar]]

        """"""
    if '<MASK>':
        nproc = cpu_count()
    if '<MASK>':
        nmols = len(mols)
    if '<MASK>':
        return self._serial(mols, nmols=nmols, quiet=quiet, ipynb=ipynb, id=id)
    else:
        return self._parallel(mols, nproc, nmols=nmols, quiet=quiet, ipynb=ipynb, id=id)",False,"['nproc is None', ""hasattr(mols, '__len__')"", 'nproc == 1']",___________________,0.0
"def get_signature_algorithm(self):
    """"""
        Return the signature algorithm used in the certificate.

        :return: The name of the algorithm.
        :rtype: :py:class:`bytes`

        :raises ValueError: If the signature algorithm is undefined.

        .. versionadded:: 0.13
        """"""
    algor = _lib.X509_get0_tbs_sigalg(self._x509)
    nid = _lib.OBJ_obj2nid(algor.algorithm)
    if '<MASK>':
        raise ValueError('Undefined signature algorithm')
    return _ffi.string(_lib.OBJ_nid2ln(nid))",False,['nid == _lib.NID_undef'],___________________,0.0
"def gaussian_cost(X):
    """"""Return the average log-likelihood of data under a standard normal
    """"""
    d, n = X.shape
    if '<MASK>':
        return 0
    sigma = np.var(X, axis=1, ddof=1)
    cost = -0.5 * d * n * np.log(2.0 * np.pi) - 0.5 * (n - 1.0) * np.sum(sigma)
    return cost",False,['n < 2'],(((((((((((((((((((,0.0
"def apply_to_with_tz(self, dttm, timezone):
    """"""We make sure that after truncating we use the correct timezone,
        even if we 'jump' over a daylight saving time switch.

        I.e. if we apply ""@d"" to `Sun Oct 30 04:30:00 CET 2016` (1477798200)
        we want to have `Sun Oct 30 00:00:00 CEST 2016` (1477778400)
        but not `Sun Oct 30 00:00:00 CET 2016` (1477782000)
        """"""
    result = self.apply_to(dttm)
    if '<MASK>':
        naive_dttm = datetime(result.year, result.month, result.day)
        result = timezone.localize(naive_dttm)
    return result",False,"['self.unit in [DAYS, WEEKS, MONTHS, YEARS]']",(((((((((((((((((((,0.0
"def mb_handler(self, args):
    """"""Handler for mb command""""""
    if '<MASK>':
        raise InvalidArgument('No s3 bucketname provided')
    self.validate('cmd|s3', args)
    self.s3handler().create_bucket(args[1])",False,['len(args) == 1'],"_(self,
    """""" """"""
    """"""
    """"""
    """"""
   ",0.0
"def get(self, deviceUid, eventId):
    """"""
        Retrieves the last cached message for specified event from a specific device.
        """"""
    if '<MASK>':
        deviceUid = DeviceUid(**deviceUid)
    url = 'api/v0002/device/types/%s/devices/%s/events/%s' % (deviceUid.typeId, deviceUid.deviceId, eventId)
    r = self._apiClient.get(url)
    if '<MASK>':
        return LastEvent(**r.json())
    else:
        raise ApiException(r)",False,"['not isinstance(deviceUid, DeviceUid) and isinstance(deviceUid, dict)', 'r.status_code == 200']",___________________,0.0
"def _ini_format(stream, options):
    """"""format options using the INI format""""""
    for optname, optdict, value in options:
        value = _format_option_value(optdict, value)
        help_opt = optdict.get('help')
        if '<MASK>':
            help_opt = normalize_text(help_opt, line_len=79, indent='# ')
            print(file=stream)
            print(help_opt, file=stream)
        else:
            print(file=stream)
        if '<MASK>':
            print('#%s=' % optname, file=stream)
        else:
            value = str(value).strip()
            if '<MASK>':
                separator = '\n ' + ' ' * len(optname)
                value = separator.join((x + ',' for x in str(value).split(',')))
                value = value[:-1]
            print('%s=%s' % (optname, value), file=stream)",False,"['help_opt', 'value is None', ""re.match('^([\\\\w-]+,)+[\\\\w-]+$', str(value))""]",___________________,0.0
"def case_mme_delete(self, case_obj, user_obj):
    """"""Delete a MatchMaker submission from a case record
           and creates the related event.
        Args:
            case_obj(dict): a scout case object
            user_obj(dict): a scout user object
        Returns:
            updated_case(dict): the updated scout case

        """"""
    institute_obj = self.institute(case_obj['owner'])
    for individual in case_obj['individuals']:
        if '<MASK>':
            self.create_event(institute=institute_obj, case=case_obj, user=user_obj, link='', category='case', verb='mme_remove', subject=individual['display_name'], level='specific')
    case_obj['mme_submission'] = None
    updated_case = self.update_case(case_obj)
    return updated_case",False,"[""individual['phenotype'] == 2""]","_,_,_,_,_,_,_,_,_,_",0.0
"def write_referrers_to_file(self, file_path='', date=str(datetime.date.today()), organization='llnl'):
    """"""
        Writes the referrers data to file.
        """"""
    self.remove_date(file_path=file_path, date=date)
    referrers_exists = os.path.isfile(file_path)
    with open(file_path, 'a') as out:
        if '<MASK>':
            out.write('date,organization,referrer,count,count_log,uniques,' + 'uniques_logged\n')
        sorted_referrers = sorted(self.referrers_lower)
        for referrer in sorted_referrers:
            ref_name = self.referrers_lower[referrer]
            count = self.referrers[ref_name][0]
            uniques = self.referrers[ref_name][1]
            if '<MASK>':
                count = 1.5
            if '<MASK>':
                uniques = 1.5
            count_logged = math.log(count)
            uniques_logged = math.log(uniques)
            out.write(date + ',' + organization + ',' + ref_name + ',' + str(count) + ',' + str(count_logged) + ',' + str(uniques) + ',' + str(uniques_logged) + '\n')
    out.close()",False,"['not referrers_exists', 'count == 1', 'uniques == 1']",___________________,0.0
"def _simplify_doc(doc):
    """"""
    Limit a document to just the three fields we should upload.
    """"""
    doc = dict(doc)
    if '<MASK>':
        raise ValueError('The document {!r} has no text field'.format(doc))
    return {'text': doc['text'], 'metadata': doc.get('metadata', []), 'title': doc.get('title', '')}",False,"[""'text' not in doc""]",(((((((((((((((((((,0.0
"def build_requirements():
    """"""构造requirements文件

        requirements文件共分为两份：

        - hive.txt  从hive项目中直接复制
        - hive-modules.txt 从指定的模块中装载依赖项

        .. note::
            requirements要求必须是顺序无关的
            因为我们会使用set来去重，并按照value排序

        """"""
    if '<MASK>':
        os.makedirs(requirements_root)
        pass
    click.echo(click.style('Generate hive requirements...', fg='yellow'))
    shutil.copy(os.path.join(hive_root, 'requirements.txt'), os.path.join(requirements_root, 'hive.txt'))
    click.echo(click.style('Generate hive-module requirements...', fg='yellow'))
    requirements_files = []
    for m in active_module_paths:
        t = os.path.join(m, 'requirements.txt')
        if '<MASK>':
            requirements_files.append(t)
        pass
    module_packages = set()
    with fileinput.input(requirements_files) as fp:
        for line in fp:
            pkg = line.split('#')[0].strip()
            if '<MASK>':
                module_packages.add(pkg)
        pass
    with click.open_file(os.path.join(requirements_root, 'hive-modules.txt'), 'w') as fp:
        for p in module_packages:
            fp.write('%s\n' % p)
        pass
    pass",False,"['not os.path.exists(requirements_root)', 'os.path.exists(t)', 'pkg']",___________________,0.0
"@self.document.synchronize
def assert_current_path():
    if '<MASK>':
        raise ExpectationNotMet(query.failure_message)",False,['not query.resolves_for(self)'],___________________,0.0
"def parse_hgnc_genes(lines):
    """"""Parse lines with hgnc formated genes

        This is designed to take a dump with genes from HGNC.
        This is downloaded from:
        ftp://ftp.ebi.ac.uk/pub/databases/genenames/new/tsv/hgnc_complete_set.txt

        Args:
            lines(iterable(str)): An iterable with HGNC formated genes
        Yields:
            hgnc_gene(dict): A dictionary with the relevant information
    """"""
    header = []
    logger.info('Parsing hgnc genes...')
    for index, line in enumerate(lines):
        if '<MASK>':
            header = line.split('\t')
        elif '<MASK>':
            hgnc_gene = parse_hgnc_line(line=line, header=header)
            if '<MASK>':
                yield hgnc_gene",False,"['index == 0', 'len(line) > 1', 'hgnc_gene']","_,,,,,,,,,,,,,,,,,,",0.0
"def wrapper(f):

    @wraps(f)
    def decorated(*args, **kwargs):
        for func in self._before_request_funcs:
            func()
        if '<MASK>':
            return f(*args, **kwargs)
        valid, req = self.verify_request(scopes)
        for func in self._after_request_funcs:
            valid, req = func(valid, req)
        if '<MASK>':
            if '<MASK>':
                return self._invalid_response(req)
            return abort(401)
        request.oauth = req
        return f(*args, **kwargs)
    return decorated",False,"[""hasattr(request, 'oauth') and request.oauth"", 'not valid', 'self._invalid_response']",___________________,0.0
"def lookupmodule(name):
    """"""lookupmodule()->(module, file) translates a possibly incomplete
    file or module name into an absolute file name. None can be
    returned for either of the values positions of module or file when
    no or module or file is found.
    """"""
    if '<MASK>':
        return (sys.modules[name], sys.modules[name].__file__)
    if '<MASK>':
        return (None, name)
    f = os.path.join(sys.path[0], name)
    if '<MASK>':
        return (None, f)
    root, ext = os.path.splitext(name)
    if '<MASK>':
        name = name + '.py'
        pass
    if '<MASK>':
        return (None, name)
    for dirname in sys.path:
        while os.path.islink(dirname):
            dirname = os.readlink(dirname)
            pass
        fullname = os.path.join(dirname, name)
        if '<MASK>':
            return (None, fullname)
        pass
    return (None, None)",False,"['sys.modules.get(name)', 'os.path.isabs(name) and readable(name)', 'readable(f)', ""ext == ''"", 'os.path.isabs(name)', 'readable(fullname)']",(((((((((((((((((((,0.0
"def write(self, ostream, kmip_version=enums.KMIPVersion.KMIP_1_0):
    """"""
        Write the data encoding the RevocationReason object to a stream.

        Args:
            ostream (Stream): A data stream in which to encode object data,
                supporting a write method; usually a BytearrayStream object.
            kmip_version (KMIPVersion): An enumeration defining the KMIP
                version with which the object will be encoded. Optional,
                defaults to KMIP 1.0.
        """"""
    tstream = BytearrayStream()
    self.revocation_code.write(tstream, kmip_version=kmip_version)
    if '<MASK>':
        self.revocation_message.write(tstream, kmip_version=kmip_version)
    self.length = tstream.length()
    super(RevocationReason, self).write(ostream, kmip_version=kmip_version)
    ostream.write(tstream.buffer)",False,['self.revocation_message is not None'],___________________,0.0
"def extract_schema(uri):
    """"""
    Extracts Schema information from Iglu URI

    >>> extract_schema(""iglu:com.acme-corporation_underscore/event_name-dash/jsonschema/1-10-1"")['vendor']
    'com.acme-corporation_underscore'
    """"""
    match = re.match(SCHEMA_URI_REGEX, uri)
    if '<MASK>':
        return {'vendor': match.group(1), 'name': match.group(2), 'format': match.group(3), 'version': match.group(4)}
    else:
        raise SnowplowEventTransformationException(['Schema {} does not conform to regular expression {}'.format(uri, SCHEMA_URI)])",False,['match'],___________________,0.0
"@self.document.synchronize(errors=(WindowError,), wait=wait)
def switch_and_get_matching_window():
    original_window_handle = self.driver.current_window_handle
    try:
        for handle in self.driver.window_handles:
            self.driver.switch_to_window(handle)
            result = window()
            if '<MASK>':
                return Window(self, handle)
    except Exception:
        self.driver.switch_to_window(original_window_handle)
        raise
    self.driver.switch_to_window(original_window_handle)
    raise WindowError('Could not find a window matching lambda')",False,['result'],___________________,0.0
